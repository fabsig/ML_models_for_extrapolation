{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 361287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openml\n",
    "from sklearn.linear_model import LinearRegression \n",
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from engression import engression\n",
    "import torch\n",
    "from rtdl_revisiting_models import MLP, ResNet, FTTransformer\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "from pygam import LinearGAM\n",
    "import gower\n",
    "from utils import EarlyStopping, train, train_trans, train_no_early_stopping, train_trans_no_early_stopping\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import shutil\n",
    "import gpboost as gpb\n",
    "\n",
    "#openml.config.apikey = 'FILL_IN_OPENML_API_KEY'  # set the OpenML Api Key\n",
    "#SUITE_ID = 336 # Regression on numerical features\n",
    "#SUITE_ID = 337 # Classification on numerical features\n",
    "SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "\n",
    "task_id=361287\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "N_TRIALS=100\n",
    "N_SAMPLES=100\n",
    "PATIENCE=40\n",
    "N_EPOCHS=1000\n",
    "GP_ITERATIONS=1000\n",
    "BATCH_SIZE=1024\n",
    "seed=10\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "CHECKPOINT_PATH = f'CHECKPOINTS/GOWER/task_{task_id}.pt'\n",
    "\n",
    "print(f\"Task {task_id}\")\n",
    "\n",
    "task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "dataset = task.get_dataset()\n",
    "\n",
    "X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "\n",
    "if task_id==361099:\n",
    "    y=np.log(y)\n",
    "\n",
    "if len(X) > 15000:\n",
    "    indices = np.random.choice(X.index, size=15000, replace=False)\n",
    "    X = X.iloc[indices,]\n",
    "    y = y[indices]\n",
    "\n",
    "# Remove categorical columns with more than 20 unique values and non-categorical columns with less than 10 unique values\n",
    "# Remove non-categorical columns with more than 70% of the data in one category from X_clean\n",
    "for col in [attribute for attribute, indicator in zip(attribute_names, categorical_indicator) if indicator]:\n",
    "    if len(X[col].unique()) > 20:\n",
    "        X = X.drop(col, axis=1)\n",
    "\n",
    "X_clean=X.copy()\n",
    "for col in [attribute for attribute, indicator in zip(attribute_names, categorical_indicator) if not indicator]:\n",
    "    if len(X[col].unique()) < 10:\n",
    "        X = X.drop(col, axis=1)\n",
    "        X_clean = X_clean.drop(col, axis=1)\n",
    "    elif X[col].value_counts(normalize=True).max() > 0.7:\n",
    "        X_clean = X_clean.drop(col, axis=1)\n",
    "\n",
    "# Find features with absolute correlation > 0.9\n",
    "corr_matrix = X_clean.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "high_corr_features = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\n",
    "\n",
    "# Drop one of the highly correlated features from X_clean\n",
    "X_clean = X_clean.drop(high_corr_features, axis=1)\n",
    "\n",
    "# Rename columns to avoid problems with LGBM\n",
    "X = X.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "# Compute Gower distance and define train and test set\n",
    "# calculate the Gower distance matrix for the entire dataset\n",
    "for col in X_clean.select_dtypes(['category']).columns:\n",
    "    X_clean[col] = X_clean[col].astype('object')\n",
    "\n",
    "gower_dist_matrix = gower.gower_matrix(X_clean)\n",
    "\n",
    "# calculate the Gower distance for each data point\n",
    "gower_dist = np.mean(gower_dist_matrix, axis=1)\n",
    "\n",
    "gower_dist=pd.Series(gower_dist,index=X_clean.index)\n",
    "far_index=gower_dist.index[np.where(gower_dist>=np.quantile(gower_dist,0.8))[0]]\n",
    "close_index=gower_dist.index[np.where(gower_dist<np.quantile(gower_dist,0.8))[0]]\n",
    "\n",
    "X_clean_ = X_clean.loc[close_index,:]\n",
    "\n",
    "for col in X_clean_.select_dtypes(['category']).columns:\n",
    "    X_clean_[col] = X_clean_[col].astype('object')\n",
    "\n",
    "# calculate the Gower distance matrix for the training set\n",
    "gower_dist_matrix_train = gower.gower_matrix(X_clean_)\n",
    "\n",
    "# calculate the Gower distance for each data point in the training set\n",
    "gower_dist_train = np.mean(gower_dist_matrix_train, axis=1)\n",
    "\n",
    "gower_dist_train=pd.Series(gower_dist_train,index=X_clean_.index)\n",
    "far_index_train=gower_dist_train.index[np.where(gower_dist_train>=np.quantile(gower_dist_train,0.8))[0]]\n",
    "close_index_train=gower_dist_train.index[np.where(gower_dist_train<np.quantile(gower_dist_train,0.8))[0]]\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "# Modify X_train_, X_val, X_train, and X_test to have dummy variables\n",
    "non_dummy_cols = X.select_dtypes(exclude=['bool', 'category', 'object', 'string']).columns\n",
    "X = pd.get_dummies(X, drop_first=True).astype('float32')\n",
    "\n",
    "X_train = X.loc[close_index,:]\n",
    "X_test = X.loc[far_index,:]\n",
    "y_train = y.loc[close_index]\n",
    "y_test = y.loc[far_index]\n",
    "\n",
    "X_train_ = X_train.loc[close_index_train,:]\n",
    "X_val = X_train.loc[far_index_train,:]\n",
    "y_train_ = y_train.loc[close_index_train]\n",
    "y_val = y_train.loc[far_index_train]\n",
    "\n",
    "# Standardize the data for non-dummy variables\n",
    "mean_X_train_ = np.mean(X_train_[non_dummy_cols], axis=0)\n",
    "std_X_train_ = np.std(X_train_[non_dummy_cols], axis=0)\n",
    "X_train_[non_dummy_cols] = (X_train_[non_dummy_cols] - mean_X_train_) / std_X_train_\n",
    "X_val = X_val.copy()\n",
    "X_val[non_dummy_cols] = (X_val[non_dummy_cols] - mean_X_train_) / std_X_train_\n",
    "\n",
    "mean_X_train = np.mean(X_train[non_dummy_cols], axis=0)\n",
    "std_X_train = np.std(X_train[non_dummy_cols], axis=0)\n",
    "X_train[non_dummy_cols] = (X_train[non_dummy_cols] - mean_X_train) / std_X_train\n",
    "X_test = X_test.copy()\n",
    "X_test[non_dummy_cols] = (X_test[non_dummy_cols] - mean_X_train) / std_X_train\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train__tensor = torch.tensor(X_train_.values, dtype=torch.float32)\n",
    "y_train__tensor = torch.tensor(y_train_.values, dtype=torch.float32)\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Convert to use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU\")\n",
    "    X_train__tensor = X_train__tensor.cuda()\n",
    "    y_train__tensor = y_train__tensor.cuda()\n",
    "    X_train_tensor = X_train_tensor.cuda()\n",
    "    y_train_tensor = y_train_tensor.cuda()\n",
    "    X_val_tensor = X_val_tensor.cuda()\n",
    "    y_val_tensor = y_val_tensor.cuda()\n",
    "    X_test_tensor = X_test_tensor.cuda()\n",
    "    y_test_tensor = y_test_tensor.cuda()\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Create flattened versions of the data\n",
    "y_val_np = y_val.values.flatten()\n",
    "y_test_np = y_test.values.flatten()\n",
    "\n",
    "# Create TensorDatasets for training and validation sets\n",
    "train__dataset = TensorDataset(X_train__tensor, y_train__tensor)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders for training and validation sets\n",
    "train__loader = DataLoader(train__dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Define d_out and d_in\n",
    "d_out = 1  \n",
    "d_in=X_train_.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-04-25 16:33:49,350] A new study created in memory with name: no-name-3b98ca3b-5d98-47ea-b568-ad5ea64833dc\n",
      "[W 2024-04-25 16:37:13,213] Trial 0 failed with parameters: {'n_blocks': 4, 'd_block': 20, 'dropout': 0.6336482349262754, 'learning_rate': 0.010495405390719734, 'weight_decay': 3.1083868392602017e-06, 'n_epochs': 1000} because of the following error: The value nan is not acceptable.\n",
      "[W 2024-04-25 16:37:13,249] Trial 0 failed with value tensor(nan).\n",
      "[W 2024-04-25 16:38:23,912] Trial 1 failed with parameters: {'n_blocks': 2, 'd_block': 107, 'dropout': 0.7605307121989587, 'learning_rate': 0.0002860388842288948, 'weight_decay': 2.765025054332623e-08} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\dalma\\AppData\\Local\\Temp\\ipykernel_27504\\769408317.py\", line 30, in MLP_opt\n",
      "    n_epochs=train(MLP_model, criterion, optimizer, n_epochs, train__loader, val_loader, early_stopping, CHECKPOINT_PATH)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\NUMERICAL AND CATEGORICAL FEATURES\\REGRESSION\\RMSE\\utils.py\", line 52, in train\n",
      "    outputs = model(batch_X).reshape(-1,)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\rtdl_revisiting_models.py\", line 79, in forward\n",
      "    x = block(x)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "KeyboardInterrupt\n",
      "[W 2024-04-25 16:38:24,195] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[98], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m sampler_MLP \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[0;32m     48\u001b[0m study_MLP \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(sampler\u001b[38;5;241m=\u001b[39msampler_MLP, direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 49\u001b[0m \u001b[43mstudy_MLP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMLP_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_TRIALS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m MLP_model \u001b[38;5;241m=\u001b[39m MLP(\n\u001b[0;32m     52\u001b[0m     d_in\u001b[38;5;241m=\u001b[39md_in,\n\u001b[0;32m     53\u001b[0m     d_out\u001b[38;5;241m=\u001b[39md_out,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m     dropout\u001b[38;5;241m=\u001b[39mstudy_MLP\u001b[38;5;241m.\u001b[39mbest_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     57\u001b[0m     )\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[98], line 30\u001b[0m, in \u001b[0;36mMLP_opt\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     27\u001b[0m     MLP_model \u001b[38;5;241m=\u001b[39m MLP_model\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     29\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(patience\u001b[38;5;241m=\u001b[39mPATIENCE, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, path\u001b[38;5;241m=\u001b[39mCHECKPOINT_PATH)\n\u001b[1;32m---> 30\u001b[0m n_epochs\u001b[38;5;241m=\u001b[39m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mMLP_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain__loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCHECKPOINT_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_int(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m, n_epochs, n_epochs)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Point prediction\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\NUMERICAL AND CATEGORICAL FEATURES\\REGRESSION\\RMSE\\utils.py:52\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, criterion, optimizer, training_iterations, train_loader, val_loader, early_stopping, checkpoint_path)\u001b[0m\n\u001b[0;32m     49\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[1;32m---> 52\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_X\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,)\n\u001b[0;32m     53\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, batch_y)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Backward pass and optimize\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\rtdl_revisiting_models.py:79\u001b[0m, in \u001b[0;36mMLP.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Do the forward pass.\"\"\"\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks:\n\u001b[1;32m---> 79\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     81\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(x)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_TRIALS=5\n",
    "#### MLP\n",
    "def MLP_opt(trial):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    n_blocks = trial.suggest_int(\"n_blocks\", 1, 5)\n",
    "    d_block = trial.suggest_int(\"d_block\", 10, 500)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0, 1)\n",
    "\n",
    "    MLP_model = MLP(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    n_blocks=n_blocks,\n",
    "    d_block=d_block,\n",
    "    dropout=dropout,\n",
    "    )\n",
    "    n_epochs=N_EPOCHS\n",
    "    learning_rate=trial.suggest_float('learning_rate', 0.0001, 0.05, log=True)\n",
    "    weight_decay=trial.suggest_float('weight_decay', 1e-8, 1e-3, log=True)\n",
    "    optimizer=torch.optim.Adam(MLP_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        MLP_model = MLP_model.cuda()\n",
    "\n",
    "    early_stopping = EarlyStopping(patience=PATIENCE, verbose=False, path=CHECKPOINT_PATH)\n",
    "    n_epochs=train(MLP_model, criterion, optimizer, n_epochs, train__loader, val_loader, early_stopping, CHECKPOINT_PATH)\n",
    "    n_epochs = trial.suggest_int('n_epochs', n_epochs, n_epochs)\n",
    "\n",
    "    # Point prediction\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        for batch_X, _ in val_loader:\n",
    "            batch_predictions = MLP_model(batch_X).reshape(-1,)\n",
    "            predictions.append(batch_predictions.cpu().numpy())\n",
    "\n",
    "    y_val_hat_MLP = torch.Tensor(np.concatenate(predictions))\n",
    "    if torch.cuda.is_available():\n",
    "        y_val_hat_MLP = y_val_hat_MLP.cuda()\n",
    "    RMSE_MLP=torch.sqrt(torch.mean(torch.square(y_val_tensor - y_val_hat_MLP)))\n",
    "\n",
    "    return RMSE_MLP\n",
    "\n",
    "sampler_MLP = optuna.samplers.TPESampler(seed=seed)\n",
    "study_MLP = optuna.create_study(sampler=sampler_MLP, direction='minimize')\n",
    "study_MLP.optimize(MLP_opt, n_trials=N_TRIALS)\n",
    "\n",
    "MLP_model = MLP(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    n_blocks=study_MLP.best_params['n_blocks'],\n",
    "    d_block=study_MLP.best_params['d_block'],\n",
    "    dropout=study_MLP.best_params['dropout'],\n",
    "    )\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    MLP_model = MLP_model.cuda()\n",
    "    \n",
    "n_epochs=study_MLP.best_params['n_epochs']\n",
    "learning_rate=study_MLP.best_params['learning_rate']\n",
    "weight_decay=study_MLP.best_params['weight_decay']\n",
    "optimizer=torch.optim.Adam(MLP_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "train_no_early_stopping(MLP_model, criterion, optimizer, n_epochs, train_loader)\n",
    "\n",
    "# Point prediction\n",
    "predictions = []\n",
    "with torch.no_grad():\n",
    "    for batch_X, _ in test_loader:\n",
    "        batch_predictions = MLP_model(batch_X).reshape(-1,)\n",
    "        predictions.append(batch_predictions.cpu().numpy())\n",
    "\n",
    "y_test_hat_MLP = torch.Tensor(np.concatenate(predictions))\n",
    "if torch.cuda.is_available():\n",
    "    y_test_hat_MLP = y_test_hat_MLP.cuda()\n",
    "RMSE_MLP=torch.sqrt(torch.mean(torch.square(y_test_tensor - y_test_hat_MLP)))\n",
    "print(\"RMSE MLP: \", RMSE_MLP)\n",
    "del MLP_model, optimizer, criterion, y_test_hat_MLP, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.isnan(X_train).sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "\n",
    "if task_id==361099:\n",
    "    y=np.log(y)\n",
    "\n",
    "if len(X) > 15000:\n",
    "    indices = np.random.choice(X.index, size=15000, replace=False)\n",
    "    X = X.iloc[indices,]\n",
    "    y = y[indices]\n",
    "\n",
    "# Remove categorical columns with more than 20 unique values and non-categorical columns with less than 10 unique values\n",
    "# Remove non-categorical columns with more than 70% of the data in one category from X_clean\n",
    "for col in [attribute for attribute, indicator in zip(attribute_names, categorical_indicator) if indicator]:\n",
    "    if len(X[col].unique()) > 20:\n",
    "        X = X.drop(col, axis=1)\n",
    "\n",
    "X_clean=X.copy()\n",
    "for col in [attribute for attribute, indicator in zip(attribute_names, categorical_indicator) if not indicator]:\n",
    "    if len(X[col].unique()) < 10:\n",
    "        X = X.drop(col, axis=1)\n",
    "        X_clean = X_clean.drop(col, axis=1)\n",
    "    elif X[col].value_counts(normalize=True).max() > 0.7:\n",
    "        X_clean = X_clean.drop(col, axis=1)\n",
    "\n",
    "# Find features with absolute correlation > 0.9\n",
    "corr_matrix = X_clean.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "high_corr_features = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\n",
    "\n",
    "# Drop one of the highly correlated features from X_clean\n",
    "X_clean = X_clean.drop(high_corr_features, axis=1)\n",
    "\n",
    "# Rename columns to avoid problems with LGBM\n",
    "X = X.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oz1</th>\n",
       "      <th>oz2</th>\n",
       "      <th>oz3</th>\n",
       "      <th>oz4</th>\n",
       "      <th>oz5</th>\n",
       "      <th>oz6</th>\n",
       "      <th>oz7</th>\n",
       "      <th>oz8</th>\n",
       "      <th>oz9</th>\n",
       "      <th>oz10</th>\n",
       "      <th>...</th>\n",
       "      <th>oz254</th>\n",
       "      <th>oz256</th>\n",
       "      <th>oz257</th>\n",
       "      <th>oz258</th>\n",
       "      <th>oz259</th>\n",
       "      <th>oz260</th>\n",
       "      <th>oz261</th>\n",
       "      <th>oz262</th>\n",
       "      <th>oz264</th>\n",
       "      <th>oz265</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.106112</td>\n",
       "      <td>0.153159</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.177273</td>\n",
       "      <td>0.164345</td>\n",
       "      <td>0.180812</td>\n",
       "      <td>0.188449</td>\n",
       "      <td>0.171053</td>\n",
       "      <td>0.156839</td>\n",
       "      <td>0.366485</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.106112</td>\n",
       "      <td>0.099068</td>\n",
       "      <td>0.178231</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.145179</td>\n",
       "      <td>0.184502</td>\n",
       "      <td>0.181485</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>0.188336</td>\n",
       "      <td>0.620572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.106112</td>\n",
       "      <td>0.120172</td>\n",
       "      <td>0.317007</td>\n",
       "      <td>0.118182</td>\n",
       "      <td>0.096524</td>\n",
       "      <td>0.110701</td>\n",
       "      <td>0.108506</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.102034</td>\n",
       "      <td>0.301090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.089232</td>\n",
       "      <td>0.087194</td>\n",
       "      <td>0.193878</td>\n",
       "      <td>0.095455</td>\n",
       "      <td>0.061344</td>\n",
       "      <td>0.092251</td>\n",
       "      <td>0.081890</td>\n",
       "      <td>0.092105</td>\n",
       "      <td>0.081679</td>\n",
       "      <td>0.274523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.111846</td>\n",
       "      <td>0.118452</td>\n",
       "      <td>0.271429</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.119729</td>\n",
       "      <td>0.143911</td>\n",
       "      <td>0.153152</td>\n",
       "      <td>0.118421</td>\n",
       "      <td>0.164176</td>\n",
       "      <td>0.561308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8880</th>\n",
       "      <td>0.455253</td>\n",
       "      <td>0.394826</td>\n",
       "      <td>0.284354</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>0.351773</td>\n",
       "      <td>0.564576</td>\n",
       "      <td>0.442494</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.456440</td>\n",
       "      <td>0.561308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8881</th>\n",
       "      <td>0.129336</td>\n",
       "      <td>0.161179</td>\n",
       "      <td>0.425850</td>\n",
       "      <td>0.245455</td>\n",
       "      <td>0.226020</td>\n",
       "      <td>0.254613</td>\n",
       "      <td>0.281939</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.241231</td>\n",
       "      <td>0.608311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8882</th>\n",
       "      <td>0.111846</td>\n",
       "      <td>0.155457</td>\n",
       "      <td>0.505442</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.209552</td>\n",
       "      <td>0.201107</td>\n",
       "      <td>0.244162</td>\n",
       "      <td>0.197368</td>\n",
       "      <td>0.205962</td>\n",
       "      <td>0.561308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8883</th>\n",
       "      <td>0.129336</td>\n",
       "      <td>0.179882</td>\n",
       "      <td>0.531973</td>\n",
       "      <td>0.236364</td>\n",
       "      <td>0.223691</td>\n",
       "      <td>0.239852</td>\n",
       "      <td>0.236435</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.214358</td>\n",
       "      <td>0.437330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8884</th>\n",
       "      <td>0.655156</td>\n",
       "      <td>0.481052</td>\n",
       "      <td>0.193878</td>\n",
       "      <td>0.763636</td>\n",
       "      <td>0.402673</td>\n",
       "      <td>0.817343</td>\n",
       "      <td>0.579009</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>0.666174</td>\n",
       "      <td>0.597411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8885 rows × 255 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           oz1       oz2       oz3       oz4       oz5       oz6       oz7  \\\n",
       "0     0.106112  0.153159  0.533333  0.177273  0.164345  0.180812  0.188449   \n",
       "1     0.106112  0.099068  0.178231  0.181818  0.145179  0.184502  0.181485   \n",
       "2     0.106112  0.120172  0.317007  0.118182  0.096524  0.110701  0.108506   \n",
       "3     0.089232  0.087194  0.193878  0.095455  0.061344  0.092251  0.081890   \n",
       "4     0.111846  0.118452  0.271429  0.150000  0.119729  0.143911  0.153152   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8880  0.455253  0.394826  0.284354  0.527273  0.351773  0.564576  0.442494   \n",
       "8881  0.129336  0.161179  0.425850  0.245455  0.226020  0.254613  0.281939   \n",
       "8882  0.111846  0.155457  0.505442  0.204545  0.209552  0.201107  0.244162   \n",
       "8883  0.129336  0.179882  0.531973  0.236364  0.223691  0.239852  0.236435   \n",
       "8884  0.655156  0.481052  0.193878  0.763636  0.402673  0.817343  0.579009   \n",
       "\n",
       "           oz8       oz9      oz10  ...  oz254  oz256  oz257  oz258  oz259  \\\n",
       "0     0.171053  0.156839  0.366485  ...    0.0      0    0.0    0.0    0.0   \n",
       "1     0.184211  0.188336  0.620572  ...    0.0      0    0.0    0.0    0.0   \n",
       "2     0.105263  0.102034  0.301090  ...    0.0      0    0.0    0.0    0.0   \n",
       "3     0.092105  0.081679  0.274523  ...    0.0      0    0.0    0.0    0.0   \n",
       "4     0.118421  0.164176  0.561308  ...    0.0      0    0.0    0.0    0.0   \n",
       "...        ...       ...       ...  ...    ...    ...    ...    ...    ...   \n",
       "8880  0.631579  0.456440  0.561308  ...    0.0      0    0.0    0.0    0.0   \n",
       "8881  0.263158  0.241231  0.608311  ...    0.0      0    0.0    0.0    0.0   \n",
       "8882  0.197368  0.205962  0.561308  ...    0.0      0    0.0    0.0    0.0   \n",
       "8883  0.236842  0.214358  0.437330  ...    0.0      0    0.0    0.0    0.0   \n",
       "8884  0.921053  0.666174  0.597411  ...    0.0      0    0.0    0.0    0.0   \n",
       "\n",
       "      oz260  oz261  oz262  oz264  oz265  \n",
       "0         0    0.0    0.0    0.0      0  \n",
       "1         0    0.0    0.0    0.0      0  \n",
       "2         0    0.0    0.0    0.0      0  \n",
       "3         0    0.0    0.0    0.0      0  \n",
       "4         0    0.0    0.0    0.0      0  \n",
       "...     ...    ...    ...    ...    ...  \n",
       "8880      0    0.0    0.0    0.0      0  \n",
       "8881      0    0.0    0.0    0.0      0  \n",
       "8882      0    0.0    0.0    0.0      0  \n",
       "8883      0    0.0    0.0    0.0      0  \n",
       "8884      0    0.0    0.0    0.0      0  \n",
       "\n",
       "[8885 rows x 255 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oz1</th>\n",
       "      <th>oz3</th>\n",
       "      <th>oz5</th>\n",
       "      <th>oz10</th>\n",
       "      <th>oz12</th>\n",
       "      <th>oz18</th>\n",
       "      <th>oz24</th>\n",
       "      <th>oz31</th>\n",
       "      <th>oz36</th>\n",
       "      <th>oz38</th>\n",
       "      <th>...</th>\n",
       "      <th>oz216</th>\n",
       "      <th>oz217</th>\n",
       "      <th>oz223</th>\n",
       "      <th>oz224</th>\n",
       "      <th>oz231</th>\n",
       "      <th>oz232</th>\n",
       "      <th>oz239</th>\n",
       "      <th>oz256</th>\n",
       "      <th>oz260</th>\n",
       "      <th>oz265</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.106112</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.164345</td>\n",
       "      <td>0.366485</td>\n",
       "      <td>0.007072</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.042617</td>\n",
       "      <td>0.057284</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021665</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003564</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.106112</td>\n",
       "      <td>0.178231</td>\n",
       "      <td>0.145179</td>\n",
       "      <td>0.620572</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>0.239560</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0.033559</td>\n",
       "      <td>0.018877</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.106112</td>\n",
       "      <td>0.317007</td>\n",
       "      <td>0.096524</td>\n",
       "      <td>0.301090</td>\n",
       "      <td>0.041018</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.082513</td>\n",
       "      <td>0.077102</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.089232</td>\n",
       "      <td>0.193878</td>\n",
       "      <td>0.061344</td>\n",
       "      <td>0.274523</td>\n",
       "      <td>0.079208</td>\n",
       "      <td>0.349451</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>0.018710</td>\n",
       "      <td>0.038895</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.111846</td>\n",
       "      <td>0.271429</td>\n",
       "      <td>0.119729</td>\n",
       "      <td>0.561308</td>\n",
       "      <td>0.005658</td>\n",
       "      <td>0.320879</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>0.092412</td>\n",
       "      <td>0.065234</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029936</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8880</th>\n",
       "      <td>0.455253</td>\n",
       "      <td>0.284354</td>\n",
       "      <td>0.351773</td>\n",
       "      <td>0.561308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.134066</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>0.165025</td>\n",
       "      <td>0.081000</td>\n",
       "      <td>0.034444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.140954</td>\n",
       "      <td>0.117352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029703</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8881</th>\n",
       "      <td>0.129336</td>\n",
       "      <td>0.425850</td>\n",
       "      <td>0.226020</td>\n",
       "      <td>0.608311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.186813</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>0.038707</td>\n",
       "      <td>0.025471</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034126</td>\n",
       "      <td>0.003255</td>\n",
       "      <td>0.008147</td>\n",
       "      <td>0.001320</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8882</th>\n",
       "      <td>0.111846</td>\n",
       "      <td>0.505442</td>\n",
       "      <td>0.209552</td>\n",
       "      <td>0.561308</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>0.279121</td>\n",
       "      <td>0.000983</td>\n",
       "      <td>0.092808</td>\n",
       "      <td>0.043313</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8883</th>\n",
       "      <td>0.129336</td>\n",
       "      <td>0.531973</td>\n",
       "      <td>0.223691</td>\n",
       "      <td>0.437330</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.106370</td>\n",
       "      <td>0.086353</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.100716</td>\n",
       "      <td>0.012114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010891</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8884</th>\n",
       "      <td>0.655156</td>\n",
       "      <td>0.193878</td>\n",
       "      <td>0.402673</td>\n",
       "      <td>0.597411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120879</td>\n",
       "      <td>0.032710</td>\n",
       "      <td>0.220264</td>\n",
       "      <td>0.162567</td>\n",
       "      <td>0.384995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015095</td>\n",
       "      <td>0.003925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.453563</td>\n",
       "      <td>0.037429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8885 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           oz1       oz3       oz5      oz10      oz12      oz18      oz24  \\\n",
       "0     0.106112  0.533333  0.164345  0.366485  0.007072  0.230769  0.001084   \n",
       "1     0.106112  0.178231  0.145179  0.620572  0.002829  0.239560  0.000974   \n",
       "2     0.106112  0.317007  0.096524  0.301090  0.041018  0.353846  0.001015   \n",
       "3     0.089232  0.193878  0.061344  0.274523  0.079208  0.349451  0.000985   \n",
       "4     0.111846  0.271429  0.119729  0.561308  0.005658  0.320879  0.000973   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8880  0.455253  0.284354  0.351773  0.561308  0.000000  0.134066  0.002055   \n",
       "8881  0.129336  0.425850  0.226020  0.608311  0.000000  0.186813  0.000986   \n",
       "8882  0.111846  0.505442  0.209552  0.561308  0.001414  0.279121  0.000983   \n",
       "8883  0.129336  0.531973  0.223691  0.437330  0.001414  0.257143  0.001167   \n",
       "8884  0.655156  0.193878  0.402673  0.597411  0.000000  0.120879  0.032710   \n",
       "\n",
       "          oz31      oz36      oz38  ...     oz216     oz217     oz223  \\\n",
       "0     0.042617  0.057284  0.000135  ...  0.000001  0.000193  0.000000   \n",
       "1     0.033559  0.018877  0.000286  ...  0.000007  0.000656  0.000000   \n",
       "2     0.082513  0.077102  0.000027  ...  0.000000  0.000055  0.000000   \n",
       "3     0.018710  0.038895  0.000019  ...  0.000000  0.000038  0.000000   \n",
       "4     0.092412  0.065234  0.000085  ...  0.000001  0.000236  0.000000   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "8880  0.165025  0.081000  0.034444  ...  0.000008  0.000001  0.140954   \n",
       "8881  0.038707  0.025471  0.001018  ...  0.000038  0.001372  0.000000   \n",
       "8882  0.092808  0.043313  0.000313  ...  0.000007  0.000636  0.000000   \n",
       "8883  0.106370  0.086353  0.000445  ...  0.000002  0.000147  0.100716   \n",
       "8884  0.220264  0.162567  0.384995  ...  0.015095  0.003925  0.000000   \n",
       "\n",
       "         oz224     oz231     oz232     oz239  oz256  oz260  oz265  \n",
       "0     0.021665  0.000000  0.003564  0.001650      0      0      0  \n",
       "1     0.024162  0.000000  0.000000  0.000000      0      0      0  \n",
       "2     0.012414  0.000000  0.000000  0.000000      0      0      0  \n",
       "3     0.009285  0.000000  0.000000  0.000000      0      0      0  \n",
       "4     0.029936  0.000000  0.000000  0.000000      0      0      0  \n",
       "...        ...       ...       ...       ...    ...    ...    ...  \n",
       "8880  0.117352  0.000000  0.000000  0.029703      0      0      0  \n",
       "8881  0.034126  0.003255  0.008147  0.001320      0      0      0  \n",
       "8882  0.039725  0.000000  0.002037  0.000000      0      0      0  \n",
       "8883  0.012114  0.000000  0.000000  0.010891      0      0      0  \n",
       "8884  0.453563  0.037429  0.000000  0.000000      0      0      0  \n",
       "\n",
       "[8885 rows x 57 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oz1</th>\n",
       "      <th>oz2</th>\n",
       "      <th>oz3</th>\n",
       "      <th>oz4</th>\n",
       "      <th>oz5</th>\n",
       "      <th>oz6</th>\n",
       "      <th>oz7</th>\n",
       "      <th>oz8</th>\n",
       "      <th>oz9</th>\n",
       "      <th>oz10</th>\n",
       "      <th>...</th>\n",
       "      <th>oz254</th>\n",
       "      <th>oz257</th>\n",
       "      <th>oz258</th>\n",
       "      <th>oz259</th>\n",
       "      <th>oz261</th>\n",
       "      <th>oz262</th>\n",
       "      <th>oz264</th>\n",
       "      <th>oz256_1</th>\n",
       "      <th>oz260_1</th>\n",
       "      <th>oz265_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.884618</td>\n",
       "      <td>-0.739788</td>\n",
       "      <td>0.515927</td>\n",
       "      <td>-0.595265</td>\n",
       "      <td>-0.544639</td>\n",
       "      <td>-0.471132</td>\n",
       "      <td>-0.469190</td>\n",
       "      <td>-0.254175</td>\n",
       "      <td>-0.772635</td>\n",
       "      <td>-0.893560</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046927</td>\n",
       "      <td>-0.062632</td>\n",
       "      <td>-0.071735</td>\n",
       "      <td>-0.021859</td>\n",
       "      <td>-0.133986</td>\n",
       "      <td>-0.051635</td>\n",
       "      <td>-0.061955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.884618</td>\n",
       "      <td>-1.522186</td>\n",
       "      <td>-1.937543</td>\n",
       "      <td>-0.537708</td>\n",
       "      <td>-0.797438</td>\n",
       "      <td>-0.425048</td>\n",
       "      <td>-0.550386</td>\n",
       "      <td>-0.085653</td>\n",
       "      <td>-0.362768</td>\n",
       "      <td>1.683122</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046927</td>\n",
       "      <td>-0.062632</td>\n",
       "      <td>-0.071735</td>\n",
       "      <td>-0.021859</td>\n",
       "      <td>-0.133986</td>\n",
       "      <td>-0.051635</td>\n",
       "      <td>-0.061955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.805870</td>\n",
       "      <td>-1.241807</td>\n",
       "      <td>-1.293620</td>\n",
       "      <td>-0.940646</td>\n",
       "      <td>-1.133123</td>\n",
       "      <td>-0.931978</td>\n",
       "      <td>-0.880730</td>\n",
       "      <td>-0.928263</td>\n",
       "      <td>-0.677160</td>\n",
       "      <td>1.082130</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046927</td>\n",
       "      <td>-0.062632</td>\n",
       "      <td>-0.071735</td>\n",
       "      <td>-0.021859</td>\n",
       "      <td>-0.133986</td>\n",
       "      <td>-0.051635</td>\n",
       "      <td>-0.061955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.805870</td>\n",
       "      <td>-0.976687</td>\n",
       "      <td>-0.494599</td>\n",
       "      <td>-0.422580</td>\n",
       "      <td>-0.501244</td>\n",
       "      <td>-0.355923</td>\n",
       "      <td>-0.149957</td>\n",
       "      <td>-0.085653</td>\n",
       "      <td>-0.218000</td>\n",
       "      <td>1.648582</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046927</td>\n",
       "      <td>-0.062632</td>\n",
       "      <td>-0.071735</td>\n",
       "      <td>-0.021859</td>\n",
       "      <td>-0.133986</td>\n",
       "      <td>-0.051635</td>\n",
       "      <td>-0.061955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>-1.341054</td>\n",
       "      <td>-1.580305</td>\n",
       "      <td>-0.527500</td>\n",
       "      <td>-1.631396</td>\n",
       "      <td>-1.350336</td>\n",
       "      <td>-1.623228</td>\n",
       "      <td>-1.431307</td>\n",
       "      <td>-1.602351</td>\n",
       "      <td>-1.485807</td>\n",
       "      <td>-0.409999</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046927</td>\n",
       "      <td>-0.062632</td>\n",
       "      <td>-0.071735</td>\n",
       "      <td>-0.021859</td>\n",
       "      <td>-0.133986</td>\n",
       "      <td>-0.051635</td>\n",
       "      <td>-0.061955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8877</th>\n",
       "      <td>0.445795</td>\n",
       "      <td>0.768337</td>\n",
       "      <td>0.417229</td>\n",
       "      <td>0.613551</td>\n",
       "      <td>0.529935</td>\n",
       "      <td>0.842257</td>\n",
       "      <td>0.407290</td>\n",
       "      <td>0.925466</td>\n",
       "      <td>0.361061</td>\n",
       "      <td>-0.188947</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046927</td>\n",
       "      <td>-0.062632</td>\n",
       "      <td>-0.071735</td>\n",
       "      <td>-0.021859</td>\n",
       "      <td>-0.133986</td>\n",
       "      <td>-0.051635</td>\n",
       "      <td>-0.061955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8878</th>\n",
       "      <td>-0.565669</td>\n",
       "      <td>-0.247272</td>\n",
       "      <td>0.797940</td>\n",
       "      <td>-0.077199</td>\n",
       "      <td>0.229362</td>\n",
       "      <td>-0.010298</td>\n",
       "      <td>0.400621</td>\n",
       "      <td>0.251378</td>\n",
       "      <td>-0.048585</td>\n",
       "      <td>0.612381</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046927</td>\n",
       "      <td>-0.062632</td>\n",
       "      <td>-0.071735</td>\n",
       "      <td>-0.021859</td>\n",
       "      <td>-0.133986</td>\n",
       "      <td>-0.051635</td>\n",
       "      <td>-0.061955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8881</th>\n",
       "      <td>-0.565669</td>\n",
       "      <td>-0.623783</td>\n",
       "      <td>-0.226695</td>\n",
       "      <td>0.268182</td>\n",
       "      <td>0.268852</td>\n",
       "      <td>0.450548</td>\n",
       "      <td>0.620842</td>\n",
       "      <td>0.925466</td>\n",
       "      <td>0.325549</td>\n",
       "      <td>1.558784</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046927</td>\n",
       "      <td>-0.062632</td>\n",
       "      <td>-0.071735</td>\n",
       "      <td>-0.021859</td>\n",
       "      <td>-0.133986</td>\n",
       "      <td>-0.051635</td>\n",
       "      <td>-0.061955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8882</th>\n",
       "      <td>-0.805870</td>\n",
       "      <td>-0.706548</td>\n",
       "      <td>0.323223</td>\n",
       "      <td>-0.249896</td>\n",
       "      <td>0.051640</td>\n",
       "      <td>-0.217673</td>\n",
       "      <td>0.180387</td>\n",
       "      <td>0.082856</td>\n",
       "      <td>-0.133403</td>\n",
       "      <td>1.082130</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046927</td>\n",
       "      <td>-0.062632</td>\n",
       "      <td>-0.071735</td>\n",
       "      <td>-0.021859</td>\n",
       "      <td>-0.133986</td>\n",
       "      <td>-0.051635</td>\n",
       "      <td>-0.061955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8883</th>\n",
       "      <td>-0.565669</td>\n",
       "      <td>-0.353253</td>\n",
       "      <td>0.506530</td>\n",
       "      <td>0.153055</td>\n",
       "      <td>0.238133</td>\n",
       "      <td>0.266202</td>\n",
       "      <td>0.090296</td>\n",
       "      <td>0.588422</td>\n",
       "      <td>-0.024146</td>\n",
       "      <td>-0.175124</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.046927</td>\n",
       "      <td>-0.062632</td>\n",
       "      <td>-0.071735</td>\n",
       "      <td>-0.021859</td>\n",
       "      <td>-0.133986</td>\n",
       "      <td>-0.051635</td>\n",
       "      <td>-0.061955</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7108 rows × 255 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           oz1       oz2       oz3       oz4       oz5       oz6       oz7  \\\n",
       "0    -0.884618 -0.739788  0.515927 -0.595265 -0.544639 -0.471132 -0.469190   \n",
       "1    -0.884618 -1.522186 -1.937543 -0.537708 -0.797438 -0.425048 -0.550386   \n",
       "4    -0.805870 -1.241807 -1.293620 -0.940646 -1.133123 -0.931978 -0.880730   \n",
       "5    -0.805870 -0.976687 -0.494599 -0.422580 -0.501244 -0.355923 -0.149957   \n",
       "8    -1.341054 -1.580305 -0.527500 -1.631396 -1.350336 -1.623228 -1.431307   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8877  0.445795  0.768337  0.417229  0.613551  0.529935  0.842257  0.407290   \n",
       "8878 -0.565669 -0.247272  0.797940 -0.077199  0.229362 -0.010298  0.400621   \n",
       "8881 -0.565669 -0.623783 -0.226695  0.268182  0.268852  0.450548  0.620842   \n",
       "8882 -0.805870 -0.706548  0.323223 -0.249896  0.051640 -0.217673  0.180387   \n",
       "8883 -0.565669 -0.353253  0.506530  0.153055  0.238133  0.266202  0.090296   \n",
       "\n",
       "           oz8       oz9      oz10  ...     oz254     oz257     oz258  \\\n",
       "0    -0.254175 -0.772635 -0.893560  ... -0.046927 -0.062632 -0.071735   \n",
       "1    -0.085653 -0.362768  1.683122  ... -0.046927 -0.062632 -0.071735   \n",
       "4    -0.928263 -0.677160  1.082130  ... -0.046927 -0.062632 -0.071735   \n",
       "5    -0.085653 -0.218000  1.648582  ... -0.046927 -0.062632 -0.071735   \n",
       "8    -1.602351 -1.485807 -0.409999  ... -0.046927 -0.062632 -0.071735   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "8877  0.925466  0.361061 -0.188947  ... -0.046927 -0.062632 -0.071735   \n",
       "8878  0.251378 -0.048585  0.612381  ... -0.046927 -0.062632 -0.071735   \n",
       "8881  0.925466  0.325549  1.558784  ... -0.046927 -0.062632 -0.071735   \n",
       "8882  0.082856 -0.133403  1.082130  ... -0.046927 -0.062632 -0.071735   \n",
       "8883  0.588422 -0.024146 -0.175124  ... -0.046927 -0.062632 -0.071735   \n",
       "\n",
       "         oz259     oz261     oz262     oz264  oz256_1  oz260_1  oz265_1  \n",
       "0    -0.021859 -0.133986 -0.051635 -0.061955      0.0      0.0      0.0  \n",
       "1    -0.021859 -0.133986 -0.051635 -0.061955      0.0      0.0      0.0  \n",
       "4    -0.021859 -0.133986 -0.051635 -0.061955      0.0      0.0      0.0  \n",
       "5    -0.021859 -0.133986 -0.051635 -0.061955      0.0      0.0      0.0  \n",
       "8    -0.021859 -0.133986 -0.051635 -0.061955      0.0      0.0      0.0  \n",
       "...        ...       ...       ...       ...      ...      ...      ...  \n",
       "8877 -0.021859 -0.133986 -0.051635 -0.061955      0.0      0.0      0.0  \n",
       "8878 -0.021859 -0.133986 -0.051635 -0.061955      0.0      0.0      0.0  \n",
       "8881 -0.021859 -0.133986 -0.051635 -0.061955      0.0      0.0      0.0  \n",
       "8882 -0.021859 -0.133986 -0.051635 -0.061955      0.0      0.0      0.0  \n",
       "8883 -0.021859 -0.133986 -0.051635 -0.061955      0.0      0.0      0.0  \n",
       "\n",
       "[7108 rows x 255 columns]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(X_clean.dtypes!='float')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oz1</th>\n",
       "      <th>oz3</th>\n",
       "      <th>oz5</th>\n",
       "      <th>oz10</th>\n",
       "      <th>oz12</th>\n",
       "      <th>oz18</th>\n",
       "      <th>oz24</th>\n",
       "      <th>oz31</th>\n",
       "      <th>oz36</th>\n",
       "      <th>oz38</th>\n",
       "      <th>...</th>\n",
       "      <th>oz216</th>\n",
       "      <th>oz217</th>\n",
       "      <th>oz223</th>\n",
       "      <th>oz224</th>\n",
       "      <th>oz231</th>\n",
       "      <th>oz232</th>\n",
       "      <th>oz239</th>\n",
       "      <th>oz256_1</th>\n",
       "      <th>oz260_1</th>\n",
       "      <th>oz265_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.106112</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.164345</td>\n",
       "      <td>0.366485</td>\n",
       "      <td>0.007072</td>\n",
       "      <td>0.230769</td>\n",
       "      <td>0.001084</td>\n",
       "      <td>0.042617</td>\n",
       "      <td>0.057284</td>\n",
       "      <td>0.000135</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000193</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021665</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003564</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.106112</td>\n",
       "      <td>0.178231</td>\n",
       "      <td>0.145179</td>\n",
       "      <td>0.620572</td>\n",
       "      <td>0.002829</td>\n",
       "      <td>0.239560</td>\n",
       "      <td>0.000974</td>\n",
       "      <td>0.033559</td>\n",
       "      <td>0.018877</td>\n",
       "      <td>0.000286</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000656</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.106112</td>\n",
       "      <td>0.317007</td>\n",
       "      <td>0.096524</td>\n",
       "      <td>0.301090</td>\n",
       "      <td>0.041018</td>\n",
       "      <td>0.353846</td>\n",
       "      <td>0.001015</td>\n",
       "      <td>0.082513</td>\n",
       "      <td>0.077102</td>\n",
       "      <td>0.000027</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000055</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012414</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.089232</td>\n",
       "      <td>0.193878</td>\n",
       "      <td>0.061344</td>\n",
       "      <td>0.274523</td>\n",
       "      <td>0.079208</td>\n",
       "      <td>0.349451</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>0.018710</td>\n",
       "      <td>0.038895</td>\n",
       "      <td>0.000019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009285</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.111846</td>\n",
       "      <td>0.271429</td>\n",
       "      <td>0.119729</td>\n",
       "      <td>0.561308</td>\n",
       "      <td>0.005658</td>\n",
       "      <td>0.320879</td>\n",
       "      <td>0.000973</td>\n",
       "      <td>0.092412</td>\n",
       "      <td>0.065234</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.000236</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029936</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8880</th>\n",
       "      <td>0.455253</td>\n",
       "      <td>0.284354</td>\n",
       "      <td>0.351773</td>\n",
       "      <td>0.561308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.134066</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>0.165025</td>\n",
       "      <td>0.081000</td>\n",
       "      <td>0.034444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000001</td>\n",
       "      <td>0.140954</td>\n",
       "      <td>0.117352</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.029703</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8881</th>\n",
       "      <td>0.129336</td>\n",
       "      <td>0.425850</td>\n",
       "      <td>0.226020</td>\n",
       "      <td>0.608311</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.186813</td>\n",
       "      <td>0.000986</td>\n",
       "      <td>0.038707</td>\n",
       "      <td>0.025471</td>\n",
       "      <td>0.001018</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>0.001372</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.034126</td>\n",
       "      <td>0.003255</td>\n",
       "      <td>0.008147</td>\n",
       "      <td>0.001320</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8882</th>\n",
       "      <td>0.111846</td>\n",
       "      <td>0.505442</td>\n",
       "      <td>0.209552</td>\n",
       "      <td>0.561308</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>0.279121</td>\n",
       "      <td>0.000983</td>\n",
       "      <td>0.092808</td>\n",
       "      <td>0.043313</td>\n",
       "      <td>0.000313</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>0.000636</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.039725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.002037</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8883</th>\n",
       "      <td>0.129336</td>\n",
       "      <td>0.531973</td>\n",
       "      <td>0.223691</td>\n",
       "      <td>0.437330</td>\n",
       "      <td>0.001414</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.001167</td>\n",
       "      <td>0.106370</td>\n",
       "      <td>0.086353</td>\n",
       "      <td>0.000445</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>0.000147</td>\n",
       "      <td>0.100716</td>\n",
       "      <td>0.012114</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010891</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8884</th>\n",
       "      <td>0.655156</td>\n",
       "      <td>0.193878</td>\n",
       "      <td>0.402673</td>\n",
       "      <td>0.597411</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.120879</td>\n",
       "      <td>0.032710</td>\n",
       "      <td>0.220264</td>\n",
       "      <td>0.162567</td>\n",
       "      <td>0.384995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.015095</td>\n",
       "      <td>0.003925</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.453563</td>\n",
       "      <td>0.037429</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8885 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           oz1       oz3       oz5      oz10      oz12      oz18      oz24  \\\n",
       "0     0.106112  0.533333  0.164345  0.366485  0.007072  0.230769  0.001084   \n",
       "1     0.106112  0.178231  0.145179  0.620572  0.002829  0.239560  0.000974   \n",
       "2     0.106112  0.317007  0.096524  0.301090  0.041018  0.353846  0.001015   \n",
       "3     0.089232  0.193878  0.061344  0.274523  0.079208  0.349451  0.000985   \n",
       "4     0.111846  0.271429  0.119729  0.561308  0.005658  0.320879  0.000973   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8880  0.455253  0.284354  0.351773  0.561308  0.000000  0.134066  0.002055   \n",
       "8881  0.129336  0.425850  0.226020  0.608311  0.000000  0.186813  0.000986   \n",
       "8882  0.111846  0.505442  0.209552  0.561308  0.001414  0.279121  0.000983   \n",
       "8883  0.129336  0.531973  0.223691  0.437330  0.001414  0.257143  0.001167   \n",
       "8884  0.655156  0.193878  0.402673  0.597411  0.000000  0.120879  0.032710   \n",
       "\n",
       "          oz31      oz36      oz38  ...     oz216     oz217     oz223  \\\n",
       "0     0.042617  0.057284  0.000135  ...  0.000001  0.000193  0.000000   \n",
       "1     0.033559  0.018877  0.000286  ...  0.000007  0.000656  0.000000   \n",
       "2     0.082513  0.077102  0.000027  ...  0.000000  0.000055  0.000000   \n",
       "3     0.018710  0.038895  0.000019  ...  0.000000  0.000038  0.000000   \n",
       "4     0.092412  0.065234  0.000085  ...  0.000001  0.000236  0.000000   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "8880  0.165025  0.081000  0.034444  ...  0.000008  0.000001  0.140954   \n",
       "8881  0.038707  0.025471  0.001018  ...  0.000038  0.001372  0.000000   \n",
       "8882  0.092808  0.043313  0.000313  ...  0.000007  0.000636  0.000000   \n",
       "8883  0.106370  0.086353  0.000445  ...  0.000002  0.000147  0.100716   \n",
       "8884  0.220264  0.162567  0.384995  ...  0.015095  0.003925  0.000000   \n",
       "\n",
       "         oz224     oz231     oz232     oz239  oz256_1  oz260_1  oz265_1  \n",
       "0     0.021665  0.000000  0.003564  0.001650      0.0      0.0      0.0  \n",
       "1     0.024162  0.000000  0.000000  0.000000      0.0      0.0      0.0  \n",
       "2     0.012414  0.000000  0.000000  0.000000      0.0      0.0      0.0  \n",
       "3     0.009285  0.000000  0.000000  0.000000      0.0      0.0      0.0  \n",
       "4     0.029936  0.000000  0.000000  0.000000      0.0      0.0      0.0  \n",
       "...        ...       ...       ...       ...      ...      ...      ...  \n",
       "8880  0.117352  0.000000  0.000000  0.029703      0.0      0.0      0.0  \n",
       "8881  0.034126  0.003255  0.008147  0.001320      0.0      0.0      0.0  \n",
       "8882  0.039725  0.000000  0.002037  0.000000      0.0      0.0      0.0  \n",
       "8883  0.012114  0.000000  0.000000  0.010891      0.0      0.0      0.0  \n",
       "8884  0.453563  0.037429  0.000000  0.000000      0.0      0.0      0.0  \n",
       "\n",
       "[8885 rows x 57 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.get_dummies(X_clean, drop_first=True).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>oz1</th>\n",
       "      <th>oz2</th>\n",
       "      <th>oz3</th>\n",
       "      <th>oz4</th>\n",
       "      <th>oz5</th>\n",
       "      <th>oz6</th>\n",
       "      <th>oz7</th>\n",
       "      <th>oz8</th>\n",
       "      <th>oz9</th>\n",
       "      <th>oz10</th>\n",
       "      <th>...</th>\n",
       "      <th>oz254</th>\n",
       "      <th>oz257</th>\n",
       "      <th>oz258</th>\n",
       "      <th>oz259</th>\n",
       "      <th>oz261</th>\n",
       "      <th>oz262</th>\n",
       "      <th>oz264</th>\n",
       "      <th>oz256_1</th>\n",
       "      <th>oz260_1</th>\n",
       "      <th>oz265_1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.106112</td>\n",
       "      <td>0.153159</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.177273</td>\n",
       "      <td>0.164345</td>\n",
       "      <td>0.180812</td>\n",
       "      <td>0.188449</td>\n",
       "      <td>0.171053</td>\n",
       "      <td>0.156839</td>\n",
       "      <td>0.366485</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.106112</td>\n",
       "      <td>0.099068</td>\n",
       "      <td>0.178231</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.145179</td>\n",
       "      <td>0.184502</td>\n",
       "      <td>0.181485</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>0.188336</td>\n",
       "      <td>0.620572</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.106112</td>\n",
       "      <td>0.120172</td>\n",
       "      <td>0.317007</td>\n",
       "      <td>0.118182</td>\n",
       "      <td>0.096524</td>\n",
       "      <td>0.110701</td>\n",
       "      <td>0.108506</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.102034</td>\n",
       "      <td>0.301090</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.089232</td>\n",
       "      <td>0.087194</td>\n",
       "      <td>0.193878</td>\n",
       "      <td>0.095455</td>\n",
       "      <td>0.061344</td>\n",
       "      <td>0.092251</td>\n",
       "      <td>0.081890</td>\n",
       "      <td>0.092105</td>\n",
       "      <td>0.081679</td>\n",
       "      <td>0.274523</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.111846</td>\n",
       "      <td>0.118452</td>\n",
       "      <td>0.271429</td>\n",
       "      <td>0.150000</td>\n",
       "      <td>0.119729</td>\n",
       "      <td>0.143911</td>\n",
       "      <td>0.153152</td>\n",
       "      <td>0.118421</td>\n",
       "      <td>0.164176</td>\n",
       "      <td>0.561308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8880</th>\n",
       "      <td>0.455253</td>\n",
       "      <td>0.394826</td>\n",
       "      <td>0.284354</td>\n",
       "      <td>0.527273</td>\n",
       "      <td>0.351773</td>\n",
       "      <td>0.564576</td>\n",
       "      <td>0.442494</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.456440</td>\n",
       "      <td>0.561308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8881</th>\n",
       "      <td>0.129336</td>\n",
       "      <td>0.161179</td>\n",
       "      <td>0.425850</td>\n",
       "      <td>0.245455</td>\n",
       "      <td>0.226020</td>\n",
       "      <td>0.254613</td>\n",
       "      <td>0.281939</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.241231</td>\n",
       "      <td>0.608311</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8882</th>\n",
       "      <td>0.111846</td>\n",
       "      <td>0.155457</td>\n",
       "      <td>0.505442</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.209552</td>\n",
       "      <td>0.201107</td>\n",
       "      <td>0.244162</td>\n",
       "      <td>0.197368</td>\n",
       "      <td>0.205962</td>\n",
       "      <td>0.561308</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8883</th>\n",
       "      <td>0.129336</td>\n",
       "      <td>0.179882</td>\n",
       "      <td>0.531973</td>\n",
       "      <td>0.236364</td>\n",
       "      <td>0.223691</td>\n",
       "      <td>0.239852</td>\n",
       "      <td>0.236435</td>\n",
       "      <td>0.236842</td>\n",
       "      <td>0.214358</td>\n",
       "      <td>0.437330</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8884</th>\n",
       "      <td>0.655156</td>\n",
       "      <td>0.481052</td>\n",
       "      <td>0.193878</td>\n",
       "      <td>0.763636</td>\n",
       "      <td>0.402673</td>\n",
       "      <td>0.817343</td>\n",
       "      <td>0.579009</td>\n",
       "      <td>0.921053</td>\n",
       "      <td>0.666174</td>\n",
       "      <td>0.597411</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8885 rows × 255 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           oz1       oz2       oz3       oz4       oz5       oz6       oz7  \\\n",
       "0     0.106112  0.153159  0.533333  0.177273  0.164345  0.180812  0.188449   \n",
       "1     0.106112  0.099068  0.178231  0.181818  0.145179  0.184502  0.181485   \n",
       "2     0.106112  0.120172  0.317007  0.118182  0.096524  0.110701  0.108506   \n",
       "3     0.089232  0.087194  0.193878  0.095455  0.061344  0.092251  0.081890   \n",
       "4     0.111846  0.118452  0.271429  0.150000  0.119729  0.143911  0.153152   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "8880  0.455253  0.394826  0.284354  0.527273  0.351773  0.564576  0.442494   \n",
       "8881  0.129336  0.161179  0.425850  0.245455  0.226020  0.254613  0.281939   \n",
       "8882  0.111846  0.155457  0.505442  0.204545  0.209552  0.201107  0.244162   \n",
       "8883  0.129336  0.179882  0.531973  0.236364  0.223691  0.239852  0.236435   \n",
       "8884  0.655156  0.481052  0.193878  0.763636  0.402673  0.817343  0.579009   \n",
       "\n",
       "           oz8       oz9      oz10  ...  oz254  oz257  oz258  oz259  oz261  \\\n",
       "0     0.171053  0.156839  0.366485  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "1     0.184211  0.188336  0.620572  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "2     0.105263  0.102034  0.301090  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "3     0.092105  0.081679  0.274523  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "4     0.118421  0.164176  0.561308  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "...        ...       ...       ...  ...    ...    ...    ...    ...    ...   \n",
       "8880  0.631579  0.456440  0.561308  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "8881  0.263158  0.241231  0.608311  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "8882  0.197368  0.205962  0.561308  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "8883  0.236842  0.214358  0.437330  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "8884  0.921053  0.666174  0.597411  ...    0.0    0.0    0.0    0.0    0.0   \n",
       "\n",
       "      oz262  oz264  oz256_1  oz260_1  oz265_1  \n",
       "0       0.0    0.0      0.0      0.0      0.0  \n",
       "1       0.0    0.0      0.0      0.0      0.0  \n",
       "2       0.0    0.0      0.0      0.0      0.0  \n",
       "3       0.0    0.0      0.0      0.0      0.0  \n",
       "4       0.0    0.0      0.0      0.0      0.0  \n",
       "...     ...    ...      ...      ...      ...  \n",
       "8880    0.0    0.0      0.0      0.0      0.0  \n",
       "8881    0.0    0.0      0.0      0.0      0.0  \n",
       "8882    0.0    0.0      0.0      0.0      0.0  \n",
       "8883    0.0    0.0      0.0      0.0      0.0  \n",
       "8884    0.0    0.0      0.0      0.0      0.0  \n",
       "\n",
       "[8885 rows x 255 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
