{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import setuptools\n",
    "import openml\n",
    "from sklearn.linear_model import LinearRegression \n",
    "import lightgbm as lgbm\n",
    "import lightgbmlss\n",
    "import optuna\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from engression import engression, engression_bagged\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rtdl_revisiting_models import MLP, ResNet, FTTransformer\n",
    "from properscoring import crps_gaussian, crps_ensemble\n",
    "import random\n",
    "import gpytorch\n",
    "import tqdm.auto as tqdm\n",
    "from lightgbmlss.model import *\n",
    "from lightgbmlss.distributions.Gaussian import *\n",
    "from drf import drf\n",
    "from pygam import LinearGAM, s, f\n",
    "from utils import EarlyStopping, train, train_trans, train_no_early_stopping, train_trans_no_early_stopping, train_GP, ExactGPModel\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "SUITE_ID = 336 # Regression on numerical features\n",
    "#SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "\n",
    "task_id=361072\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 361072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n"
     ]
    }
   ],
   "source": [
    "# Create the checkpoint directory if it doesn't exist\n",
    "os.makedirs('CHECKPOINTS/CLUSTERING', exist_ok=True)\n",
    "CHECKPOINT_PATH = f'CHECKPOINTS/CLUSTERING/task_{task_id}.pt'\n",
    "\n",
    "print(f\"Task {task_id}\")\n",
    "\n",
    "task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "dataset = task.get_dataset()\n",
    "\n",
    "X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "N_TRIALS=2\n",
    "N_SAMPLES=100\n",
    "PATIENCE=40\n",
    "N_EPOCHS=1000\n",
    "GP_ITERATIONS=1000\n",
    "BATCH_SIZE=1024\n",
    "seed=10\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "# New new implementation\n",
    "N_CLUSTERS=20\n",
    "# calculate the mean and covariance matrix of the dataset\n",
    "mean = np.mean(X, axis=0)\n",
    "cov = np.cov(X.T)\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# transform data to compute the clusters\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=0, n_init=\"auto\").fit(X_scaled)\n",
    "distances=[]\n",
    "mahalanobis_dist=[]\n",
    "counts=[]\n",
    "ideal_len=len(kmeans.labels_)/5\n",
    "for i in np.arange(N_CLUSTERS):\n",
    "    distances.append(np.abs(np.sum(kmeans.labels_==i)-ideal_len))\n",
    "    counts.append(np.sum(kmeans.labels_==i))\n",
    "    mean_k= np.mean(X.loc[kmeans.labels_==i,:], axis=0)\n",
    "    mahalanobis_dist.append(mahalanobis(mean_k, mean, np.linalg.inv(cov)))\n",
    "\n",
    "dist_df=pd.DataFrame(data={'mahalanobis_dist': mahalanobis_dist, 'count': counts}, index=np.arange(N_CLUSTERS))\n",
    "dist_df=dist_df.sort_values('mahalanobis_dist', ascending=False)\n",
    "dist_df['cumulative_count']=dist_df['count'].cumsum()\n",
    "dist_df['abs_diff']=np.abs(dist_df['cumulative_count']-ideal_len)\n",
    "\n",
    "final=(np.where(dist_df['abs_diff']==np.min(dist_df['abs_diff']))[0])[0]\n",
    "labelss=dist_df.index[0:final+1].to_list()\n",
    "labels=pd.Series(kmeans.labels_).isin(labelss)\n",
    "labels.index=X.index\n",
    "close_index=labels.index[np.where(labels==False)[0]]\n",
    "far_index=labels.index[np.where(labels==True)[0]]\n",
    "\n",
    "X_train = X.loc[close_index,:]\n",
    "X_test = X.loc[far_index,:]\n",
    "y_train = y.loc[close_index]\n",
    "y_test = y.loc[far_index]\n",
    "\n",
    "# calculate the mean and covariance matrix of the dataset\n",
    "mean_ = np.mean(X_train, axis=0)\n",
    "cov_ = np.cov(X_train.T)\n",
    "scaler_ = StandardScaler()\n",
    "\n",
    "# transform data to compute the clusters\n",
    "X_train_scaled = scaler_.fit_transform(X_train)\n",
    "\n",
    "kmeans_ = KMeans(n_clusters=N_CLUSTERS, random_state=0, n_init=\"auto\").fit(X_train_scaled)\n",
    "distances_=[]\n",
    "counts_=[]\n",
    "mahalanobis_dist_=[]\n",
    "ideal_len_=len(kmeans_.labels_)/5\n",
    "for i in np.arange(N_CLUSTERS):\n",
    "    distances_.append(np.abs(np.sum(kmeans_.labels_==i)-ideal_len_))\n",
    "    counts_.append(np.sum(kmeans_.labels_==i))\n",
    "    mean_k_= np.mean(X_train.loc[kmeans_.labels_==i,:], axis=0)\n",
    "    mahalanobis_dist_.append(mahalanobis(mean_k_, mean_, np.linalg.inv(cov_)))\n",
    "\n",
    "dist_df_=pd.DataFrame(data={'mahalanobis_dist': mahalanobis_dist_, 'count': counts_}, index=np.arange(N_CLUSTERS))\n",
    "dist_df_=dist_df_.sort_values('mahalanobis_dist', ascending=False)\n",
    "dist_df_['cumulative_count']=dist_df_['count'].cumsum()\n",
    "dist_df_['abs_diff']=np.abs(dist_df_['cumulative_count']-ideal_len_)\n",
    "\n",
    "final_=(np.where(dist_df_['abs_diff']==np.min(dist_df_['abs_diff']))[0])[0]\n",
    "labelss_=dist_df_.index[0:final_+1].to_list()\n",
    "labels_=pd.Series(kmeans_.labels_).isin(labelss_)\n",
    "labels_.index=X_train.index\n",
    "close_index_=labels_.index[np.where(labels_==False)[0]]\n",
    "far_index_=labels_.index[np.where(labels_==True)[0]]\n",
    "\n",
    "X_train_ = X_train.loc[close_index_,:]\n",
    "X_val = X_train.loc[far_index_,:]\n",
    "y_train_ = y_train.loc[close_index_]\n",
    "y_val = y_train.loc[far_index_]\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train__tensor = torch.tensor(X_train_.values, dtype=torch.float32)\n",
    "y_train__tensor = torch.tensor(y_train_.values, dtype=torch.float32)\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Convert to use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    X_train__tensor = X_train__tensor.cuda()\n",
    "    y_train__tensor = y_train__tensor.cuda()\n",
    "    X_train_tensor = X_train_tensor.cuda()\n",
    "    y_train_tensor = y_train_tensor.cuda()\n",
    "    X_val_tensor = X_val_tensor.cuda()\n",
    "    y_val_tensor = y_val_tensor.cuda()\n",
    "    X_test_tensor = X_test_tensor.cuda()\n",
    "    y_test_tensor = y_test_tensor.cuda()\n",
    "\n",
    "# Create flattened versions of the data\n",
    "y_val_np = y_val.values.flatten()\n",
    "y_test_np = y_test.values.flatten()\n",
    "\n",
    "# Create TensorDatasets for training and validation sets\n",
    "train__dataset = TensorDataset(X_train__tensor, y_train__tensor)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders for training and validation sets\n",
    "train__loader = DataLoader(train__dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-04 19:02:45,213] A new study created in memory with name: no-name-356d681b-1170-40ba-b0c6-b3f0b278d480\n",
      "[I 2024-03-04 19:02:52,404] Trial 0 finished with value: 6.937608925970823 and parameters: {'learning_rate': 0.0713003929222653, 'n_estimators': 108, 'reg_lambda': 0.005044685709888605, 'max_depth': 23, 'min_child_samples': 55}. Best is trial 0 with value: 6.937608925970823.\n",
      "[I 2024-03-04 19:02:55,046] Trial 1 finished with value: 12.527109038809398 and parameters: {'learning_rate': 0.0006784471913345375, 'n_estimators': 179, 'reg_lambda': 0.0699481785242808, 'max_depth': 6, 'min_child_samples': 18}. Best is trial 0 with value: 6.937608925970823.\n",
      "[I 2024-03-04 19:02:55,049] A new study created in memory with name: no-name-ea90304b-f111-4642-bd43-36333a33cebc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.771320643266746, 0.0207519493594015, 0.6336482349262754, 0.7488038825386119, 0.4985070123025904, 0.22479664553084766, 0.19806286475962398, 0.7605307121989587, 0.16911083656253545, 0.08833981417401027, 0.6853598183677972, 0.9533933461949365, 0.003948266327914451, 0.5121922633857766, 0.8126209616521135, 0.6125260668293881, 0.7217553174317995, 0.29187606817063316, 0.9177741225129434, 0.7145757833976906, 0.5425443680112613, 0.14217004760152696, 0.3733407600514692, 0.6741336150663453, 0.4418331744229961, 0.4340139933332937, 0.6177669784693172, 0.5131382425543909, 0.6503971819314672, 0.6010389534045444, 0.8052231968327465, 0.5216471523936341, 0.9086488808086682, 0.3192360889885453, 0.09045934927090737, 0.30070005663620336, 0.11398436186354977, 0.8286813263076767, 0.04689631938924976, 0.6262871483113925, 0.5475861559192435, 0.8192869956700687, 0.1989475396788123, 0.8568503024577332, 0.3516526394320879, 0.7546476915298572, 0.2959617068796787, 0.8839364795611863, 0.3255116378322488, 0.16501589771914849, 0.3925292439465873, 0.0934603745586503, 0.8211056578369285, 0.15115201964256386, 0.3841144486921996, 0.9442607122388011, 0.9876254749018722, 0.4563045470947841, 0.8261228438427398, 0.25137413420705934, 0.5973716482308843, 0.9028317603316274, 0.5345579488018151, 0.5902013629854229, 0.03928176722538734, 0.3571817586345363, 0.07961309015596418, 0.30545991834281827, 0.330719311982132, 0.7738302962105958, 0.039959208689977266, 0.42949217843163834, 0.3149268718426883, 0.6364911430675446, 0.34634715008003303, 0.04309735620499444, 0.879915174517916, 0.763240587143681, 0.8780966427248583, 0.41750914383926696, 0.6055775643937568, 0.5134666274082884, 0.5978366479629736, 0.2622156611319503, 0.30087130894070724, 0.025399782050106068, 0.30306256065103476, 0.24207587540352737, 0.5575781886626442, 0.5655070198881675, 0.47513224741505056, 0.2927979762895091, 0.06425106069482445, 0.9788191457576426, 0.33970784363786366, 0.4950486308824543, 0.9770807259226818, 0.4407738249006665, 0.3182728054789512, 0.5197969858753801]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-04 19:03:00,524] Trial 0 finished with value: 9.07003089184482 and parameters: {'num_trees': 409, 'mtry': 1, 'min_node_size': 67}. Best is trial 0 with value: 9.07003089184482.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.771320643266746, 0.0207519493594015, 0.6336482349262754, 0.7488038825386119, 0.4985070123025904, 0.22479664553084766, 0.19806286475962398, 0.7605307121989587, 0.16911083656253545, 0.08833981417401027, 0.6853598183677972, 0.9533933461949365, 0.003948266327914451, 0.5121922633857766, 0.8126209616521135, 0.6125260668293881, 0.7217553174317995, 0.29187606817063316, 0.9177741225129434, 0.7145757833976906, 0.5425443680112613, 0.14217004760152696, 0.3733407600514692, 0.6741336150663453, 0.4418331744229961, 0.4340139933332937, 0.6177669784693172, 0.5131382425543909, 0.6503971819314672, 0.6010389534045444, 0.8052231968327465, 0.5216471523936341, 0.9086488808086682, 0.3192360889885453, 0.09045934927090737, 0.30070005663620336, 0.11398436186354977, 0.8286813263076767, 0.04689631938924976, 0.6262871483113925, 0.5475861559192435, 0.8192869956700687, 0.1989475396788123, 0.8568503024577332, 0.3516526394320879, 0.7546476915298572, 0.2959617068796787, 0.8839364795611863, 0.3255116378322488, 0.16501589771914849, 0.3925292439465873, 0.0934603745586503, 0.8211056578369285, 0.15115201964256386, 0.3841144486921996, 0.9442607122388011, 0.9876254749018722, 0.4563045470947841, 0.8261228438427398, 0.25137413420705934, 0.5973716482308843, 0.9028317603316274, 0.5345579488018151, 0.5902013629854229, 0.03928176722538734, 0.3571817586345363, 0.07961309015596418, 0.30545991834281827, 0.330719311982132, 0.7738302962105958, 0.039959208689977266, 0.42949217843163834, 0.3149268718426883, 0.6364911430675446, 0.34634715008003303, 0.04309735620499444, 0.879915174517916, 0.763240587143681, 0.8780966427248583, 0.41750914383926696, 0.6055775643937568, 0.5134666274082884, 0.5978366479629736, 0.2622156611319503, 0.30087130894070724, 0.025399782050106068, 0.30306256065103476, 0.24207587540352737, 0.5575781886626442, 0.5655070198881675, 0.47513224741505056, 0.2927979762895091, 0.06425106069482445, 0.9788191457576426, 0.33970784363786366, 0.4950486308824543, 0.9770807259226818, 0.4407738249006665, 0.3182728054789512, 0.5197969858753801]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-04 19:03:08,247] Trial 1 finished with value: 7.275542030588492 and parameters: {'num_trees': 400, 'mtry': 15, 'min_node_size': 30}. Best is trial 1 with value: 7.275542030588492.\n",
      "[I 2024-03-04 19:03:08,250] A new study created in memory with name: no-name-7a4ea8ea-e886-40fc-89d9-9a08012593dc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Residual blocks (skip-connections) are typically recommended for more than 2 layers; turn it on by setting resblock=True.\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1024.\n",
      "[Epoch 1 (0%), batch 6] energy-loss: 0.4376,  E(|Y-Yhat|): 1.0720,  E(|Yhat-Yhat'|): 1.2687\n",
      "[Epoch 100 (84%), batch 6] energy-loss: 0.2423,  E(|Y-Yhat|): 0.4711,  E(|Yhat-Yhat'|): 0.4575\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.1010,  E(|Y-Yhat|): 2.3519,  E(|Yhat-Yhat'|): 2.5017\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-04 19:04:49,599] Trial 0 finished with value: 3.041407315526319 and parameters: {'learning_rate': 0.0034885205571560775, 'num_epoches': 118, 'num_layer': 4, 'hidden_dim': 400, 'resblock': True}. Best is trial 0 with value: 3.041407315526319.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1024.\n",
      "[Epoch 1 (0%), batch 6] energy-loss: 0.6360,  E(|Y-Yhat|): 0.8290,  E(|Yhat-Yhat'|): 0.3860\n",
      "[Epoch 100 (13%), batch 6] energy-loss: 0.2307,  E(|Y-Yhat|): 0.4422,  E(|Yhat-Yhat'|): 0.4230\n",
      "[Epoch 200 (25%), batch 6] energy-loss: 0.2261,  E(|Y-Yhat|): 0.4416,  E(|Yhat-Yhat'|): 0.4311\n",
      "[Epoch 300 (38%), batch 6] energy-loss: 0.2274,  E(|Y-Yhat|): 0.4418,  E(|Yhat-Yhat'|): 0.4289\n",
      "[Epoch 400 (51%), batch 6] energy-loss: 0.2288,  E(|Y-Yhat|): 0.4356,  E(|Yhat-Yhat'|): 0.4136\n",
      "[Epoch 500 (64%), batch 6] energy-loss: 0.2395,  E(|Y-Yhat|): 0.4487,  E(|Yhat-Yhat'|): 0.4183\n",
      "[Epoch 600 (76%), batch 6] energy-loss: 0.2258,  E(|Y-Yhat|): 0.4368,  E(|Yhat-Yhat'|): 0.4220\n",
      "[Epoch 700 (89%), batch 6] energy-loss: 0.1808,  E(|Y-Yhat|): 0.4118,  E(|Yhat-Yhat'|): 0.4620\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0890,  E(|Y-Yhat|): 2.1953,  E(|Yhat-Yhat'|): 2.2128\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-04 19:06:17,801] Trial 1 finished with value: 4.233781359117593 and parameters: {'learning_rate': 0.0002489577954043506, 'num_epoches': 785, 'num_layer': 2, 'hidden_dim': 135, 'resblock': False}. Best is trial 0 with value: 3.041407315526319.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.771320643266746, 0.0207519493594015, 0.6336482349262754, 0.7488038825386119, 0.4985070123025904, 0.22479664553084766, 0.19806286475962398, 0.7605307121989587, 0.16911083656253545, 0.08833981417401027, 0.6853598183677972, 0.9533933461949365, 0.003948266327914451, 0.5121922633857766, 0.8126209616521135, 0.6125260668293881, 0.7217553174317995, 0.29187606817063316, 0.9177741225129434, 0.7145757833976906, 0.5425443680112613, 0.14217004760152696, 0.3733407600514692, 0.6741336150663453, 0.4418331744229961, 0.4340139933332937, 0.6177669784693172, 0.5131382425543909, 0.6503971819314672, 0.6010389534045444, 0.8052231968327465, 0.5216471523936341, 0.9086488808086682, 0.3192360889885453, 0.09045934927090737, 0.30070005663620336, 0.11398436186354977, 0.8286813263076767, 0.04689631938924976, 0.6262871483113925, 0.5475861559192435, 0.8192869956700687, 0.1989475396788123, 0.8568503024577332, 0.3516526394320879, 0.7546476915298572, 0.2959617068796787, 0.8839364795611863, 0.3255116378322488, 0.16501589771914849, 0.3925292439465873, 0.0934603745586503, 0.8211056578369285, 0.15115201964256386, 0.3841144486921996, 0.9442607122388011, 0.9876254749018722, 0.4563045470947841, 0.8261228438427398, 0.25137413420705934, 0.5973716482308843, 0.9028317603316274, 0.5345579488018151, 0.5902013629854229, 0.03928176722538734, 0.3571817586345363, 0.07961309015596418, 0.30545991834281827, 0.330719311982132, 0.7738302962105958, 0.039959208689977266, 0.42949217843163834, 0.3149268718426883, 0.6364911430675446, 0.34634715008003303, 0.04309735620499444, 0.879915174517916, 0.763240587143681, 0.8780966427248583, 0.41750914383926696, 0.6055775643937568, 0.5134666274082884, 0.5978366479629736, 0.2622156611319503, 0.30087130894070724, 0.025399782050106068, 0.30306256065103476, 0.24207587540352737, 0.5575781886626442, 0.5655070198881675, 0.47513224741505056, 0.2927979762895091, 0.06425106069482445, 0.9788191457576426, 0.33970784363786366, 0.4950486308824543, 0.9770807259226818, 0.4407738249006665, 0.3182728054789512, 0.5197969858753801]\n",
      "Running on CPU.\n",
      "\n",
      "Residual blocks (skip-connections) are typically recommended for more than 2 layers; turn it on by setting resblock=True.\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1024.\n",
      "[Epoch 1 (0%), batch 7] energy-loss: 0.3068,  E(|Y-Yhat|): 0.8871,  E(|Yhat-Yhat'|): 1.1607\n",
      "[Epoch 100 (84%), batch 7] energy-loss: 0.1209,  E(|Y-Yhat|): 0.2849,  E(|Yhat-Yhat'|): 0.3282\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.2372,  E(|Y-Yhat|): 2.8857,  E(|Yhat-Yhat'|): 3.2971\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "CRPS linear regression:  30.335993731879395\n",
      "CRPS boosted trees 9.56145418608216\n",
      "CRPS random forest 13.631153757122418\n",
      "CRPS engression 9.167586232235394\n"
     ]
    }
   ],
   "source": [
    "dtrain_ = lgb.Dataset(torch.tensor(X_train_.values, dtype=torch.float32).clone().detach(), label=y_train_.values)\n",
    "\n",
    "def boosted(trial):\n",
    "\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.5, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 30),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "    }\n",
    "    opt_params = params.copy()\n",
    "    n_rounds = opt_params[\"n_estimators\"]\n",
    "    del opt_params[\"n_estimators\"]\n",
    "    opt_params['feature_pre_filter']=False\n",
    "\n",
    "    # Use LightGBMLossGuideRegressor for distributional prediction\n",
    "    boosted_tree_model = LightGBMLSS(Gaussian(stabilization=\"None\", response_fn=\"exp\", loss_fn=\"nll\"))\n",
    "    boosted_tree_model.train(opt_params, dtrain_, num_boost_round=n_rounds)\n",
    "\n",
    "    # Predict both the mean and standard deviation\n",
    "    pred_params=boosted_tree_model.predict(X_val, pred_type=\"parameters\")\n",
    "    y_val_hat_boost=pred_params['loc']\n",
    "    y_val_hat_std = pred_params['scale']\n",
    "\n",
    "    # Calculate the CRPS for each prediction\n",
    "    crps_values = [crps_gaussian(y_val_np[i], mu=y_val_hat_boost[i], sig=y_val_hat_std[i]) for i in range(len(y_val))]\n",
    "\n",
    "    # Return the mean CRPS as the objective to be minimized\n",
    "    return np.mean(crps_values)\n",
    "\n",
    "sampler_boost = optuna.samplers.TPESampler(seed=seed)\n",
    "study_boost = optuna.create_study(sampler=sampler_boost, direction='minimize')\n",
    "study_boost.optimize(boosted, n_trials=N_TRIALS)\n",
    "\n",
    "np.random.seed(seed)\n",
    "quantiles=list(np.random.uniform(0,1,N_SAMPLES))\n",
    "def rf(trial):\n",
    "    params = {'num_trees': trial.suggest_int('num_trees', 100, 500),\n",
    "        'mtry': trial.suggest_int('mtry', 1, 30),\n",
    "        'min_node_size': trial.suggest_int('min_node_size', 10, 100)}\n",
    "    \n",
    "    drf_model = drf(**params, seed=seed)\n",
    "    drf_model.fit(X_train_, y_train_)\n",
    "    \n",
    "    # Generate a sample from the drf model for each data point\n",
    "    y_val_hat=drf_model.predict(newdata = X_val, functional = \"quantile\", quantiles=quantiles)\n",
    "\n",
    "    # Calculate the CRPS for each prediction\n",
    "    crps_values = [crps_ensemble(y_val_np[i], y_val_hat.quantile[i].reshape(-1)) for i in range(len(y_val_np))]\n",
    "\n",
    "    # Return the mean CRPS as the objective to be minimized\n",
    "    return np.mean(crps_values)\n",
    "\n",
    "sampler_drf = optuna.samplers.TPESampler(seed=seed)\n",
    "study_drf = optuna.create_study(sampler=sampler_drf, direction='minimize')\n",
    "study_drf.optimize(rf, n_trials=N_TRIALS)\n",
    "\n",
    "\n",
    "def engressor_NN(trial):\n",
    "\n",
    "    params = {'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.01, log=True),\n",
    "            'num_epoches': trial.suggest_int('num_epoches', 100, 1000),\n",
    "            'num_layer': trial.suggest_int('num_layer', 2, 5),\n",
    "            'hidden_dim': trial.suggest_int('hidden_dim', 100, 500),\n",
    "            'resblock': trial.suggest_categorical('resblock', [True, False])}\n",
    "    params['noise_dim']=params['hidden_dim']\n",
    "\n",
    "    # Check if CUDA is available and if so, move the tensors and the model to the GPU\n",
    "    if torch.cuda.is_available():\n",
    "        engressor_model=engression(X_train__tensor, y_train__tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'], device=\"cuda\")\n",
    "    else:\n",
    "        engressor_model=engression(X_train__tensor, y_train__tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'])\n",
    "    \n",
    "    # Generate a sample from the engression model for each data point\n",
    "    y_val_hat_engression_samples = [engressor_model.sample(torch.Tensor(np.array([X_val.values[i]])), sample_size=N_SAMPLES) for i in range(len(X_val))]\n",
    "\n",
    "    # Calculate the CRPS for each prediction\n",
    "    crps_values = [crps_ensemble(y_val_np[i], np.array(y_val_hat_engression_samples[i]).reshape(-1,)) for i in range(len(y_val_np))]\n",
    "\n",
    "    return np.mean(crps_values)\n",
    "\n",
    "sampler_engression = optuna.samplers.TPESampler(seed=seed)\n",
    "study_engression = optuna.create_study(sampler=sampler_engression, direction='minimize')\n",
    "study_engression.optimize(engressor_NN, n_trials=N_TRIALS)\n",
    "\n",
    "\n",
    "dtrain = lgb.Dataset(torch.tensor(X_train.values, dtype=torch.float32).clone().detach(), label=y_train.values)\n",
    "opt_params = study_boost.best_params.copy()\n",
    "n_rounds = opt_params[\"n_estimators\"]\n",
    "del opt_params[\"n_estimators\"]\n",
    "opt_params['feature_pre_filter']=False\n",
    "# Use LightGBMLossGuideRegressor for distributional prediction\n",
    "boosted_tree_model = LightGBMLSS(Gaussian(stabilization=\"None\", response_fn=\"exp\", loss_fn=\"nll\"))\n",
    "boosted_tree_model.train(opt_params, dtrain, num_boost_round=n_rounds)\n",
    "# Predict both the mean and standard deviation\n",
    "pred_params=boosted_tree_model.predict(X_test, pred_type=\"parameters\")\n",
    "y_test_hat_boost=pred_params['loc']\n",
    "y_test_hat_std = pred_params['scale']\n",
    "# Calculate the CRPS for each prediction\n",
    "crps_values = [crps_gaussian(y_test_np[i], mu=y_test_hat_boost[i], sig=y_test_hat_std[i]) for i in range(len(y_test))]\n",
    "# Return the mean CRPS as the objective to be minimized\n",
    "CRPS_boosted=np.mean(crps_values)\n",
    "\n",
    "drf_model=drf(**study_drf.best_params, seed=seed)\n",
    "drf_model.fit(X_train, y_train)\n",
    "# Generate a sample from the drf model for each data point\n",
    "y_test_hat_drf=drf_model.predict(newdata = X_test, functional = \"quantile\", quantiles=quantiles)\n",
    "# Calculate the CRPS for each prediction\n",
    "crps_values = [crps_ensemble(y_test_np[i], y_test_hat_drf.quantile[i].reshape(-1)) for i in range(len(y_test_np))]\n",
    "# Return the mean CRPS as the objective to be minimized\n",
    "CRPS_rf=np.mean(crps_values)\n",
    "\n",
    "lin_reg=LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "y_test_hat_linreg=lin_reg.predict(X_test)\n",
    "# Calculate the standard deviation of the residuals\n",
    "std_dev = np.std(y_test - y_test_hat_linreg)\n",
    "# Calculate the CRPS for each prediction\n",
    "crps_values = [crps_gaussian(y_test_np[i], mu=y_test_hat_linreg[i], sig=std_dev) for i in range(len(y_test_np))]\n",
    "CRPS_linreg = np.mean(crps_values)\n",
    "\n",
    "params=study_engression.best_params\n",
    "params['noise_dim']=params['hidden_dim']\n",
    "X_train_tensor = torch.Tensor(np.array(X_train))\n",
    "y_train_tensor = torch.Tensor(np.array(y_train).reshape(-1,1))\n",
    "\n",
    "# Check if CUDA is available and if so, move the tensors and the model to the GPU\n",
    "if torch.cuda.is_available():\n",
    "    engressor_model=engression(X_train_tensor, y_train_tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'], device=\"cuda\")\n",
    "else:\n",
    "    engressor_model=engression(X_train_tensor, y_train_tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'])\n",
    "# Generate a sample from the engression model for each data point\n",
    "y_test_hat_engression_samples = [engressor_model.sample(torch.Tensor(np.array([X_test.values[i]])).cuda() if torch.cuda.is_available() else torch.Tensor(np.array([X_test.values[i]])), sample_size=N_SAMPLES) for i in range(len(X_test))]\n",
    "# Calculate the CRPS for each prediction\n",
    "crps_values = [crps_ensemble(y_test_np[i], np.array(y_test_hat_engression_samples[i]).reshape(-1,)) for i in range(len(y_test_np))]\n",
    "CRPS_engression=np.mean(crps_values)\n",
    "\n",
    "print(\"CRPS linear regression: \",CRPS_linreg)\n",
    "print(\"CRPS boosted trees\", CRPS_boosted)\n",
    "print(\"CRPS random forest\", CRPS_rf)\n",
    "print(\"CRPS engression\", CRPS_engression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-04 19:08:30,657] A new study created in memory with name: no-name-e2a9d29b-e253-4507-af13-0e3a103bf9bd\n",
      "[I 2024-03-04 19:08:33,473] Trial 0 finished with value: 6.937608925970823 and parameters: {'learning_rate': 0.0713003929222653, 'n_estimators': 108, 'reg_lambda': 0.005044685709888605, 'max_depth': 23, 'min_child_samples': 55}. Best is trial 0 with value: 6.937608925970823.\n",
      "[I 2024-03-04 19:08:36,708] Trial 1 finished with value: 12.527109038809398 and parameters: {'learning_rate': 0.0006784471913345375, 'n_estimators': 179, 'reg_lambda': 0.0699481785242808, 'max_depth': 6, 'min_child_samples': 18}. Best is trial 0 with value: 6.937608925970823.\n",
      "[I 2024-03-04 19:08:36,711] A new study created in memory with name: no-name-4f377d26-6161-4e4f-8457-c817e189b3ef\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.771320643266746, 0.0207519493594015, 0.6336482349262754, 0.7488038825386119, 0.4985070123025904, 0.22479664553084766, 0.19806286475962398, 0.7605307121989587, 0.16911083656253545, 0.08833981417401027, 0.6853598183677972, 0.9533933461949365, 0.003948266327914451, 0.5121922633857766, 0.8126209616521135, 0.6125260668293881, 0.7217553174317995, 0.29187606817063316, 0.9177741225129434, 0.7145757833976906, 0.5425443680112613, 0.14217004760152696, 0.3733407600514692, 0.6741336150663453, 0.4418331744229961, 0.4340139933332937, 0.6177669784693172, 0.5131382425543909, 0.6503971819314672, 0.6010389534045444, 0.8052231968327465, 0.5216471523936341, 0.9086488808086682, 0.3192360889885453, 0.09045934927090737, 0.30070005663620336, 0.11398436186354977, 0.8286813263076767, 0.04689631938924976, 0.6262871483113925, 0.5475861559192435, 0.8192869956700687, 0.1989475396788123, 0.8568503024577332, 0.3516526394320879, 0.7546476915298572, 0.2959617068796787, 0.8839364795611863, 0.3255116378322488, 0.16501589771914849, 0.3925292439465873, 0.0934603745586503, 0.8211056578369285, 0.15115201964256386, 0.3841144486921996, 0.9442607122388011, 0.9876254749018722, 0.4563045470947841, 0.8261228438427398, 0.25137413420705934, 0.5973716482308843, 0.9028317603316274, 0.5345579488018151, 0.5902013629854229, 0.03928176722538734, 0.3571817586345363, 0.07961309015596418, 0.30545991834281827, 0.330719311982132, 0.7738302962105958, 0.039959208689977266, 0.42949217843163834, 0.3149268718426883, 0.6364911430675446, 0.34634715008003303, 0.04309735620499444, 0.879915174517916, 0.763240587143681, 0.8780966427248583, 0.41750914383926696, 0.6055775643937568, 0.5134666274082884, 0.5978366479629736, 0.2622156611319503, 0.30087130894070724, 0.025399782050106068, 0.30306256065103476, 0.24207587540352737, 0.5575781886626442, 0.5655070198881675, 0.47513224741505056, 0.2927979762895091, 0.06425106069482445, 0.9788191457576426, 0.33970784363786366, 0.4950486308824543, 0.9770807259226818, 0.4407738249006665, 0.3182728054789512, 0.5197969858753801]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-04 19:08:47,006] Trial 0 finished with value: 9.07003089184482 and parameters: {'num_trees': 409, 'mtry': 1, 'min_node_size': 67}. Best is trial 0 with value: 9.07003089184482.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.771320643266746, 0.0207519493594015, 0.6336482349262754, 0.7488038825386119, 0.4985070123025904, 0.22479664553084766, 0.19806286475962398, 0.7605307121989587, 0.16911083656253545, 0.08833981417401027, 0.6853598183677972, 0.9533933461949365, 0.003948266327914451, 0.5121922633857766, 0.8126209616521135, 0.6125260668293881, 0.7217553174317995, 0.29187606817063316, 0.9177741225129434, 0.7145757833976906, 0.5425443680112613, 0.14217004760152696, 0.3733407600514692, 0.6741336150663453, 0.4418331744229961, 0.4340139933332937, 0.6177669784693172, 0.5131382425543909, 0.6503971819314672, 0.6010389534045444, 0.8052231968327465, 0.5216471523936341, 0.9086488808086682, 0.3192360889885453, 0.09045934927090737, 0.30070005663620336, 0.11398436186354977, 0.8286813263076767, 0.04689631938924976, 0.6262871483113925, 0.5475861559192435, 0.8192869956700687, 0.1989475396788123, 0.8568503024577332, 0.3516526394320879, 0.7546476915298572, 0.2959617068796787, 0.8839364795611863, 0.3255116378322488, 0.16501589771914849, 0.3925292439465873, 0.0934603745586503, 0.8211056578369285, 0.15115201964256386, 0.3841144486921996, 0.9442607122388011, 0.9876254749018722, 0.4563045470947841, 0.8261228438427398, 0.25137413420705934, 0.5973716482308843, 0.9028317603316274, 0.5345579488018151, 0.5902013629854229, 0.03928176722538734, 0.3571817586345363, 0.07961309015596418, 0.30545991834281827, 0.330719311982132, 0.7738302962105958, 0.039959208689977266, 0.42949217843163834, 0.3149268718426883, 0.6364911430675446, 0.34634715008003303, 0.04309735620499444, 0.879915174517916, 0.763240587143681, 0.8780966427248583, 0.41750914383926696, 0.6055775643937568, 0.5134666274082884, 0.5978366479629736, 0.2622156611319503, 0.30087130894070724, 0.025399782050106068, 0.30306256065103476, 0.24207587540352737, 0.5575781886626442, 0.5655070198881675, 0.47513224741505056, 0.2927979762895091, 0.06425106069482445, 0.9788191457576426, 0.33970784363786366, 0.4950486308824543, 0.9770807259226818, 0.4407738249006665, 0.3182728054789512, 0.5197969858753801]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-04 19:08:59,240] Trial 1 finished with value: 7.275542030588492 and parameters: {'num_trees': 400, 'mtry': 15, 'min_node_size': 30}. Best is trial 1 with value: 7.275542030588492.\n",
      "[I 2024-03-04 19:08:59,270] A new study created in memory with name: no-name-53c4582e-504f-440f-9a36-d447215e770f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Residual blocks (skip-connections) are typically recommended for more than 2 layers; turn it on by setting resblock=True.\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1024.\n",
      "[Epoch 1 (0%), batch 6] energy-loss: 0.4376,  E(|Y-Yhat|): 1.0720,  E(|Yhat-Yhat'|): 1.2687\n",
      "[Epoch 100 (84%), batch 6] energy-loss: 0.2423,  E(|Y-Yhat|): 0.4711,  E(|Yhat-Yhat'|): 0.4575\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.1010,  E(|Y-Yhat|): 2.3519,  E(|Yhat-Yhat'|): 2.5017\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-04 19:11:02,423] Trial 0 finished with value: 3.041407315526319 and parameters: {'learning_rate': 0.0034885205571560775, 'num_epoches': 118, 'num_layer': 4, 'hidden_dim': 400, 'resblock': True}. Best is trial 0 with value: 3.041407315526319.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1024.\n",
      "[Epoch 1 (0%), batch 6] energy-loss: 0.6360,  E(|Y-Yhat|): 0.8290,  E(|Yhat-Yhat'|): 0.3860\n",
      "[Epoch 100 (13%), batch 6] energy-loss: 0.2307,  E(|Y-Yhat|): 0.4422,  E(|Yhat-Yhat'|): 0.4230\n",
      "[Epoch 200 (25%), batch 6] energy-loss: 0.2261,  E(|Y-Yhat|): 0.4416,  E(|Yhat-Yhat'|): 0.4311\n",
      "[Epoch 300 (38%), batch 6] energy-loss: 0.2274,  E(|Y-Yhat|): 0.4418,  E(|Yhat-Yhat'|): 0.4289\n",
      "[Epoch 400 (51%), batch 6] energy-loss: 0.2288,  E(|Y-Yhat|): 0.4356,  E(|Yhat-Yhat'|): 0.4136\n",
      "[Epoch 500 (64%), batch 6] energy-loss: 0.2395,  E(|Y-Yhat|): 0.4487,  E(|Yhat-Yhat'|): 0.4183\n",
      "[Epoch 600 (76%), batch 6] energy-loss: 0.2258,  E(|Y-Yhat|): 0.4368,  E(|Yhat-Yhat'|): 0.4220\n",
      "[Epoch 700 (89%), batch 6] energy-loss: 0.1808,  E(|Y-Yhat|): 0.4118,  E(|Yhat-Yhat'|): 0.4620\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0890,  E(|Y-Yhat|): 2.1953,  E(|Yhat-Yhat'|): 2.2128\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-04 19:13:02,384] Trial 1 finished with value: 4.233781359117593 and parameters: {'learning_rate': 0.0002489577954043506, 'num_epoches': 785, 'num_layer': 2, 'hidden_dim': 135, 'resblock': False}. Best is trial 0 with value: 3.041407315526319.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.771320643266746, 0.0207519493594015, 0.6336482349262754, 0.7488038825386119, 0.4985070123025904, 0.22479664553084766, 0.19806286475962398, 0.7605307121989587, 0.16911083656253545, 0.08833981417401027, 0.6853598183677972, 0.9533933461949365, 0.003948266327914451, 0.5121922633857766, 0.8126209616521135, 0.6125260668293881, 0.7217553174317995, 0.29187606817063316, 0.9177741225129434, 0.7145757833976906, 0.5425443680112613, 0.14217004760152696, 0.3733407600514692, 0.6741336150663453, 0.4418331744229961, 0.4340139933332937, 0.6177669784693172, 0.5131382425543909, 0.6503971819314672, 0.6010389534045444, 0.8052231968327465, 0.5216471523936341, 0.9086488808086682, 0.3192360889885453, 0.09045934927090737, 0.30070005663620336, 0.11398436186354977, 0.8286813263076767, 0.04689631938924976, 0.6262871483113925, 0.5475861559192435, 0.8192869956700687, 0.1989475396788123, 0.8568503024577332, 0.3516526394320879, 0.7546476915298572, 0.2959617068796787, 0.8839364795611863, 0.3255116378322488, 0.16501589771914849, 0.3925292439465873, 0.0934603745586503, 0.8211056578369285, 0.15115201964256386, 0.3841144486921996, 0.9442607122388011, 0.9876254749018722, 0.4563045470947841, 0.8261228438427398, 0.25137413420705934, 0.5973716482308843, 0.9028317603316274, 0.5345579488018151, 0.5902013629854229, 0.03928176722538734, 0.3571817586345363, 0.07961309015596418, 0.30545991834281827, 0.330719311982132, 0.7738302962105958, 0.039959208689977266, 0.42949217843163834, 0.3149268718426883, 0.6364911430675446, 0.34634715008003303, 0.04309735620499444, 0.879915174517916, 0.763240587143681, 0.8780966427248583, 0.41750914383926696, 0.6055775643937568, 0.5134666274082884, 0.5978366479629736, 0.2622156611319503, 0.30087130894070724, 0.025399782050106068, 0.30306256065103476, 0.24207587540352737, 0.5575781886626442, 0.5655070198881675, 0.47513224741505056, 0.2927979762895091, 0.06425106069482445, 0.9788191457576426, 0.33970784363786366, 0.4950486308824543, 0.9770807259226818, 0.4407738249006665, 0.3182728054789512, 0.5197969858753801]\n",
      "Running on CPU.\n",
      "\n",
      "Residual blocks (skip-connections) are typically recommended for more than 2 layers; turn it on by setting resblock=True.\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1024.\n",
      "[Epoch 1 (0%), batch 7] energy-loss: 0.3068,  E(|Y-Yhat|): 0.8871,  E(|Yhat-Yhat'|): 1.1607\n",
      "[Epoch 100 (84%), batch 7] energy-loss: 0.1209,  E(|Y-Yhat|): 0.2849,  E(|Yhat-Yhat'|): 0.3282\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.2372,  E(|Y-Yhat|): 2.8857,  E(|Yhat-Yhat'|): 3.2971\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "CRPS linear regression:  30.335993731879395\n",
      "CRPS boosted trees 9.56145418608216\n",
      "CRPS random forest 13.631153757122418\n",
      "CRPS engression 9.167586232235394\n"
     ]
    }
   ],
   "source": [
    "dtrain_ = lgb.Dataset(torch.tensor(X_train_.values, dtype=torch.float32).clone().detach(), label=y_train_.values)\n",
    "\n",
    "def boosted(trial):\n",
    "\n",
    "    params = {\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.5, log=True),\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 1, 30),\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 10, 100),\n",
    "    }\n",
    "    opt_params = params.copy()\n",
    "    n_rounds = opt_params[\"n_estimators\"]\n",
    "    del opt_params[\"n_estimators\"]\n",
    "    opt_params['feature_pre_filter']=False\n",
    "\n",
    "    # Use LightGBMLossGuideRegressor for distributional prediction\n",
    "    boosted_tree_model = LightGBMLSS(Gaussian(stabilization=\"None\", response_fn=\"exp\", loss_fn=\"nll\"))\n",
    "    boosted_tree_model.train(opt_params, dtrain_, num_boost_round=n_rounds)\n",
    "\n",
    "    # Predict both the mean and standard deviation\n",
    "    pred_params=boosted_tree_model.predict(X_val, pred_type=\"parameters\")\n",
    "    y_val_hat_boost=pred_params['loc']\n",
    "    y_val_hat_std = pred_params['scale']\n",
    "\n",
    "    # Calculate the CRPS for each prediction\n",
    "    crps_values = [crps_gaussian(y_val_np[i], mu=y_val_hat_boost[i], sig=y_val_hat_std[i]) for i in range(len(y_val))]\n",
    "\n",
    "    # Return the mean CRPS as the objective to be minimized\n",
    "    return np.mean(crps_values)\n",
    "\n",
    "sampler_boost = optuna.samplers.TPESampler(seed=seed)\n",
    "study_boost = optuna.create_study(sampler=sampler_boost, direction='minimize')\n",
    "study_boost.optimize(boosted, n_trials=N_TRIALS)\n",
    "\n",
    "np.random.seed(seed)\n",
    "quantiles=list(np.random.uniform(0,1,N_SAMPLES))\n",
    "def rf(trial):\n",
    "    params = {'num_trees': trial.suggest_int('num_trees', 100, 500),\n",
    "        'mtry': trial.suggest_int('mtry', 1, 30),\n",
    "        'min_node_size': trial.suggest_int('min_node_size', 10, 100)}\n",
    "    \n",
    "    drf_model = drf(**params, seed=seed)\n",
    "    drf_model.fit(X_train_, y_train_)\n",
    "    \n",
    "    # Generate a sample from the drf model for each data point\n",
    "    y_val_hat=drf_model.predict(newdata = X_val, functional = \"quantile\", quantiles=quantiles)\n",
    "\n",
    "    # Calculate the CRPS for each prediction\n",
    "    crps_values = [crps_ensemble(y_val_np[i], y_val_hat.quantile[i].reshape(-1)) for i in range(len(y_val_np))]\n",
    "\n",
    "    # Return the mean CRPS as the objective to be minimized\n",
    "    return np.mean(crps_values)\n",
    "\n",
    "sampler_drf = optuna.samplers.TPESampler(seed=seed)\n",
    "study_drf = optuna.create_study(sampler=sampler_drf, direction='minimize')\n",
    "study_drf.optimize(rf, n_trials=N_TRIALS)\n",
    "\n",
    "\n",
    "def engressor_NN(trial):\n",
    "\n",
    "    params = {'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.01, log=True),\n",
    "            'num_epoches': trial.suggest_int('num_epoches', 100, 1000),\n",
    "            'num_layer': trial.suggest_int('num_layer', 2, 5),\n",
    "            'hidden_dim': trial.suggest_int('hidden_dim', 100, 500),\n",
    "            'resblock': trial.suggest_categorical('resblock', [True, False])}\n",
    "    params['noise_dim']=params['hidden_dim']\n",
    "\n",
    "    # Check if CUDA is available and if so, move the tensors and the model to the GPU\n",
    "    if torch.cuda.is_available():\n",
    "        engressor_model=engression(X_train__tensor, y_train__tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'], device=\"cuda\")\n",
    "    else:\n",
    "        engressor_model=engression(X_train__tensor, y_train__tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'])\n",
    "    \n",
    "    # Generate a sample from the engression model for each data point\n",
    "    y_val_hat_engression_samples = [engressor_model.sample(torch.Tensor(np.array([X_val.values[i]])), sample_size=N_SAMPLES) for i in range(len(X_val))]\n",
    "\n",
    "    # Calculate the CRPS for each prediction\n",
    "    crps_values = [crps_ensemble(y_val_np[i], np.array(y_val_hat_engression_samples[i]).reshape(-1,)) for i in range(len(y_val_np))]\n",
    "\n",
    "    return np.mean(crps_values)\n",
    "\n",
    "sampler_engression = optuna.samplers.TPESampler(seed=seed)\n",
    "study_engression = optuna.create_study(sampler=sampler_engression, direction='minimize')\n",
    "study_engression.optimize(engressor_NN, n_trials=N_TRIALS)\n",
    "\n",
    "\n",
    "dtrain = lgb.Dataset(torch.tensor(X_train.values, dtype=torch.float32).clone().detach(), label=y_train.values)\n",
    "opt_params = study_boost.best_params.copy()\n",
    "n_rounds = opt_params[\"n_estimators\"]\n",
    "del opt_params[\"n_estimators\"]\n",
    "opt_params['feature_pre_filter']=False\n",
    "# Use LightGBMLossGuideRegressor for distributional prediction\n",
    "boosted_tree_model = LightGBMLSS(Gaussian(stabilization=\"None\", response_fn=\"exp\", loss_fn=\"nll\"))\n",
    "boosted_tree_model.train(opt_params, dtrain, num_boost_round=n_rounds)\n",
    "# Predict both the mean and standard deviation\n",
    "pred_params=boosted_tree_model.predict(X_test, pred_type=\"parameters\")\n",
    "y_test_hat_boost=pred_params['loc']\n",
    "y_test_hat_std = pred_params['scale']\n",
    "# Calculate the CRPS for each prediction\n",
    "crps_values = [crps_gaussian(y_test_np[i], mu=y_test_hat_boost[i], sig=y_test_hat_std[i]) for i in range(len(y_test))]\n",
    "# Return the mean CRPS as the objective to be minimized\n",
    "CRPS_boosted=np.mean(crps_values)\n",
    "\n",
    "drf_model=drf(**study_drf.best_params, seed=seed)\n",
    "drf_model.fit(X_train, y_train)\n",
    "# Generate a sample from the drf model for each data point\n",
    "y_test_hat_drf=drf_model.predict(newdata = X_test, functional = \"quantile\", quantiles=quantiles)\n",
    "# Calculate the CRPS for each prediction\n",
    "crps_values = [crps_ensemble(y_test_np[i], y_test_hat_drf.quantile[i].reshape(-1)) for i in range(len(y_test_np))]\n",
    "# Return the mean CRPS as the objective to be minimized\n",
    "CRPS_rf=np.mean(crps_values)\n",
    "\n",
    "lin_reg=LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "y_test_hat_linreg=lin_reg.predict(X_test)\n",
    "# Calculate the standard deviation of the residuals\n",
    "std_dev = np.std(y_test - y_test_hat_linreg)\n",
    "# Calculate the CRPS for each prediction\n",
    "crps_values = [crps_gaussian(y_test_np[i], mu=y_test_hat_linreg[i], sig=std_dev) for i in range(len(y_test_np))]\n",
    "CRPS_linreg = np.mean(crps_values)\n",
    "\n",
    "params=study_engression.best_params\n",
    "params['noise_dim']=params['hidden_dim']\n",
    "X_train_tensor = torch.Tensor(np.array(X_train))\n",
    "y_train_tensor = torch.Tensor(np.array(y_train).reshape(-1,1))\n",
    "\n",
    "# Check if CUDA is available and if so, move the tensors and the model to the GPU\n",
    "if torch.cuda.is_available():\n",
    "    engressor_model=engression(X_train_tensor, y_train_tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'], device=\"cuda\")\n",
    "else:\n",
    "    engressor_model=engression(X_train_tensor, y_train_tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'])\n",
    "# Generate a sample from the engression model for each data point\n",
    "y_test_hat_engression_samples = [engressor_model.sample(torch.Tensor(np.array([X_test.values[i]])).cuda() if torch.cuda.is_available() else torch.Tensor(np.array([X_test.values[i]])), sample_size=N_SAMPLES) for i in range(len(X_test))]\n",
    "# Calculate the CRPS for each prediction\n",
    "crps_values = [crps_ensemble(y_test_np[i], np.array(y_test_hat_engression_samples[i]).reshape(-1,)) for i in range(len(y_test_np))]\n",
    "CRPS_engression=np.mean(crps_values)\n",
    "\n",
    "print(\"CRPS linear regression: \",CRPS_linreg)\n",
    "print(\"CRPS boosted trees\", CRPS_boosted)\n",
    "print(\"CRPS random forest\", CRPS_rf)\n",
    "print(\"CRPS engression\", CRPS_engression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([85., 81., 87.,  ..., 85., 85., 87.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_val_tensor.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-05 10:01:42,647] A new study created in memory with name: no-name-e547f643-08e0-47ef-9c4c-a9171ff6ac54\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Residual blocks (skip-connections) are typically recommended for more than 2 layers; turn it on by setting resblock=True.\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1024.\n",
      "[Epoch 1 (0%), batch 6] energy-loss: 0.4338,  E(|Y-Yhat|): 1.0227,  E(|Yhat-Yhat'|): 1.1779\n",
      "[Epoch 100 (84%), batch 6] energy-loss: 0.2444,  E(|Y-Yhat|): 0.4991,  E(|Yhat-Yhat'|): 0.5094\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.1617,  E(|Y-Yhat|): 2.4525,  E(|Yhat-Yhat'|): 2.5816\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-05 10:03:14,305] Trial 0 finished with value: 3.997297018729463 and parameters: {'learning_rate': 0.0034885205571560775, 'num_epoches': 118, 'num_layer': 4, 'hidden_dim': 400, 'resblock': True}. Best is trial 0 with value: 3.997297018729463.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1024.\n",
      "[Epoch 1 (0%), batch 6] energy-loss: 0.5307,  E(|Y-Yhat|): 0.7151,  E(|Yhat-Yhat'|): 0.3689\n",
      "[Epoch 100 (13%), batch 6] energy-loss: 0.2189,  E(|Y-Yhat|): 0.4321,  E(|Yhat-Yhat'|): 0.4265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2024-03-05 10:03:29,467] Trial 1 failed with parameters: {'learning_rate': 0.0002489577954043506, 'num_epoches': 785, 'num_layer': 2, 'hidden_dim': 135, 'resblock': False} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\dalma\\AppData\\Local\\Temp\\ipykernel_6472\\2127494930.py\", line 14, in engressor_NN\n",
      "    engressor_model=engression(X_train__tensor, y_train__tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'])\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\engression\\engression.py\", line 47, in engression\n",
      "    engressor.train(x, y, num_epoches=num_epoches, batch_size=batch_size,\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\engression\\engression.py\", line 262, in train\n",
      "    y_sample1 = self.model(x_batch)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\engression\\models.py\", line 223, in forward\n",
      "    return self.out_layer(self.inter_layer(self.input_layer(x)))\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\engression\\models.py\", line 35, in forward\n",
      "    out = self.layer(out)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 114, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "KeyboardInterrupt\n",
      "[W 2024-03-05 10:03:29,504] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 26\u001b[0m\n\u001b[0;32m     24\u001b[0m sampler_engression \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39mseed)\n\u001b[0;32m     25\u001b[0m study_engression \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(sampler\u001b[38;5;241m=\u001b[39msampler_engression, direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 26\u001b[0m \u001b[43mstudy_engression\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengressor_NN\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_TRIALS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m# Check if CUDA is available and if so, move the tensors and the model to the GPU\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m, in \u001b[0;36mengressor_NN\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     12\u001b[0m     engressor_model\u001b[38;5;241m=\u001b[39mengression(X_train__tensor, y_train__tensor\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m), lr\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m], num_epoches\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_epoches\u001b[39m\u001b[38;5;124m'\u001b[39m],num_layer\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_layer\u001b[39m\u001b[38;5;124m'\u001b[39m], hidden_dim\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m], noise_dim\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnoise_dim\u001b[39m\u001b[38;5;124m'\u001b[39m], batch_size\u001b[38;5;241m=\u001b[39mBATCH_SIZE, resblock\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mresblock\u001b[39m\u001b[38;5;124m'\u001b[39m], device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 14\u001b[0m     engressor_model\u001b[38;5;241m=\u001b[39m\u001b[43mengression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train__tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train__tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlearning_rate\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epoches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_epoches\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_layer\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhidden_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_dim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnoise_dim\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mBATCH_SIZE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresblock\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresblock\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# Generate a sample from the engression model for each data point\u001b[39;00m\n\u001b[0;32m     17\u001b[0m y_val_hat_engression_samples \u001b[38;5;241m=\u001b[39m [engressor_model\u001b[38;5;241m.\u001b[39msample(torch\u001b[38;5;241m.\u001b[39mTensor(np\u001b[38;5;241m.\u001b[39marray([X_val\u001b[38;5;241m.\u001b[39mvalues[i]])), sample_size\u001b[38;5;241m=\u001b[39mN_SAMPLES) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_val))]\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\engression\\engression.py:47\u001b[0m, in \u001b[0;36mengression\u001b[1;34m(x, y, sigmoid, num_layer, hidden_dim, noise_dim, add_bn, resblock, beta, lr, num_epoches, batch_size, print_every_nepoch, print_times_per_epoch, device, standardize, verbose)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe sample sizes for the covariates and response do not match. Please check.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     42\u001b[0m engressor \u001b[38;5;241m=\u001b[39m Engressor(in_dim\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], out_dim\u001b[38;5;241m=\u001b[39my\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \n\u001b[0;32m     43\u001b[0m                       num_layer\u001b[38;5;241m=\u001b[39mnum_layer, hidden_dim\u001b[38;5;241m=\u001b[39mhidden_dim, noise_dim\u001b[38;5;241m=\u001b[39mnoise_dim, \n\u001b[0;32m     44\u001b[0m                       sigmoid\u001b[38;5;241m=\u001b[39msigmoid, resblock\u001b[38;5;241m=\u001b[39mresblock, add_bn\u001b[38;5;241m=\u001b[39madd_bn, beta\u001b[38;5;241m=\u001b[39mbeta,\n\u001b[0;32m     45\u001b[0m                       lr\u001b[38;5;241m=\u001b[39mlr, num_epoches\u001b[38;5;241m=\u001b[39mnum_epoches, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, \n\u001b[0;32m     46\u001b[0m                       standardize\u001b[38;5;241m=\u001b[39mstandardize, device\u001b[38;5;241m=\u001b[39mdevice, check_device\u001b[38;5;241m=\u001b[39mverbose, verbose\u001b[38;5;241m=\u001b[39mverbose)\n\u001b[1;32m---> 47\u001b[0m \u001b[43mengressor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epoches\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epoches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprint_every_nepoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_every_nepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_times_per_epoch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_times_per_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     49\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstandardize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstandardize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m engressor\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\engression\\engression.py:262\u001b[0m, in \u001b[0;36mEngressor.train\u001b[1;34m(self, x, y, num_epoches, batch_size, print_every_nepoch, print_times_per_epoch, standardize, verbose)\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_idx, (x_batch, y_batch) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m--> 262\u001b[0m     y_sample1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    263\u001b[0m     y_sample2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(x_batch)\n\u001b[0;32m    264\u001b[0m     loss, loss1, loss2 \u001b[38;5;241m=\u001b[39m energy_loss_two_sample(y_batch, y_sample1, y_sample2, beta\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbeta, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\engression\\models.py:223\u001b[0m, in \u001b[0;36mStoNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnet(x)\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 223\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_layer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minter_layer(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m))\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\engression\\models.py:35\u001b[0m, in \u001b[0;36mStoLayer.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     33\u001b[0m eps \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise_dim, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     34\u001b[0m out \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, eps], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_act \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     37\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_act(out)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def engressor_NN(trial):\n",
    "\n",
    "    params = {'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.01, log=True),\n",
    "            'num_epoches': trial.suggest_int('num_epoches', 100, 1000),\n",
    "            'num_layer': trial.suggest_int('num_layer', 2, 5),\n",
    "            'hidden_dim': trial.suggest_int('hidden_dim', 100, 500),\n",
    "            'resblock': trial.suggest_categorical('resblock', [True, False])}\n",
    "    params['noise_dim']=params['hidden_dim']\n",
    "\n",
    "    # Check if CUDA is available and if so, move the tensors and the model to the GPU\n",
    "    if torch.cuda.is_available():\n",
    "        engressor_model=engression(X_train__tensor, y_train__tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'], device=\"cuda\")\n",
    "    else:\n",
    "        engressor_model=engression(X_train__tensor, y_train__tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'])\n",
    "    \n",
    "    # Generate a sample from the engression model for each data point\n",
    "    y_val_hat_engression_samples = [engressor_model.sample(torch.Tensor(np.array([X_val.values[i]])), sample_size=N_SAMPLES) for i in range(len(X_val))]\n",
    "\n",
    "    # Calculate the CRPS for each prediction\n",
    "    crps_values = [crps_ensemble(y_val_np[i], np.array(y_val_hat_engression_samples[i]).reshape(-1,)) for i in range(len(y_val_np))]\n",
    "\n",
    "    return np.mean(crps_values)\n",
    "\n",
    "sampler_engression = optuna.samplers.TPESampler(seed=seed)\n",
    "study_engression = optuna.create_study(sampler=sampler_engression, direction='minimize')\n",
    "study_engression.optimize(engressor_NN, n_trials=N_TRIALS)\n",
    "\n",
    "\n",
    "# Check if CUDA is available and if so, move the tensors and the model to the GPU\n",
    "if torch.cuda.is_available():\n",
    "    engressor_model=engression(X_train_tensor, y_train_tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'], device=\"cuda\")\n",
    "else:\n",
    "    engressor_model=engression(X_train_tensor, y_train_tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=BATCH_SIZE, resblock=params['resblock'])\n",
    "# Generate a sample from the engression model for each data point\n",
    "y_test_hat_engression_samples = [engressor_model.sample(torch.Tensor(np.array([X_test.values[i]])).cuda() if torch.cuda.is_available() else torch.Tensor(np.array([X_test.values[i]])), sample_size=N_SAMPLES) for i in range(len(X_test))]\n",
    "# Calculate the CRPS for each prediction\n",
    "crps_values = [crps_ensemble(y_test_np[i], np.array(y_test_hat_engression_samples[i]).reshape(-1,)) for i in range(len(y_test_np))]\n",
    "CRPS_engression=np.mean(crps_values)\n",
    "\n",
    "print(\"CRPS linear regression: \",CRPS_linreg)\n",
    "print(\"CRPS boosted trees\", CRPS_boosted)\n",
    "print(\"CRPS random forest\", CRPS_rf)\n",
    "print(\"CRPS engression\", CRPS_engression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import setuptools\n",
    "import openml\n",
    "from sklearn.linear_model import LinearRegression \n",
    "import lightgbm as lgbm\n",
    "import lightgbmlss\n",
    "import optuna\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from engression import engression, engression_bagged\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rtdl_revisiting_models import MLP, ResNet, FTTransformer\n",
    "from properscoring import crps_gaussian, crps_ensemble\n",
    "import random\n",
    "import gpytorch\n",
    "import tqdm.auto as tqdm\n",
    "from lightgbmlss.model import *\n",
    "from lightgbmlss.distributions.Gaussian import *\n",
    "from pygam import LinearGAM, s, f\n",
    "from utils import EarlyStopping, train, train_trans, train_no_early_stopping, train_trans_no_early_stopping, train_GP, ExactGPModel\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from drf import drf\n",
    "\n",
    "SUITE_ID = 336 # Regression on numerical features\n",
    "#SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "\n",
    "task_id=361074"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(benchmark_suite.tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 361074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 48\u001b[0m\n\u001b[0;32m     45\u001b[0m cov \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcov(X_train\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# calculate the Mahalanobis distance for each data point\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m mahalanobis_dist_ \u001b[38;5;241m=\u001b[39m [mahalanobis(x, mean, np\u001b[38;5;241m.\u001b[39mlinalg\u001b[38;5;241m.\u001b[39minv(cov)) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_train\u001b[38;5;241m.\u001b[39mvalues]\n\u001b[0;32m     50\u001b[0m mahalanobis_dist_\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mSeries(mahalanobis_dist_,index\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m     51\u001b[0m far_index_\u001b[38;5;241m=\u001b[39mmahalanobis_dist_\u001b[38;5;241m.\u001b[39mindex[np\u001b[38;5;241m.\u001b[39mwhere(mahalanobis_dist_\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mquantile(mahalanobis_dist_,\u001b[38;5;241m0.8\u001b[39m))[\u001b[38;5;241m0\u001b[39m]]\n",
      "Cell \u001b[1;32mIn[7], line 48\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     45\u001b[0m cov \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcov(X_train\u001b[38;5;241m.\u001b[39mT)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# calculate the Mahalanobis distance for each data point\u001b[39;00m\n\u001b[1;32m---> 48\u001b[0m mahalanobis_dist_ \u001b[38;5;241m=\u001b[39m [mahalanobis(x, mean, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcov\u001b[49m\u001b[43m)\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m X_train\u001b[38;5;241m.\u001b[39mvalues]\n\u001b[0;32m     50\u001b[0m mahalanobis_dist_\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mSeries(mahalanobis_dist_,index\u001b[38;5;241m=\u001b[39mX_train\u001b[38;5;241m.\u001b[39mindex)\n\u001b[0;32m     51\u001b[0m far_index_\u001b[38;5;241m=\u001b[39mmahalanobis_dist_\u001b[38;5;241m.\u001b[39mindex[np\u001b[38;5;241m.\u001b[39mwhere(mahalanobis_dist_\u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mquantile(mahalanobis_dist_,\u001b[38;5;241m0.8\u001b[39m))[\u001b[38;5;241m0\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\numpy\\linalg\\linalg.py:561\u001b[0m, in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    559\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    560\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[1;32m--> 561\u001b[0m ainv \u001b[38;5;241m=\u001b[39m \u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(ainv\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\numpy\\linalg\\linalg.py:112\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingular matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "# Create the checkpoint directory if it doesn't exist\n",
    "os.makedirs('CHECKPOINTS/MAHALANOBIS', exist_ok=True)\n",
    "CHECKPOINT_PATH = f'CHECKPOINTS/MAHALANOBIS/task_{task_id}.pt'\n",
    "\n",
    "print(f\"Task {task_id}\")\n",
    "\n",
    "task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "dataset = task.get_dataset()\n",
    "\n",
    "X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "N_TRIALS=100\n",
    "N_SAMPLES=100\n",
    "PATIENCE=40\n",
    "N_EPOCHS=1000\n",
    "GP_ITERATIONS=1000\n",
    "BATCH_SIZE=1024\n",
    "seed=10\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "# calculate the mean and covariance matrix of the dataset\n",
    "mean = np.mean(X, axis=0)\n",
    "cov = np.cov(X.T)\n",
    "\n",
    "# calculate the Mahalanobis distance for each data point\n",
    "mahalanobis_dist = [mahalanobis(x, mean, np.linalg.inv(cov)) for x in X.values]\n",
    "\n",
    "mahalanobis_dist=pd.Series(mahalanobis_dist,index=X.index)\n",
    "far_index=mahalanobis_dist.index[np.where(mahalanobis_dist>=np.quantile(mahalanobis_dist,0.8))[0]]\n",
    "close_index=mahalanobis_dist.index[np.where(mahalanobis_dist<np.quantile(mahalanobis_dist,0.8))[0]]\n",
    "\n",
    "X_train = X.loc[close_index,:]\n",
    "X_test = X.loc[far_index,:]\n",
    "y_train = y.loc[close_index]\n",
    "y_test = y.loc[far_index]\n",
    "\n",
    "mean = np.mean(X_train, axis=0)\n",
    "cov = np.cov(X_train.T)\n",
    "\n",
    "# calculate the Mahalanobis distance for each data point\n",
    "mahalanobis_dist_ = [mahalanobis(x, mean, np.linalg.inv(cov)) for x in X_train.values]\n",
    "\n",
    "mahalanobis_dist_=pd.Series(mahalanobis_dist_,index=X_train.index)\n",
    "far_index_=mahalanobis_dist_.index[np.where(mahalanobis_dist_>=np.quantile(mahalanobis_dist_,0.8))[0]]\n",
    "close_index_=mahalanobis_dist_.index[np.where(mahalanobis_dist_<np.quantile(mahalanobis_dist_,0.8))[0]]\n",
    "\n",
    "X_train_ = X_train.loc[close_index_,:]\n",
    "X_val = X_train.loc[far_index_,:]\n",
    "y_train_ = y_train.loc[close_index_]\n",
    "y_val = y_train.loc[far_index_]\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train__tensor = torch.tensor(X_train_.values, dtype=torch.float32)\n",
    "y_train__tensor = torch.tensor(y_train_.values, dtype=torch.float32)\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Convert to use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    X_train__tensor = X_train__tensor.cuda()\n",
    "    y_train__tensor = y_train__tensor.cuda()\n",
    "    X_train_tensor = X_train_tensor.cuda()\n",
    "    y_train_tensor = y_train_tensor.cuda()\n",
    "    X_val_tensor = X_val_tensor.cuda()\n",
    "    y_val_tensor = y_val_tensor.cuda()\n",
    "    X_test_tensor = X_test_tensor.cuda()\n",
    "    y_test_tensor = y_test_tensor.cuda()\n",
    "\n",
    "# Create flattened versions of the data\n",
    "y_val_np = y_val.values.flatten()\n",
    "y_test_np = y_test.values.flatten()\n",
    "\n",
    "# Create TensorDatasets for training and validation sets\n",
    "train__dataset = TensorDataset(X_train__tensor, y_train__tensor)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders for training and validation sets\n",
    "train__loader = DataLoader(train__dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16599, 16)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 16)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "LinAlgError",
     "evalue": "Singular matrix",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcov\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\numpy\\linalg\\linalg.py:561\u001b[0m, in \u001b[0;36minv\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m    559\u001b[0m signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD->D\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isComplexType(t) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124md->d\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    560\u001b[0m extobj \u001b[38;5;241m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_singular)\n\u001b[1;32m--> 561\u001b[0m ainv \u001b[38;5;241m=\u001b[39m \u001b[43m_umath_linalg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    562\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m wrap(ainv\u001b[38;5;241m.\u001b[39mastype(result_t, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\numpy\\linalg\\linalg.py:112\u001b[0m, in \u001b[0;36m_raise_linalgerror_singular\u001b[1;34m(err, flag)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_raise_linalgerror_singular\u001b[39m(err, flag):\n\u001b[1;32m--> 112\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m LinAlgError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSingular matrix\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: Singular matrix"
     ]
    }
   ],
   "source": [
    "np.linalg.inv(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.21391060e+04, -1.91769834e+02,  1.01992189e+01,\n",
       "        -6.20669455e+00, -4.06397490e+01, -2.38630219e+01,\n",
       "         7.67521688e+02,  7.20824919e-01, -2.46737556e+01,\n",
       "         1.17628657e-02,  1.22922006e-02,  1.22922006e-02,\n",
       "         1.24216423e-02, -8.98351803e-04, -1.40631499e-04,\n",
       "         1.24216423e-02],\n",
       "       [-1.91769834e+02,  5.87296133e+02,  5.15133831e-01,\n",
       "         9.07528413e-01,  5.10515565e-01,  7.26653674e+00,\n",
       "        -1.05121549e+02, -1.36988171e-02, -1.96439028e-01,\n",
       "         4.29649933e-04,  4.58733734e-04,  4.58733734e-04,\n",
       "         4.74203922e-04, -3.41005033e-05, -1.72670663e-05,\n",
       "         4.74203922e-04],\n",
       "       [ 1.01992189e+01,  5.15133831e-01,  7.73643481e-02,\n",
       "         3.00655351e-03,  1.20565177e-03,  1.36462404e-02,\n",
       "        -2.73906768e-01,  5.96023040e-04, -1.70299365e-02,\n",
       "         1.27764791e-05,  1.67190809e-05,  1.67190809e-05,\n",
       "         1.78566964e-05, -5.95066117e-06, -1.55978696e-06,\n",
       "         1.78566964e-05],\n",
       "       [-6.20669455e+00,  9.07528413e-01,  3.00655351e-03,\n",
       "         1.06015784e-02,  8.77643127e-03, -1.52741018e-01,\n",
       "        -8.89562710e-01, -2.66577129e-04, -1.25284688e-02,\n",
       "        -1.61572458e-05, -1.60832605e-05, -1.60832605e-05,\n",
       "        -1.60945499e-05, -3.87390013e-08,  2.35541395e-08,\n",
       "        -1.60945499e-05],\n",
       "       [-4.06397490e+01,  5.10515565e-01,  1.20565177e-03,\n",
       "         8.77643127e-03,  8.69933275e-01,  7.00245844e-01,\n",
       "        -1.69754424e+00, -7.33985344e-03,  5.56671600e-03,\n",
       "        -8.46883934e-05, -8.42867846e-05, -8.42867846e-05,\n",
       "        -8.41007984e-05, -1.30529309e-07, -1.32361901e-07,\n",
       "        -8.41007984e-05],\n",
       "       [-2.38630219e+01,  7.26653674e+00,  1.36462404e-02,\n",
       "        -1.52741018e-01,  7.00245844e-01,  1.49046160e+01,\n",
       "        -1.43241604e+00, -7.05927428e-03, -1.20865369e-02,\n",
       "         9.06751521e-04,  9.11530716e-04,  9.11530716e-04,\n",
       "         9.15829176e-04, -9.42821097e-06, -6.22981431e-06,\n",
       "         9.15829176e-04],\n",
       "       [ 7.67521688e+02, -1.05121549e+02, -2.73906768e-01,\n",
       "        -8.89562710e-01, -1.69754424e+00, -1.43241604e+00,\n",
       "         1.00119190e+02,  3.73537725e-02, -1.39415927e-01,\n",
       "         5.69335614e-04,  5.58799142e-04,  5.58799142e-04,\n",
       "         5.56632099e-04,  9.31120060e-06,  2.45929184e-06,\n",
       "         5.56632099e-04],\n",
       "       [ 7.20824919e-01, -1.36988171e-02,  5.96023040e-04,\n",
       "        -2.66577129e-04, -7.33985344e-03, -7.05927428e-03,\n",
       "         3.73537725e-02,  1.31903185e-04, -1.49383034e-04,\n",
       "         1.82989498e-06,  1.90689369e-06,  1.90689369e-06,\n",
       "         1.92291737e-06, -1.21163991e-07, -2.26046654e-08,\n",
       "         1.92291737e-06],\n",
       "       [-2.46737556e+01, -1.96439028e-01, -1.70299365e-02,\n",
       "        -1.25284688e-02,  5.56671600e-03, -1.20865369e-02,\n",
       "        -1.39415927e-01, -1.49383034e-04,  3.90108876e-01,\n",
       "        -1.30960074e-06, -1.95454044e-06, -1.95454044e-06,\n",
       "        -2.14103550e-06,  1.17854625e-06,  3.13912440e-07,\n",
       "        -2.14103550e-06],\n",
       "       [ 1.17628657e-02,  4.29649933e-04,  1.27764791e-05,\n",
       "        -1.61572458e-05, -8.46883934e-05,  9.06751521e-04,\n",
       "         5.69335614e-04,  1.82989498e-06, -1.30960074e-06,\n",
       "         1.11783055e-07,  1.12243295e-07,  1.12243295e-07,\n",
       "         1.12419714e-07, -1.18886337e-09, -2.96238691e-10,\n",
       "         1.12419714e-07],\n",
       "       [ 1.22922006e-02,  4.58733734e-04,  1.67190809e-05,\n",
       "        -1.60832605e-05, -8.42867846e-05,  9.11530716e-04,\n",
       "         5.58799142e-04,  1.90689369e-06, -1.95454044e-06,\n",
       "         1.12243295e-07,  1.14226057e-07,  1.14226057e-07,\n",
       "         1.14588406e-07, -3.06554979e-09, -5.71425316e-10,\n",
       "         1.14588406e-07],\n",
       "       [ 1.22922006e-02,  4.58733734e-04,  1.67190809e-05,\n",
       "        -1.60832605e-05, -8.42867846e-05,  9.11530716e-04,\n",
       "         5.58799142e-04,  1.90689369e-06, -1.95454044e-06,\n",
       "         1.12243295e-07,  1.14226057e-07,  1.14226057e-07,\n",
       "         1.14588406e-07, -3.06554979e-09, -5.71425316e-10,\n",
       "         1.14588406e-07],\n",
       "       [ 1.24216423e-02,  4.74203922e-04,  1.78566964e-05,\n",
       "        -1.60945499e-05, -8.41007984e-05,  9.15829176e-04,\n",
       "         5.56632099e-04,  1.92291737e-06, -2.14103550e-06,\n",
       "         1.12419714e-07,  1.14588406e-07,  1.14588406e-07,\n",
       "         1.15309025e-07, -3.34674752e-09, -9.81576857e-10,\n",
       "         1.15309025e-07],\n",
       "       [-8.98351803e-04, -3.41005033e-05, -5.95066117e-06,\n",
       "        -3.87390013e-08, -1.30529309e-07, -9.42821097e-06,\n",
       "         9.31120060e-06, -1.21163991e-07,  1.17854625e-06,\n",
       "        -1.18886337e-09, -3.06554979e-09, -3.06554979e-09,\n",
       "        -3.34674752e-09,  3.16963985e-09,  4.25300826e-10,\n",
       "        -3.34674752e-09],\n",
       "       [-1.40631499e-04, -1.72670663e-05, -1.55978696e-06,\n",
       "         2.35541395e-08, -1.32361901e-07, -6.22981431e-06,\n",
       "         2.45929184e-06, -2.26046654e-08,  3.13912440e-07,\n",
       "        -2.96238691e-10, -5.71425316e-10, -5.71425316e-10,\n",
       "        -9.81576857e-10,  4.25300826e-10,  6.22385521e-10,\n",
       "        -9.81576857e-10],\n",
       "       [ 1.24216423e-02,  4.74203922e-04,  1.78566964e-05,\n",
       "        -1.60945499e-05, -8.41007984e-05,  9.15829176e-04,\n",
       "         5.56632099e-04,  1.92291737e-06, -2.14103550e-06,\n",
       "         1.12419714e-07,  1.14588406e-07,  1.14588406e-07,\n",
       "         1.15309025e-07, -3.34674752e-09, -9.81576857e-10,\n",
       "         1.15309025e-07]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.det(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:MahalanobisND:Wrong format in calib_entries argument. Must be convertible to integer, list or np.array\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Wrong format in calib_entries argument. Must be convertible to integer, list or np.array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmahalanobis\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Mahalanobis\n\u001b[1;32m----> 2\u001b[0m \u001b[43mMahalanobis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\mahalanobis\\__init__.py:512\u001b[0m, in \u001b[0;36mMahalanobis\u001b[1;34m(input_array, calib_rows, nan_subst_method)\u001b[0m\n\u001b[0;32m    509\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    510\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ShapeError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEmpty input array\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 512\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcalculator\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcalib_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnan_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnan_subst_method\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\mahalanobis\\__init__.py:390\u001b[0m, in \u001b[0;36mMahalanobisND.__init__\u001b[1;34m(self, array, calib_entries, nan_method)\u001b[0m\n\u001b[0;32m    387\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray \u001b[38;5;241m=\u001b[39m array\n\u001b[0;32m    388\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalib_entries \u001b[38;5;241m=\u001b[39m calib_entries\n\u001b[1;32m--> 390\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select_calibration_subarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_replace_nans()\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_calibration_mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcalibration_chunk\u001b[38;5;241m.\u001b[39mmean(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)  \u001b[38;5;66;03m# mean to which array entries will be compared\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\mahalanobis\\__init__.py:83\u001b[0m, in \u001b[0;36mMahalanobisBenchmark._select_calibration_subarray\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     81\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWrong format in calib_entries argument. Must be convertible to integer, list or np.array\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger\u001b[38;5;241m.\u001b[39merror(msg)\n\u001b[1;32m---> 83\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m     85\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdimensionality \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marray\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: Wrong format in calib_entries argument. Must be convertible to integer, list or np.array"
     ]
    }
   ],
   "source": [
    "from mahalanobis import Mahalanobis\n",
    "Mahalanobis(X,X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-05 17:27:38,141] A new study created in memory with name: no-name-21501702-7a56-4a74-98ed-f8366ed6a79e\n",
      "[I 2024-03-05 17:27:38,755] Trial 0 finished with value: 22.947252239814784 and parameters: {'n_splines': 17, 'lam': 0.001154132971137168}. Best is trial 0 with value: 22.947252239814784.\n",
      "[I 2024-03-05 17:27:38,823] Trial 1 finished with value: 10.740112483235158 and parameters: {'n_splines': 15, 'lam': 0.17636469336159113}. Best is trial 1 with value: 10.740112483235158.\n",
      "[I 2024-03-05 17:27:38,875] Trial 2 finished with value: 10.896560638127578 and parameters: {'n_splines': 12, 'lam': 0.00472487079152679}. Best is trial 1 with value: 10.740112483235158.\n",
      "[I 2024-03-05 17:27:38,936] Trial 3 finished with value: 10.703338919026589 and parameters: {'n_splines': 8, 'lam': 0.19124590142517375}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:38,990] Trial 4 finished with value: 11.200486892602623 and parameters: {'n_splines': 7, 'lam': 0.0018408544111075849}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,040] Trial 5 finished with value: 10.720930978839837 and parameters: {'n_splines': 15, 'lam': 0.7247363402746422}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,075] Trial 6 finished with value: 10.784504830252562 and parameters: {'n_splines': 5, 'lam': 0.03440145332328687}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,144] Trial 7 finished with value: 10.98992076901203 and parameters: {'n_splines': 18, 'lam': 0.06879837817714736}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,203] Trial 8 finished with value: 12.971812468294816 and parameters: {'n_splines': 16, 'lam': 0.007509797119626296}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,256] Trial 9 finished with value: 10.876867451236265 and parameters: {'n_splines': 19, 'lam': 0.13922824544531598}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,339] Trial 10 finished with value: 10.712530052622382 and parameters: {'n_splines': 10, 'lam': 0.8536439312999228}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,408] Trial 11 finished with value: 10.710547077298443 and parameters: {'n_splines': 10, 'lam': 0.7029820442590767}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,453] Trial 12 finished with value: 10.706705196949272 and parameters: {'n_splines': 9, 'lam': 0.28864747460782875}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,507] Trial 13 finished with value: 10.705114803939338 and parameters: {'n_splines': 8, 'lam': 0.24853970486508473}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,562] Trial 14 finished with value: 10.71634708082109 and parameters: {'n_splines': 5, 'lam': 0.06482547888607312}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,622] Trial 15 finished with value: 10.708039089323746 and parameters: {'n_splines': 8, 'lam': 0.30262433161267577}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,680] Trial 16 finished with value: 10.78788458645363 and parameters: {'n_splines': 12, 'lam': 0.01305628749775024}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,730] Trial 17 finished with value: 10.717766258805705 and parameters: {'n_splines': 7, 'lam': 0.08205899786080603}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,809] Trial 18 finished with value: 10.733514833597695 and parameters: {'n_splines': 11, 'lam': 0.025726384630819565}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,884] Trial 19 finished with value: 10.724623629085979 and parameters: {'n_splines': 14, 'lam': 0.4132275001264877}. Best is trial 3 with value: 10.703338919026589.\n",
      "[I 2024-03-05 17:27:39,947] Trial 20 finished with value: 10.69165906297434 and parameters: {'n_splines': 7, 'lam': 0.1416966751064176}. Best is trial 20 with value: 10.69165906297434.\n",
      "[I 2024-03-05 17:27:40,005] Trial 21 finished with value: 10.69011567790952 and parameters: {'n_splines': 7, 'lam': 0.1558279525021782}. Best is trial 21 with value: 10.69011567790952.\n",
      "[I 2024-03-05 17:27:40,053] Trial 22 finished with value: 11.110018067269309 and parameters: {'n_splines': 6, 'lam': 0.12923874230939328}. Best is trial 21 with value: 10.69011567790952.\n",
      "[I 2024-03-05 17:27:40,111] Trial 23 finished with value: 10.73418916915517 and parameters: {'n_splines': 7, 'lam': 0.040548324084896954}. Best is trial 21 with value: 10.69011567790952.\n",
      "[I 2024-03-05 17:27:40,161] Trial 24 finished with value: 10.716293739787838 and parameters: {'n_splines': 9, 'lam': 0.11107468382701746}. Best is trial 21 with value: 10.69011567790952.\n",
      "[I 2024-03-05 17:27:40,205] Trial 25 finished with value: 10.855595908395024 and parameters: {'n_splines': 6, 'lam': 0.4945327252466708}. Best is trial 21 with value: 10.69011567790952.\n",
      "[I 2024-03-05 17:27:40,269] Trial 26 finished with value: 10.70713880673174 and parameters: {'n_splines': 9, 'lam': 0.18771804473308543}. Best is trial 21 with value: 10.69011567790952.\n",
      "[I 2024-03-05 17:27:40,313] Trial 27 finished with value: 10.72538886965758 and parameters: {'n_splines': 5, 'lam': 0.047298348679016916}. Best is trial 21 with value: 10.69011567790952.\n",
      "[I 2024-03-05 17:27:40,426] Trial 28 finished with value: 10.739009303955115 and parameters: {'n_splines': 11, 'lam': 0.01849738124111256}. Best is trial 21 with value: 10.69011567790952.\n",
      "[I 2024-03-05 17:27:40,481] Trial 29 finished with value: 10.720902380992936 and parameters: {'n_splines': 8, 'lam': 0.45352670391090916}. Best is trial 21 with value: 10.69011567790952.\n",
      "[I 2024-03-05 17:27:40,543] Trial 30 finished with value: 11.119070141131916 and parameters: {'n_splines': 6, 'lam': 0.08640477515525971}. Best is trial 21 with value: 10.69011567790952.\n",
      "[I 2024-03-05 17:27:40,593] Trial 31 finished with value: 10.704860412012883 and parameters: {'n_splines': 8, 'lam': 0.2425199691366182}. Best is trial 21 with value: 10.69011567790952.\n",
      "[I 2024-03-05 17:27:40,662] Trial 32 finished with value: 10.69369241288308 and parameters: {'n_splines': 7, 'lam': 0.19815659631339977}. Best is trial 21 with value: 10.69011567790952.\n",
      "[I 2024-03-05 17:27:40,719] Trial 33 finished with value: 10.689977997044839 and parameters: {'n_splines': 7, 'lam': 0.16650510779941946}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:40,779] Trial 34 finished with value: 11.104111883955335 and parameters: {'n_splines': 6, 'lam': 0.13643197532853657}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:40,838] Trial 35 finished with value: 10.733215497921089 and parameters: {'n_splines': 7, 'lam': 0.056269701602484797}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:40,895] Trial 36 finished with value: 10.727652859326133 and parameters: {'n_splines': 13, 'lam': 0.18793008113552953}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:40,961] Trial 37 finished with value: 10.710774240809304 and parameters: {'n_splines': 10, 'lam': 0.35230920872081417}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,017] Trial 38 finished with value: 12.867875842498968 and parameters: {'n_splines': 5, 'lam': 0.0014389084003192712}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,069] Trial 39 finished with value: 10.702242628361851 and parameters: {'n_splines': 7, 'lam': 0.10875707643577283}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,129] Trial 40 finished with value: 10.716818999166847 and parameters: {'n_splines': 9, 'lam': 0.6346532240312879}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,186] Trial 41 finished with value: 10.709198780673457 and parameters: {'n_splines': 7, 'lam': 0.09571942558037608}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,244] Trial 42 finished with value: 11.084180114955695 and parameters: {'n_splines': 6, 'lam': 0.1573796114609918}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,311] Trial 43 finished with value: 10.696115431328924 and parameters: {'n_splines': 7, 'lam': 0.20931468859991814}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,363] Trial 44 finished with value: 10.703759717261539 and parameters: {'n_splines': 8, 'lam': 0.21014968613900983}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,438] Trial 45 finished with value: 17.09773056199712 and parameters: {'n_splines': 17, 'lam': 0.0023242092566905016}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,494] Trial 46 finished with value: 10.710114228005105 and parameters: {'n_splines': 11, 'lam': 0.6002972219063147}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,559] Trial 47 finished with value: 10.810001677124259 and parameters: {'n_splines': 5, 'lam': 0.3254362148982656}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,612] Trial 48 finished with value: 10.721593872057928 and parameters: {'n_splines': 10, 'lam': 0.06957715682643138}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,670] Trial 49 finished with value: 10.724183386937533 and parameters: {'n_splines': 7, 'lam': 0.03048336120424258}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,738] Trial 50 finished with value: 10.706229275681416 and parameters: {'n_splines': 9, 'lam': 0.24362812130219916}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,778] Trial 51 finished with value: 10.702113893331642 and parameters: {'n_splines': 7, 'lam': 0.1090277941160679}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,827] Trial 52 finished with value: 10.703122195168408 and parameters: {'n_splines': 8, 'lam': 0.17520318734936818}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,861] Trial 53 finished with value: 11.105143032805707 and parameters: {'n_splines': 6, 'lam': 0.13522990541552923}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,909] Trial 54 finished with value: 10.805671162426858 and parameters: {'n_splines': 7, 'lam': 0.8756486902051627}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:41,975] Trial 55 finished with value: 10.70554526912651 and parameters: {'n_splines': 8, 'lam': 0.07649216734648331}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:42,037] Trial 56 finished with value: 10.901055408579944 and parameters: {'n_splines': 6, 'lam': 0.37670825670339686}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:42,142] Trial 57 finished with value: 10.717274314799205 and parameters: {'n_splines': 5, 'lam': 0.05350023625794819}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:42,213] Trial 58 finished with value: 10.7063107268843 and parameters: {'n_splines': 9, 'lam': 0.2583673358724434}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:42,286] Trial 59 finished with value: 10.709823653350293 and parameters: {'n_splines': 7, 'lam': 0.09466047705508453}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:42,346] Trial 60 finished with value: 10.703600728025528 and parameters: {'n_splines': 8, 'lam': 0.11854752846129753}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:42,420] Trial 61 finished with value: 11.022708651945115 and parameters: {'n_splines': 20, 'lam': 0.1124545581580333}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:42,471] Trial 62 finished with value: 10.696257664052531 and parameters: {'n_splines': 7, 'lam': 0.2099123317771141}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:42,520] Trial 63 finished with value: 11.013694324196877 and parameters: {'n_splines': 6, 'lam': 0.22537184624307616}. Best is trial 33 with value: 10.689977997044839.\n",
      "[I 2024-03-05 17:27:42,572] Trial 64 finished with value: 10.689940143720507 and parameters: {'n_splines': 7, 'lam': 0.16149924969158563}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:42,619] Trial 65 finished with value: 10.703054553498703 and parameters: {'n_splines': 8, 'lam': 0.15393825588758575}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:42,667] Trial 66 finished with value: 10.709366512468872 and parameters: {'n_splines': 10, 'lam': 0.5096391140332179}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:42,726] Trial 67 finished with value: 11.069834017120066 and parameters: {'n_splines': 6, 'lam': 0.17122181488535343}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:42,771] Trial 68 finished with value: 10.749296909034452 and parameters: {'n_splines': 7, 'lam': 0.00783476715336149}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:42,819] Trial 69 finished with value: 10.70852291477327 and parameters: {'n_splines': 9, 'lam': 0.3688194722806357}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:42,860] Trial 70 finished with value: 10.798076948120489 and parameters: {'n_splines': 5, 'lam': 0.2937780201854044}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:42,907] Trial 71 finished with value: 10.693745702544632 and parameters: {'n_splines': 7, 'lam': 0.19842799701003408}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:42,958] Trial 72 finished with value: 10.70341876997132 and parameters: {'n_splines': 8, 'lam': 0.19551401465223667}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,011] Trial 73 finished with value: 10.952195955066175 and parameters: {'n_splines': 6, 'lam': 0.29577647103370375}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,075] Trial 74 finished with value: 10.745624878779875 and parameters: {'n_splines': 15, 'lam': 0.14910303504977906}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,126] Trial 75 finished with value: 10.70015364940289 and parameters: {'n_splines': 7, 'lam': 0.22484833166746326}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,177] Trial 76 finished with value: 10.76420502583025 and parameters: {'n_splines': 7, 'lam': 0.4369798741401498}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,234] Trial 77 finished with value: 10.706440943819333 and parameters: {'n_splines': 9, 'lam': 0.27076820868789264}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,277] Trial 78 finished with value: 11.119764984825766 and parameters: {'n_splines': 6, 'lam': 0.08742004512002642}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,319] Trial 79 finished with value: 10.706401363002046 and parameters: {'n_splines': 8, 'lam': 0.062196666103272305}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,365] Trial 80 finished with value: 10.767817517407908 and parameters: {'n_splines': 5, 'lam': 0.1917685653021189}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,418] Trial 81 finished with value: 10.701116632624942 and parameters: {'n_splines': 7, 'lam': 0.2282366056740934}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,459] Trial 82 finished with value: 10.692583586119813 and parameters: {'n_splines': 7, 'lam': 0.13683759087427624}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,519] Trial 83 finished with value: 11.109854770706013 and parameters: {'n_splines': 6, 'lam': 0.12944947250507843}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,560] Trial 84 finished with value: 10.703050628307473 and parameters: {'n_splines': 8, 'lam': 0.15537055470730246}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,609] Trial 85 finished with value: 10.707857976065755 and parameters: {'n_splines': 7, 'lam': 0.09804074912062231}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,640] Trial 86 finished with value: 10.736966184391175 and parameters: {'n_splines': 7, 'lam': 0.337621496137364}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,690] Trial 87 finished with value: 10.703202721878162 and parameters: {'n_splines': 8, 'lam': 0.18239720283604452}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,727] Trial 88 finished with value: 11.106619870838058 and parameters: {'n_splines': 6, 'lam': 0.1334731796290905}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,776] Trial 89 finished with value: 10.749759312072767 and parameters: {'n_splines': 13, 'lam': 0.07633791179009217}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,824] Trial 90 finished with value: 10.73881341647395 and parameters: {'n_splines': 9, 'lam': 0.047333313820304576}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,887] Trial 91 finished with value: 10.697396817146313 and parameters: {'n_splines': 7, 'lam': 0.21453821876217216}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,941] Trial 92 finished with value: 10.694618649143301 and parameters: {'n_splines': 7, 'lam': 0.20267881130298912}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:43,988] Trial 93 finished with value: 10.969135170929526 and parameters: {'n_splines': 6, 'lam': 0.27440270350415535}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:44,040] Trial 94 finished with value: 10.703058135656168 and parameters: {'n_splines': 8, 'lam': 0.16537477885630056}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:44,103] Trial 95 finished with value: 10.697872644587733 and parameters: {'n_splines': 7, 'lam': 0.11894229285198582}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:44,155] Trial 96 finished with value: 10.703518308459776 and parameters: {'n_splines': 8, 'lam': 0.20027010005037627}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:44,203] Trial 97 finished with value: 10.846175466392118 and parameters: {'n_splines': 6, 'lam': 0.5326347976685996}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:44,241] Trial 98 finished with value: 10.841803961526011 and parameters: {'n_splines': 5, 'lam': 0.40216963881506307}. Best is trial 64 with value: 10.689940143720507.\n",
      "[I 2024-03-05 17:27:44,289] Trial 99 finished with value: 10.691620823617571 and parameters: {'n_splines': 7, 'lam': 0.14192466473457643}. Best is trial 64 with value: 10.689940143720507.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 1586 is out of bounds for axis 0 with size 1586",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 38\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Calculate the CRPS\u001b[39;00m\n\u001b[0;32m     37\u001b[0m std_dev_error \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(y_test \u001b[38;5;241m-\u001b[39m y_test_hat_gam)\n\u001b[1;32m---> 38\u001b[0m crps_gam \u001b[38;5;241m=\u001b[39m [crps_gaussian(y_test_np[i], mu\u001b[38;5;241m=\u001b[39my_test_hat_gam[i], sig\u001b[38;5;241m=\u001b[39mstd_dev_error) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_test_hat_gam))]\n\u001b[0;32m     39\u001b[0m crps_gam \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(crps_gam)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCRPS GAM: \u001b[39m\u001b[38;5;124m\"\u001b[39m, crps_gam)\n",
      "Cell \u001b[1;32mIn[18], line 38\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Calculate the CRPS\u001b[39;00m\n\u001b[0;32m     37\u001b[0m std_dev_error \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstd(y_test \u001b[38;5;241m-\u001b[39m y_test_hat_gam)\n\u001b[1;32m---> 38\u001b[0m crps_gam \u001b[38;5;241m=\u001b[39m [crps_gaussian(\u001b[43my_test_np\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m, mu\u001b[38;5;241m=\u001b[39my_test_hat_gam[i], sig\u001b[38;5;241m=\u001b[39mstd_dev_error) \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_test_hat_gam))]\n\u001b[0;32m     39\u001b[0m crps_gam \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(crps_gam)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCRPS GAM: \u001b[39m\u001b[38;5;124m\"\u001b[39m, crps_gam)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 1586 is out of bounds for axis 0 with size 1586"
     ]
    }
   ],
   "source": [
    "#### GAM model\n",
    "def gam_model(trial):\n",
    "\n",
    "    # Define the hyperparameters to optimize\n",
    "    params = {'n_splines': trial.suggest_int('n_splines', 5, 20),\n",
    "            'lam': trial.suggest_float('lam', 1e-3, 1, log=True)}\n",
    "\n",
    "    # Create and train the model\n",
    "    gam = LinearGAM(s(0, n_splines=params['n_splines'], lam=params['lam'])).fit(X_train_, y_train_)\n",
    "\n",
    "    # Predict on the validation set and calculate the CRPS\n",
    "    y_val_hat_gam = gam.predict(X_val)\n",
    "    std_dev_error = np.std(y_val - y_val_hat_gam)\n",
    "    crps_gam = [crps_gaussian(y_val_np[i], mu=y_val_hat_gam[i], sig=std_dev_error) for i in range(len(y_val_hat_gam))]\n",
    "    crps_gam = np.mean(crps_gam)\n",
    "\n",
    "    return crps_gam\n",
    "\n",
    "# Create the sampler and study\n",
    "sampler_gam = optuna.samplers.TPESampler(seed=seed)\n",
    "study_gam = optuna.create_study(sampler=sampler_gam, direction='minimize')\n",
    "\n",
    "# Optimize the model\n",
    "study_gam.optimize(gam_model, n_trials=N_TRIALS)\n",
    "\n",
    "# Create the final model with the best parameters\n",
    "best_params = study_gam.best_params\n",
    "final_gam_model = LinearGAM(s(0, n_splines=best_params['n_splines'], lam=best_params['lam']))\n",
    "\n",
    "# Fit the model\n",
    "final_gam_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_hat_gam = final_gam_model.predict(X_test)\n",
    "\n",
    "# Calculate the CRPS\n",
    "std_dev_error = np.std(y_test - y_test_hat_gam)\n",
    "crps_gam = [crps_gaussian(y_test_np[i], mu=y_test_hat_gam[i], sig=std_dev_error) for i in range(len(y_test_hat_gam))]\n",
    "crps_gam = np.mean(crps_gam)\n",
    "print(\"CRPS GAM: \", crps_gam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import setuptools\n",
    "import openml\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from engression import engression, engression_bagged\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rtdl_revisiting_models import MLP, ResNet, FTTransformer\n",
    "import random\n",
    "import gpytorch\n",
    "import tqdm.auto as tqdm\n",
    "import os\n",
    "from pygam import LogisticGAM, s\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder \n",
    "from gpytorch.models import AbstractVariationalGP\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "from gpytorch.mlls.variational_elbo import VariationalELBO\n",
    "from utils import EarlyStopping, train, train_trans, train_no_early_stopping, train_trans_no_early_stopping, train_GP, ExactGPModel\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "#SUITE_ID = 336 # Regression on numerical features\n",
    "SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "print(len(benchmark_suite.tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['temp', 'windspeed_max', 'windspeed_avg', 'precipitation', 'dew_point',\n",
       "       'humidity', 'hour', 'dayminute'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n",
      "361072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8192, 21)\n",
      "361073\n",
      "(15000, 26)\n",
      "361074\n",
      "(16599, 16)\n",
      "361076\n",
      "(6497, 11)\n",
      "361077\n",
      "(13750, 33)\n",
      "361078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20640, 8)\n",
      "361079\n",
      "(22784, 16)\n",
      "361080\n",
      "(53940, 6)\n",
      "361081\n",
      "(10692, 8)\n",
      "361082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17379, 6)\n",
      "361083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(581835, 9)\n",
      "361084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21613, 15)\n",
      "361085\n",
      "(10081, 6)\n",
      "361086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(163065, 3)\n",
      "361087\n",
      "(13932, 13)\n",
      "361088\n",
      "(21263, 79)\n",
      "361279\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8885, 42)\n",
      "361280\n",
      "(4177, 7)\n",
      "361281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5465575, 8)\n"
     ]
    }
   ],
   "source": [
    "SUITE_ID = 336 # Regression on numerical features\n",
    "#SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "print(len(benchmark_suite.tasks))\n",
    "\n",
    "for task_id in benchmark_suite.tasks:\n",
    "    print(task_id)\n",
    "    task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "    dataset = task.get_dataset()\n",
    "\n",
    "    X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "            dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "    print(X.shape)\n",
    "    for col in X.columns:\n",
    "        if X[col].nunique() < 10:\n",
    "            print(col, X[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16714, 10)\n",
      "NumberOfDependents 9\n",
      "361060\n",
      "(38474, 7)\n",
      "361061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(566602, 10)\n",
      "361062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10082, 26)\n",
      "361063\n",
      "(13488, 16)\n",
      "361065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13376, 10)\n",
      "361066\n",
      "(10578, 7)\n",
      "361068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72998, 50)\n",
      "361069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(940160, 24)\n",
      "361070\n",
      "(7608, 20)\n",
      "361273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71090, 7)\n",
      "361274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57580, 54)\n",
      "361275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13272, 20)\n",
      "361276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3434, 419)\n",
      "D337 9\n",
      "D350 8\n",
      "D478 9\n",
      "D551 9\n",
      "D553 9\n",
      "D607 9\n",
      "D608 9\n",
      "D703 9\n",
      "361277\n",
      "(20634, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "361278\n",
      "(10000, 22)\n"
     ]
    }
   ],
   "source": [
    "#SUITE_ID = 336 # Regression on numerical features\n",
    "SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "print(len(benchmark_suite.tasks))\n",
    "\n",
    "for task_id in benchmark_suite.tasks:\n",
    "    print(task_id)\n",
    "    task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "    dataset = task.get_dataset()\n",
    "\n",
    "    X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "            dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "    print(X.shape)\n",
    "    for col in X.columns:\n",
    "        if X[col].nunique() < 10:\n",
    "            print(col, X[col].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 361072\n",
      "Task 361072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import setuptools\n",
    "import openml\n",
    "from sklearn.linear_model import LinearRegression \n",
    "import lightgbm as lgbm\n",
    "import lightgbmlss\n",
    "import optuna\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from engression import engression, engression_bagged\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rtdl_revisiting_models import MLP, ResNet, FTTransformer\n",
    "from properscoring import crps_gaussian, crps_ensemble\n",
    "import random\n",
    "import gpytorch\n",
    "import tqdm.auto as tqdm\n",
    "from lightgbmlss.model import *\n",
    "from lightgbmlss.distributions.Gaussian import *\n",
    "from drf import drf\n",
    "from pygam import LinearGAM, s, f\n",
    "from utils import EarlyStopping, train, train_trans, train_no_early_stopping, train_trans_no_early_stopping, train_GP, ExactGPModel\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "import rpy2.robjects as robjects\n",
    "from rpy2.robjects import pandas2ri\n",
    "from rpy2.robjects.packages import importr\n",
    "\n",
    "SUITE_ID = 336 # Regression on numerical features\n",
    "#SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "\n",
    "task_id=361072\n",
    "\n",
    "print(f\"Task {task_id}\")\n",
    "\n",
    "# Create the checkpoint directory if it doesn't exist\n",
    "os.makedirs('CHECKPOINTS/SPATIAL_DEPTH', exist_ok=True)\n",
    "CHECKPOINT_PATH = f'CHECKPOINTS/SPATIAL_DEPTH/task_{task_id}.pt'\n",
    "\n",
    "print(f\"Task {task_id}\")\n",
    "\n",
    "task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "dataset = task.get_dataset()\n",
    "\n",
    "X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "\n",
    "# Find features with absolute correlation > 0.9\n",
    "corr_matrix = X.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "high_corr_features = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\n",
    "\n",
    "# Drop one of the highly correlated features\n",
    "X = X.drop(high_corr_features, axis=1)\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "N_TRIALS=100\n",
    "N_SAMPLES=100\n",
    "PATIENCE=40\n",
    "N_EPOCHS=1000\n",
    "GP_ITERATIONS=1000\n",
    "BATCH_SIZE=1024\n",
    "seed=10\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "# activate pandas conversion for rpy2\n",
    "pandas2ri.activate()\n",
    "\n",
    "# import R's \"ddalpha\" package\n",
    "ddalpha = importr('ddalpha')\n",
    "\n",
    "# explicitly import the projDepth function\n",
    "spatialDepth = robjects.r['depth.spatial']\n",
    "\n",
    "# calculate the spatial depth for each data point\n",
    "spatial_depth = spatialDepth(X, X)\n",
    "\n",
    "spatial_depth=pd.Series(spatial_depth,index=X.index)\n",
    "far_index=spatial_depth.index[np.where(spatial_depth<=np.quantile(spatial_depth,0.2))[0]]\n",
    "close_index=spatial_depth.index[np.where(spatial_depth>np.quantile(spatial_depth,0.2))[0]]\n",
    "\n",
    "X_train = X.loc[close_index,:]\n",
    "X_test = X.loc[far_index,:]\n",
    "y_train = y.loc[close_index]\n",
    "y_test = y.loc[far_index]\n",
    "\n",
    "# convert the R vector to a pandas Series\n",
    "spatial_depth_ = spatialDepth(X_train, X_train)\n",
    "\n",
    "spatial_depth_=pd.Series(spatial_depth_,index=X_train.index)\n",
    "far_index_=spatial_depth_.index[np.where(spatial_depth_<=np.quantile(spatial_depth_,0.2))[0]]\n",
    "close_index_=spatial_depth_.index[np.where(spatial_depth_>np.quantile(spatial_depth_,0.2))[0]]\n",
    "\n",
    "X_train_ = X_train.loc[close_index_,:]\n",
    "X_val = X_train.loc[far_index_,:]\n",
    "y_train_ = y_train.loc[close_index_]\n",
    "y_val = y_train.loc[far_index_]\n",
    "\n",
    "\n",
    "# Standardize the data\n",
    "mean_X_train_ = np.mean(X_train_, axis=0)\n",
    "std_X_train_ = np.std(X_train_, axis=0)\n",
    "X_train__scaled = (X_train_ - mean_X_train_) / std_X_train_\n",
    "X_val_scaled = (X_val - mean_X_train_) / std_X_train_\n",
    "\n",
    "mean_X_train = np.mean(X_train, axis=0)\n",
    "std_X_train = np.std(X_train, axis=0)\n",
    "X_train_scaled = (X_train - mean_X_train) / std_X_train\n",
    "X_test_scaled = (X_test - mean_X_train) / std_X_train\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train__tensor = torch.tensor(X_train__scaled.values, dtype=torch.float32)\n",
    "y_train__tensor = torch.tensor(y_train_.values, dtype=torch.float32)\n",
    "X_train_tensor = torch.tensor(X_train_scaled.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_scaled.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Convert to use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    X_train__tensor = X_train__tensor.cuda()\n",
    "    y_train__tensor = y_train__tensor.cuda()\n",
    "    X_train_tensor = X_train_tensor.cuda()\n",
    "    y_train_tensor = y_train_tensor.cuda()\n",
    "    X_val_tensor = X_val_tensor.cuda()\n",
    "    y_val_tensor = y_val_tensor.cuda()\n",
    "    X_test_tensor = X_test_tensor.cuda()\n",
    "    y_test_tensor = y_test_tensor.cuda()\n",
    "\n",
    "# Create flattened versions of the data\n",
    "y_val_np = y_val.values.flatten()\n",
    "y_test_np = y_test.values.flatten()\n",
    "\n",
    "# Create TensorDatasets for training and validation sets\n",
    "train__dataset = TensorDataset(X_train__tensor, y_train__tensor)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders for training and validation sets\n",
    "train__loader = DataLoader(train__dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Define d_out and d_in\n",
    "d_out = 1  \n",
    "d_in=X_train_.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.228778214732248"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constant_prediction_train = np.array([np.mean(y_train)]*len(y_train))\n",
    "# Calculate the standard deviation of the residuals\n",
    "std_dev = np.std(y_train - constant_prediction_train)\n",
    "constant_prediction = np.array([np.mean(y_train)]*len(y_test))\n",
    "# Calculate the CRPS for each prediction\n",
    "crps_values = [crps_gaussian(y_test_np[i], mu=constant_prediction[i], sig=std_dev) for i in range(len(y_test_np))]\n",
    "CRPS_constant = np.mean(crps_values)\n",
    "CRPS_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([88.82481306, 88.82481306, 88.82481306, ..., 88.82481306,\n",
       "       88.82481306, 88.82481306])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constant_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "148.67837458822024"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CRPS_constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[26.782906346427744,\n",
       " 185.55074938195384,\n",
       " 26.79682991991504,\n",
       " 110.59774647665711,\n",
       " 26.814233393032794,\n",
       " 187.49243491262348,\n",
       " 26.838596400639656,\n",
       " 177.8113249737905,\n",
       " 157.74788152939988,\n",
       " 180.70825991211495,\n",
       " 110.59774647665711,\n",
       " 180.70825991211495,\n",
       " 180.70825991211495,\n",
       " 26.782906346427744,\n",
       " 183.61169292788176,\n",
       " 185.55074938195384,\n",
       " 26.782906346427744,\n",
       " 110.59774647665711,\n",
       " 184.58088626734454,\n",
       " 184.58088626734454,\n",
       " 176.84718750657188,\n",
       " 26.869917088485956,\n",
       " 167.25097941447643,\n",
       " 168.20667799635712,\n",
       " 189.43665132362818,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 173.95949504068867,\n",
       " 177.8113249737905,\n",
       " 183.61169292788176,\n",
       " 26.786387306053033,\n",
       " 110.59774647665711,\n",
       " 176.84718750657188,\n",
       " 110.59774647665711,\n",
       " 177.8113249737905,\n",
       " 110.59774647665711,\n",
       " 183.61169292788176,\n",
       " 165.34238663820142,\n",
       " 110.59774647665711,\n",
       " 173.95949504068867,\n",
       " 184.58088626734454,\n",
       " 26.782906346427744,\n",
       " 110.59774647665711,\n",
       " 26.79682991991504,\n",
       " 162.4867573925009,\n",
       " 26.838596400639656,\n",
       " 174.92125813966973,\n",
       " 182.64318210388237,\n",
       " 182.64318210388237,\n",
       " 26.782906346427744,\n",
       " 190.4096787244055,\n",
       " 169.16328974732107,\n",
       " 183.61169292788176,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 181.67536672505793,\n",
       " 187.49243491262348,\n",
       " 186.52126971917383,\n",
       " 26.786387306053033,\n",
       " 185.55074938195384,\n",
       " 110.59774647665711,\n",
       " 174.92125813966973,\n",
       " 171.07918915374282,\n",
       " 187.49243491262348,\n",
       " 189.43665132362818,\n",
       " 188.4642327804397,\n",
       " 163.43764332853803,\n",
       " 178.77622543118278,\n",
       " 110.59774647665711,\n",
       " 187.49243491262348,\n",
       " 174.92125813966973,\n",
       " 188.4642327804397,\n",
       " 26.838596400639656,\n",
       " 177.8113249737905,\n",
       " 185.55074938195384,\n",
       " 188.4642327804397,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 174.92125813966973,\n",
       " 26.869917088485956,\n",
       " 182.64318210388237,\n",
       " 168.20667799635712,\n",
       " 187.49243491262348,\n",
       " 180.70825991211495,\n",
       " 26.838596400639656,\n",
       " 181.67536672505793,\n",
       " 176.84718750657188,\n",
       " 178.77622543118278,\n",
       " 170.12079871377787,\n",
       " 187.49243491262348,\n",
       " 110.59774647665711,\n",
       " 181.67536672505793,\n",
       " 187.49243491262348,\n",
       " 110.59774647665711,\n",
       " 174.92125813966973,\n",
       " 190.4096787244055,\n",
       " 190.4096787244055,\n",
       " 161.53688486242672,\n",
       " 181.67536672505793,\n",
       " 181.67536672505793,\n",
       " 161.53688486242672,\n",
       " 187.49243491262348,\n",
       " 110.59774647665711,\n",
       " 184.58088626734454,\n",
       " 163.43764332853803,\n",
       " 184.58088626734454,\n",
       " 187.49243491262348,\n",
       " 184.58088626734454,\n",
       " 179.74187497834234,\n",
       " 161.53688486242672,\n",
       " 26.79682991991504,\n",
       " 188.4642327804397,\n",
       " 189.43665132362818,\n",
       " 180.70825991211495,\n",
       " 178.77622543118278,\n",
       " 168.20667799635712,\n",
       " 190.4096787244055,\n",
       " 178.77622543118278,\n",
       " 168.20667799635712,\n",
       " 26.814233393032794,\n",
       " 110.59774647665711,\n",
       " 189.43665132362818,\n",
       " 180.70825991211495,\n",
       " 182.64318210388237,\n",
       " 187.49243491262348,\n",
       " 110.59774647665711,\n",
       " 188.4642327804397,\n",
       " 170.12079871377787,\n",
       " 182.64318210388237,\n",
       " 180.70825991211495,\n",
       " 182.64318210388237,\n",
       " 182.64318210388237,\n",
       " 179.74187497834234,\n",
       " 110.59774647665711,\n",
       " 190.4096787244055,\n",
       " 188.4642327804397,\n",
       " 110.59774647665711,\n",
       " 187.49243491262348,\n",
       " 191.38330334453087,\n",
       " 190.4096787244055,\n",
       " 182.64318210388237,\n",
       " 188.4642327804397,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 189.43665132362818,\n",
       " 173.95949504068867,\n",
       " 26.908193073262286,\n",
       " 185.55074938195384,\n",
       " 187.49243491262348,\n",
       " 26.869917088485956,\n",
       " 166.29621016818717,\n",
       " 187.49243491262348,\n",
       " 181.67536672505793,\n",
       " 187.49243491262348,\n",
       " 110.59774647665711,\n",
       " 172.99855253624364,\n",
       " 181.67536672505793,\n",
       " 26.782906346427744,\n",
       " 186.52126971917383,\n",
       " 156.80334252270154,\n",
       " 26.782906346427744,\n",
       " 110.59774647665711,\n",
       " 180.70825991211495,\n",
       " 173.95949504068867,\n",
       " 110.59774647665711,\n",
       " 158.69352351632753,\n",
       " 178.77622543118278,\n",
       " 178.77622543118278,\n",
       " 185.55074938195384,\n",
       " 26.814233393032794,\n",
       " 185.55074938195384,\n",
       " 184.58088626734454,\n",
       " 189.43665132362818,\n",
       " 161.53688486242672,\n",
       " 178.77622543118278,\n",
       " 190.4096787244055,\n",
       " 185.55074938195384,\n",
       " 26.95342144314358,\n",
       " 179.74187497834234,\n",
       " 110.59774647665711,\n",
       " 174.92125813966973,\n",
       " 170.12079871377787,\n",
       " 190.4096787244055,\n",
       " 110.59774647665711,\n",
       " 186.52126971917383,\n",
       " 161.53688486242672,\n",
       " 177.8113249737905,\n",
       " 191.38330334453087,\n",
       " 176.84718750657188,\n",
       " 110.59774647665711,\n",
       " 167.25097941447643,\n",
       " 110.59774647665711,\n",
       " 187.49243491262348,\n",
       " 171.07918915374282,\n",
       " 26.786387306053033,\n",
       " 26.782906346427744,\n",
       " 163.43764332853803,\n",
       " 186.52126971917383,\n",
       " 190.4096787244055,\n",
       " 176.84718750657188,\n",
       " 179.74187497834234,\n",
       " 180.70825991211495,\n",
       " 165.34238663820142,\n",
       " 188.4642327804397,\n",
       " 175.88382712870967,\n",
       " 160.58804320837632,\n",
       " 184.58088626734454,\n",
       " 162.4867573925009,\n",
       " 181.67536672505793,\n",
       " 164.38952542069475,\n",
       " 188.4642327804397,\n",
       " 110.59774647665711,\n",
       " 171.07918915374282,\n",
       " 167.25097941447643,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 26.908193073262286,\n",
       " 184.58088626734454,\n",
       " 26.79682991991504,\n",
       " 178.77622543118278,\n",
       " 185.55074938195384,\n",
       " 183.61169292788176,\n",
       " 177.8113249737905,\n",
       " 110.59774647665711,\n",
       " 191.38330334453087,\n",
       " 169.16328974732107,\n",
       " 171.07918915374282,\n",
       " 171.07918915374282,\n",
       " 184.58088626734454,\n",
       " 185.55074938195384,\n",
       " 190.4096787244055,\n",
       " 110.59774647665711,\n",
       " 178.77622543118278,\n",
       " 174.92125813966973,\n",
       " 180.70825991211495,\n",
       " 110.59774647665711,\n",
       " 167.25097941447643,\n",
       " 26.79682991991504,\n",
       " 187.49243491262348,\n",
       " 190.4096787244055,\n",
       " 178.77622543118278,\n",
       " 176.84718750657188,\n",
       " 189.43665132362818,\n",
       " 190.4096787244055,\n",
       " 186.52126971917383,\n",
       " 172.99855253624364,\n",
       " 110.59774647665711,\n",
       " 183.61169292788176,\n",
       " 176.84718750657188,\n",
       " 174.92125813966973,\n",
       " 180.70825991211495,\n",
       " 184.58088626734454,\n",
       " 187.49243491262348,\n",
       " 186.52126971917383,\n",
       " 26.869917088485956,\n",
       " 184.58088626734454,\n",
       " 172.03844553550053,\n",
       " 110.59774647665711,\n",
       " 173.95949504068867,\n",
       " 110.59774647665711,\n",
       " 174.92125813966973,\n",
       " 173.95949504068867,\n",
       " 179.74187497834234,\n",
       " 173.95949504068867,\n",
       " 26.786387306053033,\n",
       " 184.58088626734454,\n",
       " 184.58088626734454,\n",
       " 188.4642327804397,\n",
       " 162.4867573925009,\n",
       " 172.03844553550053,\n",
       " 172.99855253624364,\n",
       " 185.55074938195384,\n",
       " 190.4096787244055,\n",
       " 175.88382712870967,\n",
       " 175.88382712870967,\n",
       " 177.8113249737905,\n",
       " 171.07918915374282,\n",
       " 191.38330334453087,\n",
       " 26.782906346427744,\n",
       " 110.59774647665711,\n",
       " 183.61169292788176,\n",
       " 164.38952542069475,\n",
       " 170.12079871377787,\n",
       " 26.79682991991504,\n",
       " 168.20667799635712,\n",
       " 165.34238663820142,\n",
       " 155.85992508343276,\n",
       " 172.99855253624364,\n",
       " 26.79682991991504,\n",
       " 178.77622543118278,\n",
       " 110.59774647665711,\n",
       " 161.53688486242672,\n",
       " 178.77622543118278,\n",
       " 189.43665132362818,\n",
       " 110.59774647665711,\n",
       " 175.88382712870967,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 26.95342144314358,\n",
       " 178.77622543118278,\n",
       " 178.77622543118278,\n",
       " 26.869917088485956,\n",
       " 110.59774647665711,\n",
       " 175.88382712870967,\n",
       " 181.67536672505793,\n",
       " 110.59774647665711,\n",
       " 172.99855253624364,\n",
       " 26.869917088485956,\n",
       " 186.52126971917383,\n",
       " 172.03844553550053,\n",
       " 182.64318210388237,\n",
       " 185.55074938195384,\n",
       " 189.43665132362818,\n",
       " 160.58804320837632,\n",
       " 27.005598758453633,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 183.61169292788176,\n",
       " 172.03844553550053,\n",
       " 176.84718750657188,\n",
       " 26.782906346427744,\n",
       " 178.77622543118278,\n",
       " 175.88382712870967,\n",
       " 110.59774647665711,\n",
       " 174.92125813966973,\n",
       " 26.782906346427744,\n",
       " 167.25097941447643,\n",
       " 110.59774647665711,\n",
       " 26.79682991991504,\n",
       " 160.58804320837632,\n",
       " 163.43764332853803,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 26.79682991991504,\n",
       " 110.59774647665711,\n",
       " 177.8113249737905,\n",
       " 188.4642327804397,\n",
       " 190.4096787244055,\n",
       " 177.8113249737905,\n",
       " 180.70825991211495,\n",
       " 110.59774647665711,\n",
       " 175.88382712870967,\n",
       " 156.80334252270154,\n",
       " 26.838596400639656,\n",
       " 182.64318210388237,\n",
       " 110.59774647665711,\n",
       " 26.838596400639656,\n",
       " 182.64318210388237,\n",
       " 168.20667799635712,\n",
       " 176.84718750657188,\n",
       " 177.8113249737905,\n",
       " 176.84718750657188,\n",
       " 181.67536672505793,\n",
       " 27.064721052449833,\n",
       " 178.77622543118278,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 176.84718750657188,\n",
       " 181.67536672505793,\n",
       " 172.99855253624364,\n",
       " 184.58088626734454,\n",
       " 110.59774647665711,\n",
       " 186.52126971917383,\n",
       " 110.59774647665711,\n",
       " 184.58088626734454,\n",
       " 171.07918915374282,\n",
       " 110.59774647665711,\n",
       " 173.95949504068867,\n",
       " 111.45625151651896,\n",
       " 26.782906346427744,\n",
       " 180.70825991211495,\n",
       " 187.49243491262348,\n",
       " 181.67536672505793,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 26.838596400639656,\n",
       " 26.79682991991504,\n",
       " 158.69352351632753,\n",
       " 182.64318210388237,\n",
       " 176.84718750657188,\n",
       " 190.4096787244055,\n",
       " 110.59774647665711,\n",
       " 172.03844553550053,\n",
       " 171.07918915374282,\n",
       " 110.59774647665711,\n",
       " 166.29621016818717,\n",
       " 190.4096787244055,\n",
       " 184.58088626734454,\n",
       " 110.59774647665711,\n",
       " 167.25097941447643,\n",
       " 178.77622543118278,\n",
       " 110.59774647665711,\n",
       " 176.84718750657188,\n",
       " 181.67536672505793,\n",
       " 170.12079871377787,\n",
       " 185.55074938195384,\n",
       " 169.16328974732107,\n",
       " 26.782906346427744,\n",
       " 172.03844553550053,\n",
       " 183.61169292788176,\n",
       " 167.25097941447643,\n",
       " 191.38330334453087,\n",
       " 26.838596400639656,\n",
       " 179.74187497834234,\n",
       " 26.95342144314358,\n",
       " 166.29621016818717,\n",
       " 26.782906346427744,\n",
       " 175.88382712870967,\n",
       " 111.45625151651896,\n",
       " 110.59774647665711,\n",
       " 189.43665132362818,\n",
       " 178.77622543118278,\n",
       " 26.908193073262286,\n",
       " 26.782906346427744,\n",
       " 177.8113249737905,\n",
       " 179.74187497834234,\n",
       " 191.38330334453087,\n",
       " 191.38330334453087,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 167.25097941447643,\n",
       " 110.59774647665711,\n",
       " 167.25097941447643,\n",
       " 165.34238663820142,\n",
       " 176.84718750657188,\n",
       " 110.59774647665711,\n",
       " 189.43665132362818,\n",
       " 110.59774647665711,\n",
       " 26.838596400639656,\n",
       " 162.4867573925009,\n",
       " 185.55074938195384,\n",
       " 187.49243491262348,\n",
       " 164.38952542069475,\n",
       " 110.59774647665711,\n",
       " 184.58088626734454,\n",
       " 184.58088626734454,\n",
       " 172.03844553550053,\n",
       " 110.59774647665711,\n",
       " 179.74187497834234,\n",
       " 180.70825991211495,\n",
       " 190.4096787244055,\n",
       " 155.85992508343276,\n",
       " 191.38330334453087,\n",
       " 183.61169292788176,\n",
       " 185.55074938195384,\n",
       " 188.4642327804397,\n",
       " 180.70825991211495,\n",
       " 110.59774647665711,\n",
       " 170.12079871377787,\n",
       " 185.55074938195384,\n",
       " 174.92125813966973,\n",
       " 188.4642327804397,\n",
       " 110.59774647665711,\n",
       " 185.55074938195384,\n",
       " 26.79682991991504,\n",
       " 190.4096787244055,\n",
       " 190.4096787244055,\n",
       " 110.59774647665711,\n",
       " 184.58088626734454,\n",
       " 110.59774647665711,\n",
       " 172.99855253624364,\n",
       " 179.74187497834234,\n",
       " 163.43764332853803,\n",
       " 186.52126971917383,\n",
       " 179.74187497834234,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 175.88382712870967,\n",
       " 26.782906346427744,\n",
       " 110.59774647665711,\n",
       " 161.53688486242672,\n",
       " 26.782906346427744,\n",
       " 187.49243491262348,\n",
       " 27.064721052449833,\n",
       " 172.99855253624364,\n",
       " 26.814233393032794,\n",
       " 189.43665132362818,\n",
       " 181.67536672505793,\n",
       " 191.38330334453087,\n",
       " 175.88382712870967,\n",
       " 180.70825991211495,\n",
       " 181.67536672505793,\n",
       " 179.74187497834234,\n",
       " 168.20667799635712,\n",
       " 168.20667799635712,\n",
       " 26.786387306053033,\n",
       " 110.59774647665711,\n",
       " 170.12079871377787,\n",
       " 181.67536672505793,\n",
       " 181.67536672505793,\n",
       " 26.814233393032794,\n",
       " 26.838596400639656,\n",
       " 110.59774647665711,\n",
       " 176.84718750657188,\n",
       " 110.59774647665711,\n",
       " 26.838596400639656,\n",
       " 180.70825991211495,\n",
       " 188.4642327804397,\n",
       " 190.4096787244055,\n",
       " 111.45625151651896,\n",
       " 175.88382712870967,\n",
       " 179.74187497834234,\n",
       " 26.786387306053033,\n",
       " 110.59774647665711,\n",
       " 162.4867573925009,\n",
       " 159.6402501217417,\n",
       " 183.61169292788176,\n",
       " 177.8113249737905,\n",
       " 27.130783832227916,\n",
       " 187.49243491262348,\n",
       " 186.52126971917383,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 157.74788152939988,\n",
       " 170.12079871377787,\n",
       " 110.59774647665711,\n",
       " 26.782906346427744,\n",
       " 178.77622543118278,\n",
       " 189.43665132362818,\n",
       " 110.59774647665711,\n",
       " 26.782906346427744,\n",
       " 184.58088626734454,\n",
       " 110.59774647665711,\n",
       " 191.38330334453087,\n",
       " 187.49243491262348,\n",
       " 162.4867573925009,\n",
       " 26.782906346427744,\n",
       " 191.38330334453087,\n",
       " 180.70825991211495,\n",
       " 187.49243491262348,\n",
       " 26.782906346427744,\n",
       " 189.43665132362818,\n",
       " 110.59774647665711,\n",
       " 172.99855253624364,\n",
       " 190.4096787244055,\n",
       " 26.79682991991504,\n",
       " 182.64318210388237,\n",
       " 182.64318210388237,\n",
       " 26.908193073262286,\n",
       " 26.782906346427744,\n",
       " 175.88382712870967,\n",
       " 110.59774647665711,\n",
       " 170.12079871377787,\n",
       " 164.38952542069475,\n",
       " 191.38330334453087,\n",
       " 110.59774647665711,\n",
       " 179.74187497834234,\n",
       " 174.92125813966973,\n",
       " 110.59774647665711,\n",
       " 26.869917088485956,\n",
       " 180.70825991211495,\n",
       " 26.838596400639656,\n",
       " 110.59774647665711,\n",
       " 169.16328974732107,\n",
       " 189.43665132362818,\n",
       " 26.79682991991504,\n",
       " 26.814233393032794,\n",
       " 181.67536672505793,\n",
       " 183.61169292788176,\n",
       " 172.03844553550053,\n",
       " 179.74187497834234,\n",
       " 188.4642327804397,\n",
       " 26.782906346427744,\n",
       " 188.4642327804397,\n",
       " 172.99855253624364,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 177.8113249737905,\n",
       " 164.38952542069475,\n",
       " 111.45625151651896,\n",
       " 165.34238663820142,\n",
       " 174.92125813966973,\n",
       " 160.58804320837632,\n",
       " 26.869917088485956,\n",
       " 165.34238663820142,\n",
       " 191.38330334453087,\n",
       " 182.64318210388237,\n",
       " 172.03844553550053,\n",
       " 168.20667799635712,\n",
       " 26.838596400639656,\n",
       " 110.59774647665711,\n",
       " 167.25097941447643,\n",
       " 177.8113249737905,\n",
       " 173.95949504068867,\n",
       " 170.12079871377787,\n",
       " 110.59774647665711,\n",
       " 159.6402501217417,\n",
       " 190.4096787244055,\n",
       " 176.84718750657188,\n",
       " 26.79682991991504,\n",
       " 110.59774647665711,\n",
       " 159.6402501217417,\n",
       " 174.92125813966973,\n",
       " 110.59774647665711,\n",
       " 187.49243491262348,\n",
       " 165.34238663820142,\n",
       " 191.38330334453087,\n",
       " 172.99855253624364,\n",
       " 171.07918915374282,\n",
       " 173.95949504068867,\n",
       " 190.4096787244055,\n",
       " 26.786387306053033,\n",
       " 181.67536672505793,\n",
       " 179.74187497834234,\n",
       " 111.45625151651896,\n",
       " 188.4642327804397,\n",
       " 176.84718750657188,\n",
       " 169.16328974732107,\n",
       " 110.59774647665711,\n",
       " 164.38952542069475,\n",
       " 187.49243491262348,\n",
       " 170.12079871377787,\n",
       " 164.38952542069475,\n",
       " 187.49243491262348,\n",
       " 184.58088626734454,\n",
       " 26.782906346427744,\n",
       " 172.99855253624364,\n",
       " 168.20667799635712,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 26.782906346427744,\n",
       " 168.20667799635712,\n",
       " 187.49243491262348,\n",
       " 172.03844553550053,\n",
       " 26.838596400639656,\n",
       " 111.45625151651896,\n",
       " 177.8113249737905,\n",
       " 189.43665132362818,\n",
       " 26.814233393032794,\n",
       " 183.61169292788176,\n",
       " 175.88382712870967,\n",
       " 173.95949504068867,\n",
       " 173.95949504068867,\n",
       " 110.59774647665711,\n",
       " 187.49243491262348,\n",
       " 26.838596400639656,\n",
       " 166.29621016818717,\n",
       " 191.38330334453087,\n",
       " 26.838596400639656,\n",
       " 172.99855253624364,\n",
       " 179.74187497834234,\n",
       " 177.8113249737905,\n",
       " 164.38952542069475,\n",
       " 110.59774647665711,\n",
       " 181.67536672505793,\n",
       " 185.55074938195384,\n",
       " 26.838596400639656,\n",
       " 161.53688486242672,\n",
       " 186.52126971917383,\n",
       " 162.4867573925009,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 171.07918915374282,\n",
       " 184.58088626734454,\n",
       " 179.74187497834234,\n",
       " 110.59774647665711,\n",
       " 189.43665132362818,\n",
       " 166.29621016818717,\n",
       " 177.8113249737905,\n",
       " 184.58088626734454,\n",
       " 110.59774647665711,\n",
       " 175.88382712870967,\n",
       " 181.67536672505793,\n",
       " 110.59774647665711,\n",
       " 190.4096787244055,\n",
       " 26.786387306053033,\n",
       " 168.20667799635712,\n",
       " 187.49243491262348,\n",
       " 176.84718750657188,\n",
       " 189.43665132362818,\n",
       " 26.782906346427744,\n",
       " 177.8113249737905,\n",
       " 175.88382712870967,\n",
       " 166.29621016818717,\n",
       " 26.782906346427744,\n",
       " 189.43665132362818,\n",
       " 181.67536672505793,\n",
       " 190.4096787244055,\n",
       " 110.59774647665711,\n",
       " 180.70825991211495,\n",
       " 110.59774647665711,\n",
       " 172.03844553550053,\n",
       " 26.782906346427744,\n",
       " 171.07918915374282,\n",
       " 110.59774647665711,\n",
       " 152.0978506875872,\n",
       " 189.43665132362818,\n",
       " 175.88382712870967,\n",
       " 182.64318210388237,\n",
       " 26.786387306053033,\n",
       " 178.77622543118278,\n",
       " 176.84718750657188,\n",
       " 159.6402501217417,\n",
       " 26.786387306053033,\n",
       " 190.4096787244055,\n",
       " 180.70825991211495,\n",
       " 188.4642327804397,\n",
       " 177.8113249737905,\n",
       " 176.84718750657188,\n",
       " 191.38330334453087,\n",
       " 180.70825991211495,\n",
       " 165.34238663820142,\n",
       " 183.61169292788176,\n",
       " 180.70825991211495,\n",
       " 180.70825991211495,\n",
       " 110.59774647665711,\n",
       " 187.49243491262348,\n",
       " 110.59774647665711,\n",
       " 186.52126971917383,\n",
       " 180.70825991211495,\n",
       " 110.59774647665711,\n",
       " 175.88382712870967,\n",
       " 179.74187497834234,\n",
       " 170.12079871377787,\n",
       " 182.64318210388237,\n",
       " 26.79682991991504,\n",
       " 170.12079871377787,\n",
       " 176.84718750657188,\n",
       " 178.77622543118278,\n",
       " 163.43764332853803,\n",
       " 174.92125813966973,\n",
       " 172.99855253624364,\n",
       " 111.45625151651896,\n",
       " 174.92125813966973,\n",
       " 176.84718750657188,\n",
       " 110.59774647665711,\n",
       " 167.25097941447643,\n",
       " 184.58088626734454,\n",
       " 172.99855253624364,\n",
       " 110.59774647665711,\n",
       " 165.34238663820142,\n",
       " 184.58088626734454,\n",
       " 163.43764332853803,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 181.67536672505793,\n",
       " 187.49243491262348,\n",
       " 26.782906346427744,\n",
       " 177.8113249737905,\n",
       " 26.814233393032794,\n",
       " 184.58088626734454,\n",
       " 169.16328974732107,\n",
       " 166.29621016818717,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 167.25097941447643,\n",
       " 179.74187497834234,\n",
       " 26.838596400639656,\n",
       " 181.67536672505793,\n",
       " 168.20667799635712,\n",
       " 26.79682991991504,\n",
       " 170.12079871377787,\n",
       " 110.59774647665711,\n",
       " 160.58804320837632,\n",
       " 183.61169292788176,\n",
       " 154.91764802519648,\n",
       " 172.99855253624364,\n",
       " 172.03844553550053,\n",
       " 155.85992508343276,\n",
       " 178.77622543118278,\n",
       " 174.92125813966973,\n",
       " 188.4642327804397,\n",
       " 170.12079871377787,\n",
       " 110.59774647665711,\n",
       " 183.61169292788176,\n",
       " 184.58088626734454,\n",
       " 181.67536672505793,\n",
       " 110.59774647665711,\n",
       " 191.38330334453087,\n",
       " 187.49243491262348,\n",
       " 110.59774647665711,\n",
       " 181.67536672505793,\n",
       " 110.59774647665711,\n",
       " 26.79682991991504,\n",
       " 185.55074938195384,\n",
       " 169.16328974732107,\n",
       " 185.55074938195384,\n",
       " 164.38952542069475,\n",
       " 184.58088626734454,\n",
       " 110.59774647665711,\n",
       " 26.786387306053033,\n",
       " 26.79682991991504,\n",
       " 184.58088626734454,\n",
       " 183.61169292788176,\n",
       " 174.92125813966973,\n",
       " 160.58804320837632,\n",
       " 27.203782079746166,\n",
       " 110.59774647665711,\n",
       " 187.49243491262348,\n",
       " 167.25097941447643,\n",
       " 110.59774647665711,\n",
       " 185.55074938195384,\n",
       " 182.64318210388237,\n",
       " 183.61169292788176,\n",
       " 160.58804320837632,\n",
       " 162.4867573925009,\n",
       " 191.38330334453087,\n",
       " 173.95949504068867,\n",
       " 159.6402501217417,\n",
       " 110.59774647665711,\n",
       " 172.99855253624364,\n",
       " 189.43665132362818,\n",
       " 169.16328974732107,\n",
       " 160.58804320837632,\n",
       " 188.4642327804397,\n",
       " 170.12079871377787,\n",
       " 156.80334252270154,\n",
       " 189.43665132362818,\n",
       " 172.03844553550053,\n",
       " 169.16328974732107,\n",
       " 179.74187497834234,\n",
       " 190.4096787244055,\n",
       " 181.67536672505793,\n",
       " 191.38330334453087,\n",
       " 187.49243491262348,\n",
       " 110.59774647665711,\n",
       " 189.43665132362818,\n",
       " 186.52126971917383,\n",
       " 165.34238663820142,\n",
       " 181.67536672505793,\n",
       " 110.59774647665711,\n",
       " 188.4642327804397,\n",
       " 175.88382712870967,\n",
       " 184.58088626734454,\n",
       " 186.52126971917383,\n",
       " 172.99855253624364,\n",
       " 181.67536672505793,\n",
       " 187.49243491262348,\n",
       " 190.4096787244055,\n",
       " 26.786387306053033,\n",
       " 110.59774647665711,\n",
       " 171.07918915374282,\n",
       " 110.59774647665711,\n",
       " 169.16328974732107,\n",
       " 26.814233393032794,\n",
       " 110.59774647665711,\n",
       " 161.53688486242672,\n",
       " 26.95342144314358,\n",
       " 110.59774647665711,\n",
       " 27.064721052449833,\n",
       " 176.84718750657188,\n",
       " 155.85992508343276,\n",
       " 191.38330334453087,\n",
       " 185.55074938195384,\n",
       " 173.95949504068867,\n",
       " 179.74187497834234,\n",
       " 153.97653038890778,\n",
       " 110.59774647665711,\n",
       " 179.74187497834234,\n",
       " 178.77622543118278,\n",
       " 26.79682991991504,\n",
       " 26.786387306053033,\n",
       " 183.61169292788176,\n",
       " 185.55074938195384,\n",
       " 178.77622543118278,\n",
       " 190.4096787244055,\n",
       " 26.79682991991504,\n",
       " 186.52126971917383,\n",
       " 183.61169292788176,\n",
       " 177.8113249737905,\n",
       " 190.4096787244055,\n",
       " 181.67536672505793,\n",
       " 186.52126971917383,\n",
       " 185.55074938195384,\n",
       " 26.79682991991504,\n",
       " 182.64318210388237,\n",
       " 172.99855253624364,\n",
       " 182.64318210388237,\n",
       " 110.59774647665711,\n",
       " 185.55074938195384,\n",
       " 168.20667799635712,\n",
       " 167.25097941447643,\n",
       " 173.95949504068867,\n",
       " 110.59774647665711,\n",
       " 185.55074938195384,\n",
       " 169.16328974732107,\n",
       " 26.814233393032794,\n",
       " 185.55074938195384,\n",
       " 180.70825991211495,\n",
       " 190.4096787244055,\n",
       " 186.52126971917383,\n",
       " 180.70825991211495,\n",
       " 162.4867573925009,\n",
       " 169.16328974732107,\n",
       " 187.49243491262348,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 181.67536672505793,\n",
       " 176.84718750657188,\n",
       " 171.07918915374282,\n",
       " 181.67536672505793,\n",
       " 110.59774647665711,\n",
       " 177.8113249737905,\n",
       " 26.786387306053033,\n",
       " 185.55074938195384,\n",
       " 26.782906346427744,\n",
       " 186.52126971917383,\n",
       " 183.61169292788176,\n",
       " 186.52126971917383,\n",
       " 172.99855253624364,\n",
       " 185.55074938195384,\n",
       " 178.77622543118278,\n",
       " 164.38952542069475,\n",
       " 26.782906346427744,\n",
       " 189.43665132362818,\n",
       " 165.34238663820142,\n",
       " 110.59774647665711,\n",
       " 163.43764332853803,\n",
       " 158.69352351632753,\n",
       " 172.03844553550053,\n",
       " 171.07918915374282,\n",
       " 176.84718750657188,\n",
       " 191.38330334453087,\n",
       " 168.20667799635712,\n",
       " 26.782906346427744,\n",
       " 188.4642327804397,\n",
       " 110.59774647665711,\n",
       " 187.49243491262348,\n",
       " 186.52126971917383,\n",
       " 184.58088626734454,\n",
       " 110.59774647665711,\n",
       " 188.4642327804397,\n",
       " 183.61169292788176,\n",
       " 168.20667799635712,\n",
       " 173.95949504068867,\n",
       " 188.4642327804397,\n",
       " 163.43764332853803,\n",
       " 180.70825991211495,\n",
       " 26.786387306053033,\n",
       " 183.61169292788176,\n",
       " 188.4642327804397,\n",
       " 177.8113249737905,\n",
       " 110.59774647665711,\n",
       " 187.49243491262348,\n",
       " 186.52126971917383,\n",
       " 186.52126971917383,\n",
       " 110.59774647665711,\n",
       " 183.61169292788176,\n",
       " 181.67536672505793,\n",
       " 172.03844553550053,\n",
       " 185.55074938195384,\n",
       " 175.88382712870967,\n",
       " 184.58088626734454,\n",
       " 170.12079871377787,\n",
       " 110.59774647665711,\n",
       " 183.61169292788176,\n",
       " 164.38952542069475,\n",
       " 26.814233393032794,\n",
       " 175.88382712870967,\n",
       " 176.84718750657188,\n",
       " 186.52126971917383,\n",
       " 188.4642327804397,\n",
       " 165.34238663820142,\n",
       " 187.49243491262348,\n",
       " 187.49243491262348,\n",
       " 179.74187497834234,\n",
       " 189.43665132362818,\n",
       " 177.8113249737905,\n",
       " 172.03844553550053,\n",
       " 179.74187497834234,\n",
       " 26.79682991991504,\n",
       " 171.07918915374282,\n",
       " 178.77622543118278,\n",
       " 169.16328974732107,\n",
       " 187.49243491262348,\n",
       " 182.64318210388237,\n",
       " 191.38330334453087,\n",
       " 176.84718750657188,\n",
       " 110.59774647665711,\n",
       " 26.79682991991504,\n",
       " 171.07918915374282,\n",
       " 26.838596400639656,\n",
       " 181.67536672505793,\n",
       " 182.64318210388237,\n",
       " 110.59774647665711,\n",
       " 110.59774647665711,\n",
       " 173.95949504068867,\n",
       " 189.43665132362818,\n",
       " 168.20667799635712,\n",
       " 26.782906346427744,\n",
       " 183.61169292788176,\n",
       " 190.4096787244055,\n",
       " 111.45625151651896,\n",
       " 172.99855253624364,\n",
       " 172.99855253624364,\n",
       " 26.95342144314358,\n",
       " 26.782906346427744,\n",
       " 182.64318210388237,\n",
       " 181.67536672505793,\n",
       " 178.77622543118278,\n",
       " 190.4096787244055,\n",
       " 191.38330334453087,\n",
       " 110.59774647665711,\n",
       " ...]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([88, 88, 88, ..., 88, 88, 88], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "constant_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       90\n",
       "2       85\n",
       "4       79\n",
       "5       92\n",
       "6       82\n",
       "        ..\n",
       "8186    85\n",
       "8188    88\n",
       "8189    92\n",
       "8190    96\n",
       "8191    80\n",
       "Name: usr, Length: 6553, dtype: uint8"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "361055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16714, 10)\n",
      "361060\n",
      "(38474, 7)\n",
      "361061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(566602, 10)\n",
      "361062\n",
      "(10082, 26)\n",
      "361063\n",
      "(13488, 16)\n",
      "361065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13376, 10)\n",
      "361066\n",
      "(10578, 7)\n",
      "361068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(72998, 50)\n",
      "361069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(940160, 24)\n",
      "361070\n",
      "(7608, 20)\n",
      "361273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(71090, 7)\n",
      "361274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(57580, 54)\n",
      "361275\n",
      "(13272, 20)\n",
      "361276\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3434, 419)\n",
      "361277\n",
      "(20634, 8)\n",
      "361278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 22)\n"
     ]
    }
   ],
   "source": [
    "#SUITE_ID = 336 # Regression on numerical features\n",
    "SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "print(len(benchmark_suite.tasks))\n",
    "\n",
    "for task_id in benchmark_suite.tasks:\n",
    "    print(task_id)\n",
    "    task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "    dataset = task.get_dataset()\n",
    "\n",
    "    X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "            dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "    print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'openml' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#SUITE_ID = 336 # Regression on numerical features\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#SUITE_ID = 337 # Classification on numerical features\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#SUITE_ID = 335 # Regression on numerical and categorical features\u001b[39;00m\n\u001b[0;32m      4\u001b[0m SUITE_ID \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m334\u001b[39m \u001b[38;5;66;03m# Classification on numerical and categorical features\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m benchmark_suite \u001b[38;5;241m=\u001b[39m \u001b[43mopenml\u001b[49m\u001b[38;5;241m.\u001b[39mstudy\u001b[38;5;241m.\u001b[39mget_suite(SUITE_ID)  \u001b[38;5;66;03m# obtain the benchmark suite\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(benchmark_suite\u001b[38;5;241m.\u001b[39mtasks))\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task_id \u001b[38;5;129;01min\u001b[39;00m benchmark_suite\u001b[38;5;241m.\u001b[39mtasks:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'openml' is not defined"
     ]
    }
   ],
   "source": [
    "#SUITE_ID = 336 # Regression on numerical features\n",
    "#SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "print(len(benchmark_suite.tasks))\n",
    "\n",
    "for task_id in benchmark_suite.tasks:\n",
    "    print(task_id)\n",
    "    task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "    dataset = task.get_dataset()\n",
    "\n",
    "    X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "            dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "    print(X.shape)\n",
    "    print(X.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[361072,\n",
       " 361073,\n",
       " 361074,\n",
       " 361076,\n",
       " 361077,\n",
       " 361078,\n",
       " 361079,\n",
       " 361080,\n",
       " 361081,\n",
       " 361082,\n",
       " 361083,\n",
       " 361084,\n",
       " 361085,\n",
       " 361086,\n",
       " 361087,\n",
       " 361088,\n",
       " 361279,\n",
       " 361280,\n",
       " 361281]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_suite.tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    }
   ],
   "source": [
    "print(X.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 361074\n",
      "Task 361074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "task_id=361074\n",
    "\n",
    "print(f\"Task {task_id}\")\n",
    "\n",
    "# Create the checkpoint directory if it doesn't exist\n",
    "os.makedirs('CHECKPOINTS/MAHALANOBIS', exist_ok=True)\n",
    "CHECKPOINT_PATH = f'CHECKPOINTS/MAHALANOBIS/task_{task_id}.pt'\n",
    "\n",
    "print(f\"Task {task_id}\")\n",
    "\n",
    "task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "dataset = task.get_dataset()\n",
    "\n",
    "X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>climbRate</th>\n",
       "      <th>Sgz</th>\n",
       "      <th>p</th>\n",
       "      <th>q</th>\n",
       "      <th>curRoll</th>\n",
       "      <th>absRoll</th>\n",
       "      <th>diffClb</th>\n",
       "      <th>diffRollRate</th>\n",
       "      <th>diffDiffClb</th>\n",
       "      <th>SaTime1</th>\n",
       "      <th>SaTime2</th>\n",
       "      <th>SaTime3</th>\n",
       "      <th>SaTime4</th>\n",
       "      <th>diffSaTime1</th>\n",
       "      <th>diffSaTime3</th>\n",
       "      <th>Sa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>118.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.005</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>390.0</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>-0.06</td>\n",
       "      <td>-0.07</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.0008</td>\n",
       "      <td>-0.0008</td>\n",
       "      <td>-0.0008</td>\n",
       "      <td>-0.0008</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>68.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.0011</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-358.0</td>\n",
       "      <td>-12.0</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>0.001</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-411.0</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>-0.18</td>\n",
       "      <td>0.02</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>0.002</td>\n",
       "      <td>1.2</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>-0.0010</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16594</th>\n",
       "      <td>299.0</td>\n",
       "      <td>-28.0</td>\n",
       "      <td>0.08</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16595</th>\n",
       "      <td>84.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.14</td>\n",
       "      <td>1.1</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.0009</td>\n",
       "      <td>-0.0009</td>\n",
       "      <td>-0.0009</td>\n",
       "      <td>-0.0009</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16596</th>\n",
       "      <td>-208.0</td>\n",
       "      <td>-6.0</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-9.0</td>\n",
       "      <td>-7.0</td>\n",
       "      <td>-0.010</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.0009</td>\n",
       "      <td>-0.0009</td>\n",
       "      <td>-0.0009</td>\n",
       "      <td>-0.0009</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16597</th>\n",
       "      <td>-146.0</td>\n",
       "      <td>-14.0</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-0.03</td>\n",
       "      <td>-0.8</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.010</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>-0.0005</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16598</th>\n",
       "      <td>282.0</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>-0.12</td>\n",
       "      <td>-1.2</td>\n",
       "      <td>-10.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.0004</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16599 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       climbRate   Sgz     p     q  curRoll  absRoll  diffClb  diffRollRate  \\\n",
       "0          118.0 -55.0 -0.28 -0.08     -0.2    -11.0     11.0         0.005   \n",
       "1          390.0 -45.0 -0.06 -0.07     -0.6    -12.0     11.0         0.010   \n",
       "2           68.0   6.0  0.11  0.15      0.6    -10.0     -9.0        -0.003   \n",
       "3         -358.0 -12.0 -0.20  0.13     -0.3    -11.0     -7.0         0.001   \n",
       "4         -411.0 -19.0 -0.18  0.02     -0.5    -11.0     -3.0         0.002   \n",
       "...          ...   ...   ...   ...      ...      ...      ...           ...   \n",
       "16594      299.0 -28.0  0.08 -0.12     -0.3     -9.0     15.0         0.010   \n",
       "16595       84.0   0.0  0.14  0.14      1.1     -8.0    -11.0        -0.014   \n",
       "16596     -208.0  -6.0 -0.48  0.09      0.2     -9.0     -7.0        -0.010   \n",
       "16597     -146.0 -14.0 -0.38 -0.03     -0.8    -10.0     10.0         0.010   \n",
       "16598      282.0 -11.0  0.10 -0.12     -1.2    -10.0     16.0         0.016   \n",
       "\n",
       "       diffDiffClb  SaTime1  SaTime2  SaTime3  SaTime4  diffSaTime1  \\\n",
       "0             -0.2  -0.0010  -0.0010  -0.0010  -0.0010       0.0000   \n",
       "1             -0.2  -0.0008  -0.0008  -0.0008  -0.0008       0.0000   \n",
       "2             -0.2  -0.0011  -0.0010  -0.0010  -0.0010      -0.0002   \n",
       "3             -0.1  -0.0010  -0.0010  -0.0010  -0.0010       0.0000   \n",
       "4              1.2  -0.0010  -0.0010  -0.0010  -0.0010       0.0000   \n",
       "...            ...      ...      ...      ...      ...          ...   \n",
       "16594         -0.2  -0.0005  -0.0005  -0.0005  -0.0005       0.0000   \n",
       "16595         -0.6  -0.0009  -0.0009  -0.0009  -0.0009       0.0000   \n",
       "16596         -0.1  -0.0009  -0.0009  -0.0009  -0.0009       0.0000   \n",
       "16597         -1.0  -0.0005  -0.0005  -0.0005  -0.0005       0.0000   \n",
       "16598         -0.1  -0.0004  -0.0004  -0.0004  -0.0004       0.0000   \n",
       "\n",
       "       diffSaTime3      Sa  \n",
       "0              0.0 -0.0010  \n",
       "1              0.0 -0.0008  \n",
       "2              0.0 -0.0010  \n",
       "3              0.0 -0.0010  \n",
       "4              0.0 -0.0010  \n",
       "...            ...     ...  \n",
       "16594          0.0 -0.0005  \n",
       "16595          0.0 -0.0009  \n",
       "16596          0.0 -0.0009  \n",
       "16597          0.0 -0.0005  \n",
       "16598          0.0 -0.0004  \n",
       "\n",
       "[16599 rows x 16 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "#SUITE_ID = 336 # Regression on numerical features\n",
    "#SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "print(len(benchmark_suite.tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    if i==2:\n",
    "        continue\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task 361072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\umap\\umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\umap\\umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using CPU\n"
     ]
    }
   ],
   "source": [
    "from umap import UMAP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import setuptools\n",
    "import openml\n",
    "from sklearn.linear_model import LinearRegression \n",
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from engression import engression, engression_bagged\n",
    "import torch\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rtdl_revisiting_models import MLP, ResNet, FTTransformer\n",
    "from properscoring import crps_gaussian, crps_ensemble\n",
    "import random\n",
    "import gpytorch\n",
    "import tqdm.auto as tqdm\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "import os\n",
    "from pygam import LinearGAM, s, f\n",
    "from utils import EarlyStopping, train, train_trans, train_no_early_stopping, train_trans_no_early_stopping, train_GP, ExactGPModel\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import re\n",
    "import shutil\n",
    "\n",
    "# Create the checkpoint directory if it doesn't exist\n",
    "if os.path.exists('CHECKPOINTS/MAHALANOBIS'):\n",
    "    shutil.rmtree('CHECKPOINTS/MAHALANOBIS')\n",
    "os.makedirs('CHECKPOINTS/MAHALANOBIS')\n",
    "\n",
    "SUITE_ID = 336 # Regression on numerical features\n",
    "#SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "\n",
    "task_id=361072\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "N_TRIALS=100\n",
    "N_SAMPLES=100\n",
    "PATIENCE=40\n",
    "N_EPOCHS=1000\n",
    "GP_ITERATIONS=1000\n",
    "BATCH_SIZE=1024\n",
    "seed=10\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "print(f\"Task {task_id}\")\n",
    "\n",
    "CHECKPOINT_PATH = f'CHECKPOINTS/MAHALANOBIS/task_{task_id}.pt'\n",
    "\n",
    "task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "dataset = task.get_dataset()\n",
    "\n",
    "X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "\n",
    "if len(X) > 15000:\n",
    "    indices = np.random.choice(X.index, size=15000, replace=False)\n",
    "    X = X.iloc[indices,]\n",
    "    y = y[indices]\n",
    "\n",
    "# Remove categorical columns with more than 20 unique values and non-categorical columns with less than 10 unique values\n",
    "# Remove non-categorical columns with more than 70% of the data in one category from X_clean\n",
    "for col in [attribute for attribute, indicator in zip(attribute_names, categorical_indicator) if indicator]:\n",
    "    if len(X[col].unique()) > 20:\n",
    "        X = X.drop(col, axis=1)\n",
    "\n",
    "X_clean=X.copy()\n",
    "for col in [attribute for attribute, indicator in zip(attribute_names, categorical_indicator) if not indicator]:\n",
    "    if len(X[col].unique()) < 10:\n",
    "        X = X.drop(col, axis=1)\n",
    "        X_clean = X_clean.drop(col, axis=1)\n",
    "    elif X[col].value_counts(normalize=True).max() > 0.7:\n",
    "        X_clean = X_clean.drop(col, axis=1)\n",
    "\n",
    "# Find features with absolute correlation > 0.9\n",
    "corr_matrix = X_clean.corr().abs()\n",
    "upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "high_corr_features = [column for column in upper_tri.columns if any(upper_tri[column] > 0.9)]\n",
    "\n",
    "# Drop one of the highly correlated features from X_clean\n",
    "X_clean = X_clean.drop(high_corr_features, axis=1)\n",
    "\n",
    "# Rename columns to avoid problems with LGBM\n",
    "X = X.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "\n",
    "# Apply UMAP decomposition\n",
    "umap = UMAP(n_components=2, random_state=42)\n",
    "X_umap = umap.fit_transform(X_clean)\n",
    "\n",
    "# calculate the Euclidean distance matrix\n",
    "euclidean_dist_matrix = euclidean_distances(X_umap)\n",
    "\n",
    "# calculate the Euclidean distance for each data point\n",
    "euclidean_dist = np.mean(euclidean_dist_matrix, axis=1)\n",
    "\n",
    "euclidean_dist = pd.Series(euclidean_dist, index=X_clean.index)\n",
    "far_index = euclidean_dist.index[np.where(euclidean_dist >= np.quantile(euclidean_dist, 0.8))[0]]\n",
    "close_index = euclidean_dist.index[np.where(euclidean_dist < np.quantile(euclidean_dist, 0.8))[0]]\n",
    "\n",
    "X_train_clean = X_clean.loc[close_index,:]\n",
    "X_train = X.loc[close_index,:]\n",
    "X_test = X.loc[far_index,:]\n",
    "y_train = y.loc[close_index]\n",
    "y_test = y.loc[far_index]\n",
    "\n",
    "# Apply UMAP decomposition on the training set\n",
    "X_umap_train = umap.fit_transform(X_train_clean)\n",
    "\n",
    "# calculate the Euclidean distance matrix for the training set\n",
    "euclidean_dist_matrix_train = euclidean_distances(X_umap_train)\n",
    "\n",
    "# calculate the Euclidean distance for each data point in the training set\n",
    "euclidean_dist_train = np.mean(euclidean_dist_matrix_train, axis=1)\n",
    "\n",
    "euclidean_dist_train = pd.Series(euclidean_dist_train, index=X_train_clean.index)\n",
    "far_index_train = euclidean_dist_train.index[np.where(euclidean_dist_train >= np.quantile(euclidean_dist_train, 0.8))[0]]\n",
    "close_index_train = euclidean_dist_train.index[np.where(euclidean_dist_train < np.quantile(euclidean_dist_train, 0.8))[0]]\n",
    "\n",
    "X_train_ = X_train.loc[close_index_train,:]\n",
    "X_val = X_train.loc[far_index_train,:]\n",
    "y_train_ = y_train.loc[close_index_train]\n",
    "y_val = y_train.loc[far_index_train]\n",
    "\n",
    "# Standardize the data\n",
    "mean_X_train_ = np.mean(X_train_, axis=0)\n",
    "std_X_train_ = np.std(X_train_, axis=0)\n",
    "X_train__scaled = (X_train_ - mean_X_train_) / std_X_train_\n",
    "X_val_scaled = (X_val - mean_X_train_) / std_X_train_\n",
    "\n",
    "mean_X_train = np.mean(X_train, axis=0)\n",
    "std_X_train = np.std(X_train, axis=0)\n",
    "X_train_scaled = (X_train - mean_X_train) / std_X_train\n",
    "X_test_scaled = (X_test - mean_X_train) / std_X_train\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train__tensor = torch.tensor(X_train__scaled.values, dtype=torch.float32)\n",
    "y_train__tensor = torch.tensor(y_train_.values, dtype=torch.float32)\n",
    "X_train_tensor = torch.tensor(X_train_scaled.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_scaled.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test_scaled.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Convert to use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Using GPU\")\n",
    "    X_train__tensor = X_train__tensor.cuda()\n",
    "    y_train__tensor = y_train__tensor.cuda()\n",
    "    X_train_tensor = X_train_tensor.cuda()\n",
    "    y_train_tensor = y_train_tensor.cuda()\n",
    "    X_val_tensor = X_val_tensor.cuda()\n",
    "    y_val_tensor = y_val_tensor.cuda()\n",
    "    X_test_tensor = X_test_tensor.cuda()\n",
    "    y_test_tensor = y_test_tensor.cuda()\n",
    "else:\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "# Create flattened versions of the data\n",
    "y_val_np = y_val.values.flatten()\n",
    "y_test_np = y_test.values.flatten()\n",
    "\n",
    "# Create TensorDatasets for training and validation sets\n",
    "train__dataset = TensorDataset(X_train__tensor, y_train__tensor)\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "# Create DataLoaders for training and validation sets\n",
    "train__loader = DataLoader(train__dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "# Define d_out and d_in\n",
    "d_out = 1  \n",
    "d_in=X_train_.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-27 21:12:39,629] A new study created in memory with name: no-name-e8758b3d-c272-427a-a523-34dc4e18a9c7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-27 21:12:48,947] Trial 0 finished with value: 2.136980892037132 and parameters: {'n_splines_lread': 80, 'lam_lread': 0.0013320229150659071, 'spline_order_lread': 4, 'n_splines_lwrite': 78, 'lam_lwrite': 0.9795848815655198, 'spline_order_lwrite': 2, 'n_splines_scall': 28, 'lam_scall': 36.57499481192727, 'spline_order_scall': 1, 'n_splines_sread': 18, 'lam_sread': 12.94669479886693, 'spline_order_sread': 5, 'n_splines_swrite': 10, 'lam_swrite': 1.1834599907542849, 'spline_order_swrite': 5, 'n_splines_fork': 65, 'lam_fork': 21.405821746591823, 'spline_order_fork': 2, 'n_splines_exec': 93, 'lam_exec': 19.384504329781148, 'spline_order_exec': 3, 'n_splines_rchar': 22, 'lam_rchar': 0.173797914304444, 'spline_order_rchar': 4, 'n_splines_wchar': 50, 'lam_wchar': 0.4018684945839337, 'spline_order_wchar': 4, 'n_splines_pgout': 56, 'lam_pgout': 7.986989097169455, 'spline_order_pgout': 4, 'n_splines_ppgout': 83, 'lam_ppgout': 1.3485992516968202, 'spline_order_ppgout': 5, 'n_splines_pgfree': 39, 'lam_pgfree': 0.003489442896348133, 'spline_order_pgfree': 2, 'n_splines_pgscan': 20, 'lam_pgscan': 93.77519109877728, 'spline_order_pgscan': 1, 'n_splines_atch': 66, 'lam_atch': 1.9298205889744646, 'spline_order_atch': 5, 'n_splines_pgin': 28, 'lam_pgin': 138.38907763091694, 'spline_order_pgin': 2, 'n_splines_ppgin': 78, 'lam_ppgin': 0.05967195151630889, 'spline_order_ppgin': 5, 'n_splines_pflt': 39, 'lam_pflt': 0.009774518801296312, 'spline_order_pflt': 2, 'n_splines_vflt': 18, 'lam_vflt': 84.45666867295606, 'spline_order_vflt': 1, 'n_splines_runqsz': 44, 'lam_runqsz': 462.98217777735624, 'spline_order_runqsz': 5, 'n_splines_freemem': 51, 'lam_freemem': 90.51844049299662, 'spline_order_freemem': 2, 'n_splines_freeswap': 64, 'lam_freeswap': 261.2104587991285, 'spline_order_freeswap': 3}. Best is trial 0 with value: 2.136980892037132.\n",
      "[I 2024-03-27 21:12:57,520] Trial 1 finished with value: 1.8906792908122165 and parameters: {'n_splines_lread': 63, 'lam_lread': 0.0017206423318105984, 'spline_order_lread': 2, 'n_splines_lwrite': 17, 'lam_lwrite': 0.06803924879984836, 'spline_order_lwrite': 2, 'n_splines_scall': 80, 'lam_scall': 0.0017368217621749208, 'spline_order_scall': 3, 'n_splines_sread': 38, 'lam_sread': 6.59093241654684, 'spline_order_sread': 2, 'n_splines_swrite': 13, 'lam_swrite': 190.32290024927522, 'spline_order_swrite': 4, 'n_splines_fork': 89, 'lam_fork': 0.3199299241425316, 'spline_order_fork': 4, 'n_splines_exec': 56, 'lam_exec': 3.8638467047838243, 'spline_order_exec': 2, 'n_splines_rchar': 37, 'lam_rchar': 0.0014203608472085642, 'spline_order_rchar': 2, 'n_splines_wchar': 32, 'lam_wchar': 2.215486622230635, 'spline_order_wchar': 3, 'n_splines_pgout': 53, 'lam_pgout': 0.05711995514636383, 'spline_order_pgout': 1, 'n_splines_ppgout': 99, 'lam_ppgout': 0.1092061412630602, 'spline_order_ppgout': 3, 'n_splines_pgfree': 98, 'lam_pgfree': 0.4412075640903393, 'spline_order_pgfree': 2, 'n_splines_pgscan': 57, 'lam_pgscan': 2.943191897953818, 'spline_order_pgscan': 5, 'n_splines_atch': 16, 'lam_atch': 0.6126110789567031, 'spline_order_atch': 4, 'n_splines_pgin': 75, 'lam_pgin': 3.281949395145967, 'spline_order_pgin': 1, 'n_splines_ppgin': 41, 'lam_ppgin': 2.3941100346917805, 'spline_order_ppgin': 2, 'n_splines_pflt': 56, 'lam_pflt': 10.985037699399216, 'spline_order_pflt': 1, 'n_splines_vflt': 14, 'lam_vflt': 0.10638865596892418, 'spline_order_vflt': 1, 'n_splines_runqsz': 26, 'lam_runqsz': 206.52021904394073, 'spline_order_runqsz': 2, 'n_splines_freemem': 29, 'lam_freemem': 32.732337561472754, 'spline_order_freemem': 1, 'n_splines_freeswap': 77, 'lam_freeswap': 0.6587365495478882, 'spline_order_freeswap': 3}. Best is trial 1 with value: 1.8906792908122165.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRPS GAM:  12.33195379119565\n"
     ]
    }
   ],
   "source": [
    "N_TRIALS=2\n",
    "#### GAM model\n",
    "def gam_model(trial):\n",
    "\n",
    "    n_splines = []\n",
    "    lam = []\n",
    "    spline_order = []\n",
    "\n",
    "    # Iterate over each covariate in X_train_\n",
    "    for col in X_train_.columns:\n",
    "        # Define the search space for n_splines, lam, and spline_order\n",
    "        n_splines.append(trial.suggest_int(f'n_splines_{col}', 10, 100))\n",
    "        lam.append(trial.suggest_float(f'lam_{col}', 1e-3, 1e3, log=True))\n",
    "        spline_order.append(trial.suggest_int(f'spline_order_{col}', 1, 5))\n",
    "    \n",
    "    # Create and train the model\n",
    "    gam = LinearGAM(n_splines=n_splines, spline_order=spline_order, lam=lam).fit(X_train_, y_train_)\n",
    "\n",
    "    # Predict on the validation set and calculate the CRPS\n",
    "    y_train__hat_gam = gam.predict(X_train_)\n",
    "    std_dev_error = np.std(y_train_ - y_train__hat_gam)\n",
    "    y_val_hat_gam = gam.predict(X_val)\n",
    "    crps_gam = [crps_gaussian(y_val_np[i], mu=y_val_hat_gam[i], sig=std_dev_error) for i in range(len(y_val_hat_gam))]\n",
    "    crps_gam = np.mean(crps_gam)\n",
    "\n",
    "    return crps_gam\n",
    "\n",
    "# Create the sampler and study\n",
    "sampler_gam = optuna.samplers.TPESampler(seed=seed)\n",
    "study_gam = optuna.create_study(sampler=sampler_gam, direction='minimize')\n",
    "\n",
    "# Optimize the model\n",
    "study_gam.optimize(gam_model, n_trials=N_TRIALS)\n",
    "\n",
    "n_splines = []\n",
    "lam = []\n",
    "spline_order = []\n",
    "\n",
    "# Create the final model with the best parameters\n",
    "best_params = study_gam.best_params\n",
    "\n",
    "# Iterate over each covariate in X_train_\n",
    "for col in X_train.columns:\n",
    "    # Define the search space for n_splines, lam, and spline_order\n",
    "    n_splines.append(best_params[f'n_splines_{col}'])\n",
    "    lam.append(best_params[f'lam_{col}'])\n",
    "    spline_order.append(best_params[f'spline_order_{col}'])\n",
    "\n",
    "final_gam_model = LinearGAM(n_splines=n_splines, spline_order=spline_order, lam=lam)\n",
    "\n",
    "# Fit the model\n",
    "final_gam_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the train set\n",
    "y_train_hat_gam = final_gam_model.predict(X_train)\n",
    "std_dev_error = np.std(y_train - y_train_hat_gam)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_hat_gam = final_gam_model.predict(X_test)\n",
    "\n",
    "# Calculate the CRPS\n",
    "crps_gam = [crps_gaussian(y_test_np[i], mu=y_test_hat_gam[i], sig=std_dev_error) for i in range(len(y_test_hat_gam))]\n",
    "crps_gam = np.mean(crps_gam)\n",
    "print(\"CRPS GAM: \", crps_gam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-27 21:25:27,265] A new study created in memory with name: no-name-24ab0b40-f220-45a3-aa79-ea5da9acd6b5\n",
      "[I 2024-03-27 21:25:37,337] Trial 0 finished with value: 14.062515995017966 and parameters: {'n_splines_lread': 80, 'lam_lread': 0.0013320229150659071, 'spline_order_lread': 4, 'n_splines_lwrite': 78, 'lam_lwrite': 0.9795848815655198, 'spline_order_lwrite': 2, 'n_splines_scall': 28, 'lam_scall': 36.57499481192727, 'spline_order_scall': 1, 'n_splines_sread': 18, 'lam_sread': 12.94669479886693, 'spline_order_sread': 5, 'n_splines_swrite': 10, 'lam_swrite': 1.1834599907542849, 'spline_order_swrite': 5, 'n_splines_fork': 65, 'lam_fork': 21.405821746591823, 'spline_order_fork': 2, 'n_splines_exec': 93, 'lam_exec': 19.384504329781148, 'spline_order_exec': 3, 'n_splines_rchar': 22, 'lam_rchar': 0.173797914304444, 'spline_order_rchar': 4, 'n_splines_wchar': 50, 'lam_wchar': 0.4018684945839337, 'spline_order_wchar': 4, 'n_splines_pgout': 56, 'lam_pgout': 7.986989097169455, 'spline_order_pgout': 4, 'n_splines_ppgout': 83, 'lam_ppgout': 1.3485992516968202, 'spline_order_ppgout': 5, 'n_splines_pgfree': 39, 'lam_pgfree': 0.003489442896348133, 'spline_order_pgfree': 2, 'n_splines_pgscan': 20, 'lam_pgscan': 93.77519109877728, 'spline_order_pgscan': 1, 'n_splines_atch': 66, 'lam_atch': 1.9298205889744646, 'spline_order_atch': 5, 'n_splines_pgin': 28, 'lam_pgin': 138.38907763091694, 'spline_order_pgin': 2, 'n_splines_ppgin': 78, 'lam_ppgin': 0.05967195151630889, 'spline_order_ppgin': 5, 'n_splines_pflt': 39, 'lam_pflt': 0.009774518801296312, 'spline_order_pflt': 2, 'n_splines_vflt': 18, 'lam_vflt': 84.45666867295606, 'spline_order_vflt': 1, 'n_splines_runqsz': 44, 'lam_runqsz': 462.98217777735624, 'spline_order_runqsz': 5, 'n_splines_freemem': 51, 'lam_freemem': 90.51844049299662, 'spline_order_freemem': 2, 'n_splines_freeswap': 64, 'lam_freeswap': 261.2104587991285, 'spline_order_freeswap': 3}. Best is trial 0 with value: 14.062515995017966.\n",
      "[I 2024-03-27 21:25:47,015] Trial 1 finished with value: 6.697463751377437 and parameters: {'n_splines_lread': 63, 'lam_lread': 0.0017206423318105984, 'spline_order_lread': 2, 'n_splines_lwrite': 17, 'lam_lwrite': 0.06803924879984836, 'spline_order_lwrite': 2, 'n_splines_scall': 80, 'lam_scall': 0.0017368217621749208, 'spline_order_scall': 3, 'n_splines_sread': 38, 'lam_sread': 6.59093241654684, 'spline_order_sread': 2, 'n_splines_swrite': 13, 'lam_swrite': 190.32290024927522, 'spline_order_swrite': 4, 'n_splines_fork': 89, 'lam_fork': 0.3199299241425316, 'spline_order_fork': 4, 'n_splines_exec': 56, 'lam_exec': 3.8638467047838243, 'spline_order_exec': 2, 'n_splines_rchar': 37, 'lam_rchar': 0.0014203608472085642, 'spline_order_rchar': 2, 'n_splines_wchar': 32, 'lam_wchar': 2.215486622230635, 'spline_order_wchar': 3, 'n_splines_pgout': 53, 'lam_pgout': 0.05711995514636383, 'spline_order_pgout': 1, 'n_splines_ppgout': 99, 'lam_ppgout': 0.1092061412630602, 'spline_order_ppgout': 3, 'n_splines_pgfree': 98, 'lam_pgfree': 0.4412075640903393, 'spline_order_pgfree': 2, 'n_splines_pgscan': 57, 'lam_pgscan': 2.943191897953818, 'spline_order_pgscan': 5, 'n_splines_atch': 16, 'lam_atch': 0.6126110789567031, 'spline_order_atch': 4, 'n_splines_pgin': 75, 'lam_pgin': 3.281949395145967, 'spline_order_pgin': 1, 'n_splines_ppgin': 41, 'lam_ppgin': 2.3941100346917805, 'spline_order_ppgin': 2, 'n_splines_pflt': 56, 'lam_pflt': 10.985037699399216, 'spline_order_pflt': 1, 'n_splines_vflt': 14, 'lam_vflt': 0.10638865596892418, 'spline_order_vflt': 1, 'n_splines_runqsz': 26, 'lam_runqsz': 206.52021904394073, 'spline_order_runqsz': 2, 'n_splines_freemem': 29, 'lam_freemem': 32.732337561472754, 'spline_order_freemem': 1, 'n_splines_freeswap': 77, 'lam_freeswap': 0.6587365495478882, 'spline_order_freeswap': 3}. Best is trial 1 with value: 6.697463751377437.\n",
      "[I 2024-03-27 21:25:58,498] Trial 2 finished with value: 27.862112631419784 and parameters: {'n_splines_lread': 23, 'lam_lread': 0.012711855064025102, 'spline_order_lread': 4, 'n_splines_lwrite': 14, 'lam_lwrite': 0.031022375250231608, 'spline_order_lwrite': 3, 'n_splines_scall': 30, 'lam_scall': 0.19429839334400542, 'spline_order_scall': 5, 'n_splines_sread': 94, 'lam_sread': 2.5147773925084724, 'spline_order_sread': 3, 'n_splines_swrite': 11, 'lam_swrite': 736.8779827891698, 'spline_order_swrite': 3, 'n_splines_fork': 82, 'lam_fork': 2.3406914117179567, 'spline_order_fork': 5, 'n_splines_exec': 63, 'lam_exec': 17.910139074496225, 'spline_order_exec': 1, 'n_splines_rchar': 48, 'lam_rchar': 14.566003100260664, 'spline_order_rchar': 1, 'n_splines_wchar': 50, 'lam_wchar': 0.009936029819741407, 'spline_order_wchar': 3, 'n_splines_pgout': 84, 'lam_pgout': 0.0034724852394159116, 'spline_order_pgout': 5, 'n_splines_ppgout': 61, 'lam_ppgout': 3.436262004752985, 'spline_order_ppgout': 1, 'n_splines_pgfree': 49, 'lam_pgfree': 0.05962420114333047, 'spline_order_pgfree': 1, 'n_splines_pgscan': 12, 'lam_pgscan': 0.5231545432214681, 'spline_order_pgscan': 4, 'n_splines_atch': 60, 'lam_atch': 0.20449448375433335, 'spline_order_atch': 1, 'n_splines_pgin': 86, 'lam_pgin': 3.9292601596402363, 'spline_order_pgin': 4, 'n_splines_ppgin': 87, 'lam_ppgin': 4.158894036069527, 'spline_order_ppgin': 4, 'n_splines_pflt': 66, 'lam_pflt': 0.001339649378783503, 'spline_order_pflt': 4, 'n_splines_vflt': 26, 'lam_vflt': 0.563748289827938, 'spline_order_vflt': 3, 'n_splines_runqsz': 54, 'lam_runqsz': 116.49700685818354, 'spline_order_runqsz': 1, 'n_splines_freemem': 11, 'lam_freemem': 123.76117697332955, 'spline_order_freemem': 4, 'n_splines_freeswap': 51, 'lam_freeswap': 0.31724182698218756, 'spline_order_freeswap': 1}. Best is trial 1 with value: 6.697463751377437.\n",
      "[I 2024-03-27 21:26:06,179] Trial 3 finished with value: 96.20336731894255 and parameters: {'n_splines_lread': 40, 'lam_lread': 0.0036978921820764866, 'spline_order_lread': 4, 'n_splines_lwrite': 17, 'lam_lwrite': 0.017206857030056973, 'spline_order_lwrite': 3, 'n_splines_scall': 36, 'lam_scall': 8.597267825984998, 'spline_order_scall': 5, 'n_splines_sread': 41, 'lam_sread': 0.0036361539438324575, 'spline_order_sread': 5, 'n_splines_swrite': 81, 'lam_swrite': 0.22935841283348843, 'spline_order_swrite': 5, 'n_splines_fork': 44, 'lam_fork': 0.03497980009634128, 'spline_order_fork': 5, 'n_splines_exec': 77, 'lam_exec': 1.1107221901111053, 'spline_order_exec': 4, 'n_splines_rchar': 29, 'lam_rchar': 236.7302148311276, 'spline_order_rchar': 5, 'n_splines_wchar': 38, 'lam_wchar': 156.06866280671636, 'spline_order_wchar': 2, 'n_splines_pgout': 12, 'lam_pgout': 0.001973199272375175, 'spline_order_pgout': 1, 'n_splines_ppgout': 16, 'lam_ppgout': 0.03506328005038613, 'spline_order_ppgout': 5, 'n_splines_pgfree': 51, 'lam_pgfree': 0.006043252451279828, 'spline_order_pgfree': 5, 'n_splines_pgscan': 46, 'lam_pgscan': 0.0014015089319297813, 'spline_order_pgscan': 5, 'n_splines_atch': 34, 'lam_atch': 17.973629472212366, 'spline_order_atch': 2, 'n_splines_pgin': 82, 'lam_pgin': 116.87272411500831, 'spline_order_pgin': 3, 'n_splines_ppgin': 60, 'lam_ppgin': 0.005433252501474346, 'spline_order_ppgin': 2, 'n_splines_pflt': 49, 'lam_pflt': 1.1674865277539999, 'spline_order_pflt': 5, 'n_splines_vflt': 37, 'lam_vflt': 0.23913947599054772, 'spline_order_vflt': 4, 'n_splines_runqsz': 47, 'lam_runqsz': 0.012858943850477863, 'spline_order_runqsz': 5, 'n_splines_freemem': 61, 'lam_freemem': 0.0022924571848600838, 'spline_order_freemem': 5, 'n_splines_freeswap': 30, 'lam_freeswap': 4.877646001265665, 'spline_order_freeswap': 2}. Best is trial 1 with value: 6.697463751377437.\n",
      "[I 2024-03-27 21:26:24,211] Trial 4 finished with value: 39.182913389513025 and parameters: {'n_splines_lread': 83, 'lam_lread': 5.826008531081302, 'spline_order_lread': 5, 'n_splines_lwrite': 36, 'lam_lwrite': 0.04454211035376219, 'spline_order_lwrite': 4, 'n_splines_scall': 95, 'lam_scall': 17.529836119817002, 'spline_order_scall': 2, 'n_splines_sread': 21, 'lam_sread': 0.5653864228210514, 'spline_order_sread': 5, 'n_splines_swrite': 71, 'lam_swrite': 0.014438510523027694, 'spline_order_swrite': 1, 'n_splines_fork': 96, 'lam_fork': 141.87115120646547, 'spline_order_fork': 5, 'n_splines_exec': 97, 'lam_exec': 3.923003489524248, 'spline_order_exec': 5, 'n_splines_rchar': 23, 'lam_rchar': 0.004827939870309317, 'spline_order_rchar': 2, 'n_splines_wchar': 79, 'lam_wchar': 0.08449912052410795, 'spline_order_wchar': 4, 'n_splines_pgout': 10, 'lam_pgout': 1.823014952774011, 'spline_order_pgout': 2, 'n_splines_ppgout': 91, 'lam_ppgout': 440.800654718713, 'spline_order_ppgout': 2, 'n_splines_pgfree': 46, 'lam_pgfree': 241.92439793462069, 'spline_order_pgfree': 1, 'n_splines_pgscan': 46, 'lam_pgscan': 0.0060402267701881686, 'spline_order_pgscan': 1, 'n_splines_atch': 81, 'lam_atch': 0.06721096517505715, 'spline_order_atch': 3, 'n_splines_pgin': 89, 'lam_pgin': 192.64261446519097, 'spline_order_pgin': 4, 'n_splines_ppgin': 68, 'lam_ppgin': 0.002186599361884947, 'spline_order_ppgin': 2, 'n_splines_pflt': 72, 'lam_pflt': 0.013619987755242426, 'spline_order_pflt': 5, 'n_splines_vflt': 56, 'lam_vflt': 34.97165955198424, 'spline_order_vflt': 1, 'n_splines_runqsz': 17, 'lam_runqsz': 0.780605880015192, 'spline_order_runqsz': 3, 'n_splines_freemem': 73, 'lam_freemem': 0.016926598162749745, 'spline_order_freemem': 4, 'n_splines_freeswap': 82, 'lam_freeswap': 0.0017800281690618295, 'spline_order_freeswap': 5}. Best is trial 1 with value: 6.697463751377437.\n",
      "[I 2024-03-27 21:26:40,963] Trial 5 finished with value: 107.85027057185764 and parameters: {'n_splines_lread': 98, 'lam_lread': 2.0135606031984965, 'spline_order_lread': 1, 'n_splines_lwrite': 41, 'lam_lwrite': 0.0013261108026040216, 'spline_order_lwrite': 5, 'n_splines_scall': 28, 'lam_scall': 0.007236275040756163, 'spline_order_scall': 4, 'n_splines_sread': 15, 'lam_sread': 0.03464538573876291, 'spline_order_sread': 3, 'n_splines_swrite': 100, 'lam_swrite': 0.007569968562303607, 'spline_order_swrite': 3, 'n_splines_fork': 64, 'lam_fork': 0.003833848656471983, 'spline_order_fork': 2, 'n_splines_exec': 75, 'lam_exec': 2.017606109176592, 'spline_order_exec': 5, 'n_splines_rchar': 62, 'lam_rchar': 0.012806498992124054, 'spline_order_rchar': 4, 'n_splines_wchar': 90, 'lam_wchar': 1.261059757691616, 'spline_order_wchar': 4, 'n_splines_pgout': 55, 'lam_pgout': 287.06183687157227, 'spline_order_pgout': 3, 'n_splines_ppgout': 58, 'lam_ppgout': 0.11372573527936965, 'spline_order_ppgout': 1, 'n_splines_pgfree': 70, 'lam_pgfree': 411.16866702117693, 'spline_order_pgfree': 4, 'n_splines_pgscan': 86, 'lam_pgscan': 0.02029787703515406, 'spline_order_pgscan': 5, 'n_splines_atch': 10, 'lam_atch': 0.02263999673998517, 'spline_order_atch': 1, 'n_splines_pgin': 92, 'lam_pgin': 289.3235731218307, 'spline_order_pgin': 3, 'n_splines_ppgin': 17, 'lam_ppgin': 0.005026626742712936, 'spline_order_ppgin': 4, 'n_splines_pflt': 76, 'lam_pflt': 0.003332499873459897, 'spline_order_pflt': 2, 'n_splines_vflt': 80, 'lam_vflt': 0.00614935562643563, 'spline_order_vflt': 3, 'n_splines_runqsz': 78, 'lam_runqsz': 0.043167515731220164, 'spline_order_runqsz': 3, 'n_splines_freemem': 53, 'lam_freemem': 2.18637249521577, 'spline_order_freemem': 3, 'n_splines_freeswap': 73, 'lam_freeswap': 20.39027494591666, 'spline_order_freeswap': 4}. Best is trial 1 with value: 6.697463751377437.\n",
      "[I 2024-03-27 21:26:49,662] Trial 6 finished with value: 21.33308602372693 and parameters: {'n_splines_lread': 13, 'lam_lread': 11.830752836915712, 'spline_order_lread': 3, 'n_splines_lwrite': 19, 'lam_lwrite': 4.843701845834949, 'spline_order_lwrite': 5, 'n_splines_scall': 76, 'lam_scall': 0.08499502310194638, 'spline_order_scall': 1, 'n_splines_sread': 13, 'lam_sread': 2.2633505533059743, 'spline_order_sread': 1, 'n_splines_swrite': 34, 'lam_swrite': 0.025958030514019792, 'spline_order_swrite': 1, 'n_splines_fork': 23, 'lam_fork': 0.0015974051446319088, 'spline_order_fork': 5, 'n_splines_exec': 42, 'lam_exec': 89.69891118554955, 'spline_order_exec': 3, 'n_splines_rchar': 33, 'lam_rchar': 0.026318750007067442, 'spline_order_rchar': 4, 'n_splines_wchar': 71, 'lam_wchar': 17.2975785400496, 'spline_order_wchar': 5, 'n_splines_pgout': 57, 'lam_pgout': 0.45354242720587773, 'spline_order_pgout': 3, 'n_splines_ppgout': 69, 'lam_ppgout': 53.86705676699372, 'spline_order_ppgout': 5, 'n_splines_pgfree': 38, 'lam_pgfree': 0.0065111639898306995, 'spline_order_pgfree': 4, 'n_splines_pgscan': 57, 'lam_pgscan': 51.10124540119885, 'spline_order_pgscan': 3, 'n_splines_atch': 86, 'lam_atch': 1.6754217633947612, 'spline_order_atch': 2, 'n_splines_pgin': 15, 'lam_pgin': 0.0035620147166877344, 'spline_order_pgin': 1, 'n_splines_ppgin': 49, 'lam_ppgin': 73.38467081723303, 'spline_order_ppgin': 5, 'n_splines_pflt': 62, 'lam_pflt': 0.05583757883982752, 'spline_order_pflt': 4, 'n_splines_vflt': 68, 'lam_vflt': 1.05062671160191, 'spline_order_vflt': 5, 'n_splines_runqsz': 83, 'lam_runqsz': 372.4913040393185, 'spline_order_runqsz': 4, 'n_splines_freemem': 99, 'lam_freemem': 19.495201801230703, 'spline_order_freemem': 1, 'n_splines_freeswap': 36, 'lam_freeswap': 111.78716570209846, 'spline_order_freeswap': 3}. Best is trial 1 with value: 6.697463751377437.\n",
      "[I 2024-03-27 21:27:00,163] Trial 7 finished with value: 32.12692599979004 and parameters: {'n_splines_lread': 25, 'lam_lread': 0.004865894329990562, 'spline_order_lread': 4, 'n_splines_lwrite': 69, 'lam_lwrite': 1.0244151412466491, 'spline_order_lwrite': 2, 'n_splines_scall': 67, 'lam_scall': 0.16220021740476448, 'spline_order_scall': 3, 'n_splines_sread': 79, 'lam_sread': 109.47750391026496, 'spline_order_sread': 1, 'n_splines_swrite': 56, 'lam_swrite': 7.463318513136203, 'spline_order_swrite': 1, 'n_splines_fork': 10, 'lam_fork': 634.2705096443026, 'spline_order_fork': 5, 'n_splines_exec': 34, 'lam_exec': 0.0472647801826096, 'spline_order_exec': 1, 'n_splines_rchar': 53, 'lam_rchar': 392.47488653417855, 'spline_order_rchar': 5, 'n_splines_wchar': 32, 'lam_wchar': 0.23396439906453964, 'spline_order_wchar': 5, 'n_splines_pgout': 35, 'lam_pgout': 423.7372514162469, 'spline_order_pgout': 4, 'n_splines_ppgout': 90, 'lam_ppgout': 168.27897245490448, 'spline_order_ppgout': 1, 'n_splines_pgfree': 81, 'lam_pgfree': 285.3208269209737, 'spline_order_pgfree': 5, 'n_splines_pgscan': 74, 'lam_pgscan': 0.0698677103655391, 'spline_order_pgscan': 2, 'n_splines_atch': 31, 'lam_atch': 0.0034583570930551855, 'spline_order_atch': 1, 'n_splines_pgin': 66, 'lam_pgin': 391.2929771974317, 'spline_order_pgin': 4, 'n_splines_ppgin': 61, 'lam_ppgin': 0.03824148710844592, 'spline_order_ppgin': 3, 'n_splines_pflt': 83, 'lam_pflt': 0.05894613894128168, 'spline_order_pflt': 1, 'n_splines_vflt': 11, 'lam_vflt': 28.835493723620946, 'spline_order_vflt': 1, 'n_splines_runqsz': 49, 'lam_runqsz': 98.28982636858667, 'spline_order_runqsz': 1, 'n_splines_freemem': 56, 'lam_freemem': 0.7538626049557706, 'spline_order_freemem': 1, 'n_splines_freeswap': 11, 'lam_freeswap': 0.09796343421969912, 'spline_order_freeswap': 1}. Best is trial 1 with value: 6.697463751377437.\n",
      "[I 2024-03-27 21:27:16,369] Trial 8 finished with value: 161.1952942779892 and parameters: {'n_splines_lread': 64, 'lam_lread': 206.78618507160795, 'spline_order_lread': 3, 'n_splines_lwrite': 13, 'lam_lwrite': 0.0030553039585351966, 'spline_order_lwrite': 1, 'n_splines_scall': 83, 'lam_scall': 63.703606095120065, 'spline_order_scall': 1, 'n_splines_sread': 88, 'lam_sread': 293.63062269518167, 'spline_order_sread': 3, 'n_splines_swrite': 36, 'lam_swrite': 114.35872681022907, 'spline_order_swrite': 4, 'n_splines_fork': 22, 'lam_fork': 0.0235794506248603, 'spline_order_fork': 5, 'n_splines_exec': 86, 'lam_exec': 0.02093514437605906, 'spline_order_exec': 5, 'n_splines_rchar': 58, 'lam_rchar': 4.598018283870405, 'spline_order_rchar': 3, 'n_splines_wchar': 68, 'lam_wchar': 0.001073836318352181, 'spline_order_wchar': 1, 'n_splines_pgout': 72, 'lam_pgout': 0.010704296184300664, 'spline_order_pgout': 4, 'n_splines_ppgout': 35, 'lam_ppgout': 0.0021410929658616444, 'spline_order_ppgout': 3, 'n_splines_pgfree': 36, 'lam_pgfree': 0.08647624304370181, 'spline_order_pgfree': 5, 'n_splines_pgscan': 27, 'lam_pgscan': 0.12144482988528843, 'spline_order_pgscan': 1, 'n_splines_atch': 46, 'lam_atch': 0.003606499451898601, 'spline_order_atch': 3, 'n_splines_pgin': 77, 'lam_pgin': 0.0010758285246015304, 'spline_order_pgin': 1, 'n_splines_ppgin': 15, 'lam_ppgin': 202.28048218022556, 'spline_order_ppgin': 5, 'n_splines_pflt': 57, 'lam_pflt': 0.015506171292591324, 'spline_order_pflt': 5, 'n_splines_vflt': 69, 'lam_vflt': 56.331391971515124, 'spline_order_vflt': 3, 'n_splines_runqsz': 81, 'lam_runqsz': 0.1020423355133954, 'spline_order_runqsz': 1, 'n_splines_freemem': 92, 'lam_freemem': 0.0010600918143363152, 'spline_order_freemem': 1, 'n_splines_freeswap': 22, 'lam_freeswap': 449.67579967063983, 'spline_order_freeswap': 2}. Best is trial 1 with value: 6.697463751377437.\n",
      "[I 2024-03-27 21:27:27,513] Trial 9 finished with value: 107.73331210610009 and parameters: {'n_splines_lread': 29, 'lam_lread': 0.03419600052628855, 'spline_order_lread': 1, 'n_splines_lwrite': 46, 'lam_lwrite': 24.331329615364968, 'spline_order_lwrite': 4, 'n_splines_scall': 32, 'lam_scall': 0.023154397446680516, 'spline_order_scall': 3, 'n_splines_sread': 88, 'lam_sread': 6.373566459814837, 'spline_order_sread': 4, 'n_splines_swrite': 49, 'lam_swrite': 0.2519257391509583, 'spline_order_swrite': 4, 'n_splines_fork': 43, 'lam_fork': 2.0681792778585497, 'spline_order_fork': 1, 'n_splines_exec': 64, 'lam_exec': 0.048476226008644714, 'spline_order_exec': 4, 'n_splines_rchar': 37, 'lam_rchar': 35.0929683758257, 'spline_order_rchar': 2, 'n_splines_wchar': 10, 'lam_wchar': 0.006897340812120462, 'spline_order_wchar': 2, 'n_splines_pgout': 27, 'lam_pgout': 1.9646818226861666, 'spline_order_pgout': 2, 'n_splines_ppgout': 80, 'lam_ppgout': 0.8092438698794575, 'spline_order_ppgout': 4, 'n_splines_pgfree': 25, 'lam_pgfree': 353.26094200813895, 'spline_order_pgfree': 4, 'n_splines_pgscan': 24, 'lam_pgscan': 21.96856224054189, 'spline_order_pgscan': 1, 'n_splines_atch': 85, 'lam_atch': 0.015130405939666353, 'spline_order_atch': 5, 'n_splines_pgin': 59, 'lam_pgin': 0.02173609874720741, 'spline_order_pgin': 2, 'n_splines_ppgin': 67, 'lam_ppgin': 6.304588927268016, 'spline_order_ppgin': 2, 'n_splines_pflt': 84, 'lam_pflt': 0.32684383833531566, 'spline_order_pflt': 5, 'n_splines_vflt': 34, 'lam_vflt': 795.340006346361, 'spline_order_vflt': 5, 'n_splines_runqsz': 50, 'lam_runqsz': 0.10287433931592678, 'spline_order_runqsz': 1, 'n_splines_freemem': 100, 'lam_freemem': 12.181398447487041, 'spline_order_freemem': 1, 'n_splines_freeswap': 14, 'lam_freeswap': 0.0011990499413529203, 'spline_order_freeswap': 4}. Best is trial 1 with value: 6.697463751377437.\n",
      "[I 2024-03-27 21:27:37,259] Trial 10 finished with value: 20.535958409682877 and parameters: {'n_splines_lread': 56, 'lam_lread': 0.14659422910648526, 'spline_order_lread': 2, 'n_splines_lwrite': 59, 'lam_lwrite': 280.6235147254821, 'spline_order_lwrite': 1, 'n_splines_scall': 54, 'lam_scall': 0.0013600698776793823, 'spline_order_scall': 4, 'n_splines_sread': 63, 'lam_sread': 0.16524603628409507, 'spline_order_sread': 2, 'n_splines_swrite': 27, 'lam_swrite': 84.00395648788371, 'spline_order_swrite': 2, 'n_splines_fork': 96, 'lam_fork': 0.08630940731756827, 'spline_order_fork': 3, 'n_splines_exec': 16, 'lam_exec': 456.0658509707542, 'spline_order_exec': 2, 'n_splines_rchar': 95, 'lam_rchar': 0.0013942841620349581, 'spline_order_rchar': 1, 'n_splines_wchar': 14, 'lam_wchar': 952.7975064461258, 'spline_order_wchar': 3, 'n_splines_pgout': 41, 'lam_pgout': 0.062453368587581416, 'spline_order_pgout': 1, 'n_splines_ppgout': 39, 'lam_ppgout': 0.0016419493917420366, 'spline_order_ppgout': 3, 'n_splines_pgfree': 99, 'lam_pgfree': 6.32861725408131, 'spline_order_pgfree': 2, 'n_splines_pgscan': 68, 'lam_pgscan': 4.529428184658494, 'spline_order_pgscan': 4, 'n_splines_atch': 16, 'lam_atch': 414.6684782735533, 'spline_order_atch': 4, 'n_splines_pgin': 41, 'lam_pgin': 1.1671481514769555, 'spline_order_pgin': 5, 'n_splines_ppgin': 36, 'lam_ppgin': 0.2917954335157036, 'spline_order_ppgin': 1, 'n_splines_pflt': 16, 'lam_pflt': 200.19078534916073, 'spline_order_pflt': 1, 'n_splines_vflt': 43, 'lam_vflt': 0.00117623273639507, 'spline_order_vflt': 2, 'n_splines_runqsz': 11, 'lam_runqsz': 5.746654769229659, 'spline_order_runqsz': 2, 'n_splines_freemem': 22, 'lam_freemem': 0.13445892978046428, 'spline_order_freemem': 2, 'n_splines_freeswap': 94, 'lam_freeswap': 1.3800724278511713, 'spline_order_freeswap': 5}. Best is trial 1 with value: 6.697463751377437.\n",
      "[I 2024-03-27 21:27:46,613] Trial 11 finished with value: 5.3706185084476505 and parameters: {'n_splines_lread': 69, 'lam_lread': 0.0012506520419624707, 'spline_order_lread': 2, 'n_splines_lwrite': 100, 'lam_lwrite': 0.41037063204998536, 'spline_order_lwrite': 2, 'n_splines_scall': 13, 'lam_scall': 624.0636115063176, 'spline_order_scall': 2, 'n_splines_sread': 38, 'lam_sread': 32.47189702300746, 'spline_order_sread': 2, 'n_splines_swrite': 13, 'lam_swrite': 4.402984611688584, 'spline_order_swrite': 5, 'n_splines_fork': 70, 'lam_fork': 20.890089908118586, 'spline_order_fork': 3, 'n_splines_exec': 44, 'lam_exec': 26.27268838758856, 'spline_order_exec': 2, 'n_splines_rchar': 10, 'lam_rchar': 0.16377928831976676, 'spline_order_rchar': 3, 'n_splines_wchar': 50, 'lam_wchar': 3.6532811572851336, 'spline_order_wchar': 4, 'n_splines_pgout': 62, 'lam_pgout': 17.690841298245918, 'spline_order_pgout': 5, 'n_splines_ppgout': 100, 'lam_ppgout': 5.4293125710698735, 'spline_order_ppgout': 4, 'n_splines_pgfree': 13, 'lam_pgfree': 2.725101542624567, 'spline_order_pgfree': 2, 'n_splines_pgscan': 97, 'lam_pgscan': 847.652869668719, 'spline_order_pgscan': 3, 'n_splines_atch': 64, 'lam_atch': 2.8754948651473153, 'spline_order_atch': 5, 'n_splines_pgin': 36, 'lam_pgin': 12.216114777964613, 'spline_order_pgin': 2, 'n_splines_ppgin': 98, 'lam_ppgin': 0.35118842739468203, 'spline_order_ppgin': 3, 'n_splines_pflt': 33, 'lam_pflt': 17.929131668727397, 'spline_order_pflt': 2, 'n_splines_vflt': 11, 'lam_vflt': 0.04299352246267102, 'spline_order_vflt': 1, 'n_splines_runqsz': 29, 'lam_runqsz': 932.8333097989146, 'spline_order_runqsz': 5, 'n_splines_freemem': 36, 'lam_freemem': 714.2342320970142, 'spline_order_freemem': 2, 'n_splines_freeswap': 65, 'lam_freeswap': 0.029703637682782086, 'spline_order_freeswap': 3}. Best is trial 11 with value: 5.3706185084476505.\n",
      "[I 2024-03-27 21:27:55,916] Trial 12 finished with value: 21.76005750394933 and parameters: {'n_splines_lread': 70, 'lam_lread': 0.2023117937960772, 'spline_order_lread': 2, 'n_splines_lwrite': 99, 'lam_lwrite': 0.2489434332843138, 'spline_order_lwrite': 2, 'n_splines_scall': 13, 'lam_scall': 999.5113478416316, 'spline_order_scall': 2, 'n_splines_sread': 40, 'lam_sread': 42.24707338316811, 'spline_order_sread': 2, 'n_splines_swrite': 21, 'lam_swrite': 10.224875806236197, 'spline_order_swrite': 4, 'n_splines_fork': 78, 'lam_fork': 24.39553874568736, 'spline_order_fork': 3, 'n_splines_exec': 45, 'lam_exec': 0.001593743358749575, 'spline_order_exec': 2, 'n_splines_rchar': 10, 'lam_rchar': 0.25817897755978736, 'spline_order_rchar': 2, 'n_splines_wchar': 31, 'lam_wchar': 6.59110010182917, 'spline_order_wchar': 3, 'n_splines_pgout': 93, 'lam_pgout': 39.944984272700175, 'spline_order_pgout': 5, 'n_splines_ppgout': 100, 'lam_ppgout': 27.83610069437735, 'spline_order_ppgout': 4, 'n_splines_pgfree': 17, 'lam_pgfree': 3.06798376918281, 'spline_order_pgfree': 2, 'n_splines_pgscan': 97, 'lam_pgscan': 876.0029204752683, 'spline_order_pgscan': 3, 'n_splines_atch': 68, 'lam_atch': 31.93925938575942, 'spline_order_atch': 4, 'n_splines_pgin': 41, 'lam_pgin': 9.332893831117705, 'spline_order_pgin': 2, 'n_splines_ppgin': 39, 'lam_ppgin': 1.038010738134838, 'spline_order_ppgin': 3, 'n_splines_pflt': 29, 'lam_pflt': 33.405421207794525, 'spline_order_pflt': 2, 'n_splines_vflt': 11, 'lam_vflt': 0.06206021023776758, 'spline_order_vflt': 2, 'n_splines_runqsz': 30, 'lam_runqsz': 16.927727952929278, 'spline_order_runqsz': 4, 'n_splines_freemem': 34, 'lam_freemem': 713.2805318216876, 'spline_order_freemem': 2, 'n_splines_freeswap': 51, 'lam_freeswap': 0.027994829673666174, 'spline_order_freeswap': 4}. Best is trial 11 with value: 5.3706185084476505.\n",
      "[I 2024-03-27 21:28:12,528] Trial 13 finished with value: 2.944528873830982 and parameters: {'n_splines_lread': 50, 'lam_lread': 0.0010907797834726664, 'spline_order_lread': 2, 'n_splines_lwrite': 99, 'lam_lwrite': 0.1851233235253282, 'spline_order_lwrite': 2, 'n_splines_scall': 57, 'lam_scall': 1.670415613345121, 'spline_order_scall': 2, 'n_splines_sread': 38, 'lam_sread': 914.5958993019948, 'spline_order_sread': 2, 'n_splines_swrite': 46, 'lam_swrite': 900.3413605251378, 'spline_order_swrite': 5, 'n_splines_fork': 82, 'lam_fork': 0.49412917065590134, 'spline_order_fork': 4, 'n_splines_exec': 27, 'lam_exec': 324.6890176371332, 'spline_order_exec': 2, 'n_splines_rchar': 10, 'lam_rchar': 0.07502790265184511, 'spline_order_rchar': 3, 'n_splines_wchar': 58, 'lam_wchar': 7.766911477233571, 'spline_order_wchar': 2, 'n_splines_pgout': 73, 'lam_pgout': 0.23581053961062648, 'spline_order_pgout': 2, 'n_splines_ppgout': 99, 'lam_ppgout': 0.05722106131473122, 'spline_order_ppgout': 4, 'n_splines_pgfree': 65, 'lam_pgfree': 0.44197027605343275, 'spline_order_pgfree': 3, 'n_splines_pgscan': 98, 'lam_pgscan': 414.4084453478255, 'spline_order_pgscan': 4, 'n_splines_atch': 46, 'lam_atch': 13.112136429725576, 'spline_order_atch': 4, 'n_splines_pgin': 44, 'lam_pgin': 0.13224370597536309, 'spline_order_pgin': 1, 'n_splines_ppgin': 91, 'lam_ppgin': 24.760776436659622, 'spline_order_ppgin': 1, 'n_splines_pflt': 38, 'lam_pflt': 8.086740636974717, 'spline_order_pflt': 1, 'n_splines_vflt': 96, 'lam_vflt': 0.027456591493487604, 'spline_order_vflt': 2, 'n_splines_runqsz': 29, 'lam_runqsz': 970.2994789571723, 'spline_order_runqsz': 2, 'n_splines_freemem': 35, 'lam_freemem': 336.2959970340618, 'spline_order_freemem': 3, 'n_splines_freeswap': 91, 'lam_freeswap': 0.020662371318248038, 'spline_order_freeswap': 2}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:28:28,542] Trial 14 finished with value: 199.57624069024914 and parameters: {'n_splines_lread': 45, 'lam_lread': 0.04846238500984842, 'spline_order_lread': 2, 'n_splines_lwrite': 97, 'lam_lwrite': 13.832188112242317, 'spline_order_lwrite': 1, 'n_splines_scall': 50, 'lam_scall': 865.6151485949238, 'spline_order_scall': 2, 'n_splines_sread': 56, 'lam_sread': 887.6243974494739, 'spline_order_sread': 2, 'n_splines_swrite': 47, 'lam_swrite': 13.576860519632714, 'spline_order_swrite': 5, 'n_splines_fork': 72, 'lam_fork': 13.235982051526026, 'spline_order_fork': 4, 'n_splines_exec': 20, 'lam_exec': 901.9083541156588, 'spline_order_exec': 2, 'n_splines_rchar': 10, 'lam_rchar': 0.9221638681691356, 'spline_order_rchar': 3, 'n_splines_wchar': 61, 'lam_wchar': 31.974019994198894, 'spline_order_wchar': 1, 'n_splines_pgout': 71, 'lam_pgout': 29.1394889459338, 'spline_order_pgout': 2, 'n_splines_ppgout': 75, 'lam_ppgout': 0.018244432250162455, 'spline_order_ppgout': 4, 'n_splines_pgfree': 69, 'lam_pgfree': 19.92913121736724, 'spline_order_pgfree': 3, 'n_splines_pgscan': 98, 'lam_pgscan': 960.6378053509288, 'spline_order_pgscan': 4, 'n_splines_atch': 48, 'lam_atch': 16.54014264142651, 'spline_order_atch': 5, 'n_splines_pgin': 44, 'lam_pgin': 0.13871840955442047, 'spline_order_pgin': 2, 'n_splines_ppgin': 98, 'lam_ppgin': 21.052409028945995, 'spline_order_ppgin': 1, 'n_splines_pflt': 12, 'lam_pflt': 696.0505822550926, 'spline_order_pflt': 3, 'n_splines_vflt': 96, 'lam_vflt': 0.024819906540281007, 'spline_order_vflt': 2, 'n_splines_runqsz': 33, 'lam_runqsz': 0.0011610859424628576, 'spline_order_runqsz': 2, 'n_splines_freemem': 40, 'lam_freemem': 926.8664494734298, 'spline_order_freemem': 3, 'n_splines_freeswap': 98, 'lam_freeswap': 0.015903901176775266, 'spline_order_freeswap': 2}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:28:41,432] Trial 15 finished with value: 52.3205019730077 and parameters: {'n_splines_lread': 45, 'lam_lread': 606.1053463984736, 'spline_order_lread': 1, 'n_splines_lwrite': 86, 'lam_lwrite': 0.8687816083982718, 'spline_order_lwrite': 3, 'n_splines_scall': 10, 'lam_scall': 2.547355513613405, 'spline_order_scall': 2, 'n_splines_sread': 31, 'lam_sread': 799.4473097135183, 'spline_order_sread': 1, 'n_splines_swrite': 65, 'lam_swrite': 0.00181998307759074, 'spline_order_swrite': 5, 'n_splines_fork': 55, 'lam_fork': 0.46382284762136017, 'spline_order_fork': 4, 'n_splines_exec': 29, 'lam_exec': 118.05847575636487, 'spline_order_exec': 1, 'n_splines_rchar': 10, 'lam_rchar': 0.07701432976848835, 'spline_order_rchar': 3, 'n_splines_wchar': 46, 'lam_wchar': 249.11442729217114, 'spline_order_wchar': 2, 'n_splines_pgout': 73, 'lam_pgout': 0.2793490297228477, 'spline_order_pgout': 3, 'n_splines_ppgout': 47, 'lam_ppgout': 9.024339372508747, 'spline_order_ppgout': 4, 'n_splines_pgfree': 61, 'lam_pgfree': 0.41410802579529854, 'spline_order_pgfree': 3, 'n_splines_pgscan': 83, 'lam_pgscan': 226.45649959219838, 'spline_order_pgscan': 2, 'n_splines_atch': 98, 'lam_atch': 230.84282652869456, 'spline_order_atch': 4, 'n_splines_pgin': 25, 'lam_pgin': 0.14346847401666316, 'spline_order_pgin': 1, 'n_splines_ppgin': 100, 'lam_ppgin': 0.18976384531901663, 'spline_order_ppgin': 1, 'n_splines_pflt': 30, 'lam_pflt': 6.041161081677112, 'spline_order_pflt': 2, 'n_splines_vflt': 99, 'lam_vflt': 3.733344540416522, 'spline_order_vflt': 2, 'n_splines_runqsz': 67, 'lam_runqsz': 13.04043076427794, 'spline_order_runqsz': 4, 'n_splines_freemem': 16, 'lam_freemem': 466.54209602971537, 'spline_order_freemem': 4, 'n_splines_freeswap': 88, 'lam_freeswap': 0.015166942056526947, 'spline_order_freeswap': 2}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:28:58,487] Trial 16 finished with value: 85.63889591986843 and parameters: {'n_splines_lread': 78, 'lam_lread': 0.0010341208175451838, 'spline_order_lread': 3, 'n_splines_lwrite': 86, 'lam_lwrite': 0.18049151733019964, 'spline_order_lwrite': 2, 'n_splines_scall': 47, 'lam_scall': 175.22797379240703, 'spline_order_scall': 2, 'n_splines_sread': 65, 'lam_sread': 49.216866535455736, 'spline_order_sread': 2, 'n_splines_swrite': 35, 'lam_swrite': 1.6493727042980626, 'spline_order_swrite': 5, 'n_splines_fork': 55, 'lam_fork': 4.462549854227272, 'spline_order_fork': 2, 'n_splines_exec': 27, 'lam_exec': 60.18555679166099, 'spline_order_exec': 3, 'n_splines_rchar': 84, 'lam_rchar': 0.7462249762700256, 'spline_order_rchar': 4, 'n_splines_wchar': 90, 'lam_wchar': 32.18273668406982, 'spline_order_wchar': 2, 'n_splines_pgout': 96, 'lam_pgout': 17.393418883323825, 'spline_order_pgout': 5, 'n_splines_ppgout': 89, 'lam_ppgout': 0.6508439462652776, 'spline_order_ppgout': 2, 'n_splines_pgfree': 11, 'lam_pgfree': 16.347125191525148, 'spline_order_pgfree': 3, 'n_splines_pgscan': 88, 'lam_pgscan': 13.470572391642136, 'spline_order_pgscan': 3, 'n_splines_atch': 38, 'lam_atch': 4.65797384859503, 'spline_order_atch': 5, 'n_splines_pgin': 52, 'lam_pgin': 22.48733754651341, 'spline_order_pgin': 2, 'n_splines_ppgin': 85, 'lam_ppgin': 342.8761982550887, 'spline_order_ppgin': 4, 'n_splines_pflt': 40, 'lam_pflt': 62.600340561907956, 'spline_order_pflt': 3, 'n_splines_vflt': 82, 'lam_vflt': 0.008760228625306388, 'spline_order_vflt': 2, 'n_splines_runqsz': 95, 'lam_runqsz': 1.8446368247018263, 'spline_order_runqsz': 3, 'n_splines_freemem': 43, 'lam_freemem': 1.2193434989587462, 'spline_order_freemem': 3, 'n_splines_freeswap': 62, 'lam_freeswap': 0.006586732227256075, 'spline_order_freeswap': 1}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:29:18,097] Trial 17 finished with value: 22.351304122482645 and parameters: {'n_splines_lread': 96, 'lam_lread': 0.30117712512969563, 'spline_order_lread': 2, 'n_splines_lwrite': 88, 'lam_lwrite': 162.91932330212936, 'spline_order_lwrite': 4, 'n_splines_scall': 64, 'lam_scall': 0.848626444667028, 'spline_order_scall': 4, 'n_splines_sread': 48, 'lam_sread': 219.02998246497185, 'spline_order_sread': 4, 'n_splines_swrite': 89, 'lam_swrite': 965.4828001159678, 'spline_order_swrite': 2, 'n_splines_fork': 84, 'lam_fork': 228.9066075088073, 'spline_order_fork': 3, 'n_splines_exec': 46, 'lam_exec': 285.26056564332856, 'spline_order_exec': 2, 'n_splines_rchar': 78, 'lam_rchar': 0.05689203707580087, 'spline_order_rchar': 3, 'n_splines_wchar': 60, 'lam_wchar': 3.931529779009035, 'spline_order_wchar': 4, 'n_splines_pgout': 85, 'lam_pgout': 91.33259781576105, 'spline_order_pgout': 2, 'n_splines_ppgout': 69, 'lam_ppgout': 0.00884876238468133, 'spline_order_ppgout': 4, 'n_splines_pgfree': 84, 'lam_pgfree': 0.07752267104039264, 'spline_order_pgfree': 1, 'n_splines_pgscan': 71, 'lam_pgscan': 242.11863078187253, 'spline_order_pgscan': 4, 'n_splines_atch': 73, 'lam_atch': 88.46030710611466, 'spline_order_atch': 4, 'n_splines_pgin': 11, 'lam_pgin': 0.17559183673035175, 'spline_order_pgin': 1, 'n_splines_ppgin': 87, 'lam_ppgin': 27.51051555610554, 'spline_order_ppgin': 3, 'n_splines_pflt': 24, 'lam_pflt': 1.6110892393607625, 'spline_order_pflt': 1, 'n_splines_vflt': 51, 'lam_vflt': 0.0011158578231871762, 'spline_order_vflt': 1, 'n_splines_runqsz': 34, 'lam_runqsz': 33.65377651429474, 'spline_order_runqsz': 5, 'n_splines_freemem': 26, 'lam_freemem': 142.01662281096924, 'spline_order_freemem': 2, 'n_splines_freeswap': 43, 'lam_freeswap': 0.10889277138974283, 'spline_order_freeswap': 2}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:29:45,902] Trial 18 finished with value: 4.371965179464004 and parameters: {'n_splines_lread': 54, 'lam_lread': 0.015276624161022363, 'spline_order_lread': 3, 'n_splines_lwrite': 69, 'lam_lwrite': 6.351130823562574, 'spline_order_lwrite': 1, 'n_splines_scall': 42, 'lam_scall': 2.7394708979810694, 'spline_order_scall': 1, 'n_splines_sread': 29, 'lam_sread': 29.502773475953944, 'spline_order_sread': 3, 'n_splines_swrite': 60, 'lam_swrite': 0.13420516493898837, 'spline_order_swrite': 4, 'n_splines_fork': 69, 'lam_fork': 100.2427634190731, 'spline_order_fork': 4, 'n_splines_exec': 40, 'lam_exec': 18.555336149800223, 'spline_order_exec': 3, 'n_splines_rchar': 17, 'lam_rchar': 6.047276186048929, 'spline_order_rchar': 3, 'n_splines_wchar': 99, 'lam_wchar': 0.06533213924578887, 'spline_order_wchar': 1, 'n_splines_pgout': 64, 'lam_pgout': 4.119581022756778, 'spline_order_pgout': 4, 'n_splines_ppgout': 25, 'lam_ppgout': 0.15865494888103854, 'spline_order_ppgout': 3, 'n_splines_pgfree': 27, 'lam_pgfree': 1.4164392903244711, 'spline_order_pgfree': 2, 'n_splines_pgscan': 94, 'lam_pgscan': 0.9239795467528806, 'spline_order_pgscan': 2, 'n_splines_atch': 54, 'lam_atch': 6.239071549065977, 'spline_order_atch': 3, 'n_splines_pgin': 26, 'lam_pgin': 0.02618548891136368, 'spline_order_pgin': 3, 'n_splines_ppgin': 77, 'lam_ppgin': 968.9216565565695, 'spline_order_ppgin': 1, 'n_splines_pflt': 46, 'lam_pflt': 5.342595511156404, 'spline_order_pflt': 2, 'n_splines_vflt': 84, 'lam_vflt': 0.009165699086108573, 'spline_order_vflt': 2, 'n_splines_runqsz': 21, 'lam_runqsz': 680.0618570797874, 'spline_order_runqsz': 2, 'n_splines_freemem': 68, 'lam_freemem': 4.345965731948593, 'spline_order_freemem': 5, 'n_splines_freeswap': 68, 'lam_freeswap': 0.0784628958487229, 'spline_order_freeswap': 3}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:30:04,774] Trial 19 finished with value: 5.991731595162861 and parameters: {'n_splines_lread': 53, 'lam_lread': 0.018393146327443488, 'spline_order_lread': 3, 'n_splines_lwrite': 66, 'lam_lwrite': 4.614342085663115, 'spline_order_lwrite': 1, 'n_splines_scall': 42, 'lam_scall': 1.825658220124644, 'spline_order_scall': 1, 'n_splines_sread': 28, 'lam_sread': 201.5059112265698, 'spline_order_sread': 4, 'n_splines_swrite': 61, 'lam_swrite': 0.11275462674623052, 'spline_order_swrite': 4, 'n_splines_fork': 44, 'lam_fork': 0.2288181988600583, 'spline_order_fork': 4, 'n_splines_exec': 35, 'lam_exec': 0.26738321450178476, 'spline_order_exec': 4, 'n_splines_rchar': 21, 'lam_rchar': 4.58781152032795, 'spline_order_rchar': 2, 'n_splines_wchar': 100, 'lam_wchar': 0.03745145482793402, 'spline_order_wchar': 1, 'n_splines_pgout': 81, 'lam_pgout': 0.10393761739405692, 'spline_order_pgout': 4, 'n_splines_ppgout': 16, 'lam_ppgout': 0.2655695263118413, 'spline_order_ppgout': 2, 'n_splines_pgfree': 29, 'lam_pgfree': 51.110416253585164, 'spline_order_pgfree': 3, 'n_splines_pgscan': 82, 'lam_pgscan': 0.4758994593493085, 'spline_order_pgscan': 2, 'n_splines_atch': 49, 'lam_atch': 958.3533425922557, 'spline_order_atch': 3, 'n_splines_pgin': 25, 'lam_pgin': 0.02045423313278899, 'spline_order_pgin': 3, 'n_splines_ppgin': 75, 'lam_ppgin': 875.7135392342446, 'spline_order_ppgin': 1, 'n_splines_pflt': 44, 'lam_pflt': 3.540601213229225, 'spline_order_pflt': 3, 'n_splines_vflt': 88, 'lam_vflt': 0.006077027476251626, 'spline_order_vflt': 4, 'n_splines_runqsz': 18, 'lam_runqsz': 994.2517512132359, 'spline_order_runqsz': 2, 'n_splines_freemem': 74, 'lam_freemem': 3.929532804906544, 'spline_order_freemem': 5, 'n_splines_freeswap': 87, 'lam_freeswap': 3.575852824797547, 'spline_order_freeswap': 4}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:30:18,452] Trial 20 finished with value: 4.0966447930156225 and parameters: {'n_splines_lread': 34, 'lam_lread': 0.008997001877884252, 'spline_order_lread': 5, 'n_splines_lwrite': 75, 'lam_lwrite': 53.28003467233155, 'spline_order_lwrite': 1, 'n_splines_scall': 61, 'lam_scall': 0.594868051898768, 'spline_order_scall': 1, 'n_splines_sread': 28, 'lam_sread': 0.388374649547542, 'spline_order_sread': 3, 'n_splines_swrite': 44, 'lam_swrite': 0.03932877484399556, 'spline_order_swrite': 3, 'n_splines_fork': 76, 'lam_fork': 102.97536673859301, 'spline_order_fork': 4, 'n_splines_exec': 10, 'lam_exec': 0.29768263012879215, 'spline_order_exec': 3, 'n_splines_rchar': 71, 'lam_rchar': 92.964183024101, 'spline_order_rchar': 4, 'n_splines_wchar': 83, 'lam_wchar': 0.19160887113987093, 'spline_order_wchar': 1, 'n_splines_pgout': 43, 'lam_pgout': 3.5600019463686636, 'spline_order_pgout': 3, 'n_splines_ppgout': 29, 'lam_ppgout': 0.0057652806357962475, 'spline_order_ppgout': 3, 'n_splines_pgfree': 63, 'lam_pgfree': 0.9937660765575025, 'spline_order_pgfree': 3, 'n_splines_pgscan': 92, 'lam_pgscan': 2.2267808958169777, 'spline_order_pgscan': 2, 'n_splines_atch': 57, 'lam_atch': 0.32010878517105984, 'spline_order_atch': 2, 'n_splines_pgin': 52, 'lam_pgin': 0.022657878689494846, 'spline_order_pgin': 5, 'n_splines_ppgin': 78, 'lam_ppgin': 991.0359148084926, 'spline_order_ppgin': 1, 'n_splines_pflt': 48, 'lam_pflt': 0.22306922146411337, 'spline_order_pflt': 1, 'n_splines_vflt': 72, 'lam_vflt': 2.726327068546947, 'spline_order_vflt': 3, 'n_splines_runqsz': 10, 'lam_runqsz': 50.47580463991455, 'spline_order_runqsz': 2, 'n_splines_freemem': 70, 'lam_freemem': 0.21823580742890292, 'spline_order_freemem': 5, 'n_splines_freeswap': 100, 'lam_freeswap': 0.08388184580212214, 'spline_order_freeswap': 3}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:30:32,247] Trial 21 finished with value: 3.3102583073842613 and parameters: {'n_splines_lread': 35, 'lam_lread': 0.0076962269725175565, 'spline_order_lread': 5, 'n_splines_lwrite': 78, 'lam_lwrite': 103.80133475084519, 'spline_order_lwrite': 1, 'n_splines_scall': 64, 'lam_scall': 0.5060526456195338, 'spline_order_scall': 1, 'n_splines_sread': 27, 'lam_sread': 0.18409129004440775, 'spline_order_sread': 3, 'n_splines_swrite': 47, 'lam_swrite': 0.05898388113261986, 'spline_order_swrite': 3, 'n_splines_fork': 76, 'lam_fork': 110.59313381153034, 'spline_order_fork': 4, 'n_splines_exec': 13, 'lam_exec': 0.30743877121185614, 'spline_order_exec': 3, 'n_splines_rchar': 68, 'lam_rchar': 67.32432585387662, 'spline_order_rchar': 4, 'n_splines_wchar': 99, 'lam_wchar': 0.2767885619880517, 'spline_order_wchar': 1, 'n_splines_pgout': 43, 'lam_pgout': 5.0655601873776295, 'spline_order_pgout': 3, 'n_splines_ppgout': 27, 'lam_ppgout': 0.005426424497032486, 'spline_order_ppgout': 3, 'n_splines_pgfree': 63, 'lam_pgfree': 0.7211912180218973, 'spline_order_pgfree': 3, 'n_splines_pgscan': 91, 'lam_pgscan': 2.2175216146277172, 'spline_order_pgscan': 2, 'n_splines_atch': 56, 'lam_atch': 0.33245721475662404, 'spline_order_atch': 2, 'n_splines_pgin': 57, 'lam_pgin': 0.02181069050704565, 'spline_order_pgin': 5, 'n_splines_ppgin': 79, 'lam_ppgin': 938.9022449838683, 'spline_order_ppgin': 1, 'n_splines_pflt': 49, 'lam_pflt': 0.31486207367423236, 'spline_order_pflt': 1, 'n_splines_vflt': 71, 'lam_vflt': 2.2628068990432233, 'spline_order_vflt': 4, 'n_splines_runqsz': 12, 'lam_runqsz': 163.7424937170347, 'spline_order_runqsz': 2, 'n_splines_freemem': 70, 'lam_freemem': 0.1472025474660888, 'spline_order_freemem': 5, 'n_splines_freeswap': 96, 'lam_freeswap': 0.18671302634469922, 'spline_order_freeswap': 3}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:30:50,596] Trial 22 finished with value: 3.6878330634271497 and parameters: {'n_splines_lread': 35, 'lam_lread': 0.00635660519840598, 'spline_order_lread': 5, 'n_splines_lwrite': 79, 'lam_lwrite': 96.28423173934819, 'spline_order_lwrite': 1, 'n_splines_scall': 64, 'lam_scall': 0.581328249034496, 'spline_order_scall': 1, 'n_splines_sread': 25, 'lam_sread': 0.05790305120870699, 'spline_order_sread': 4, 'n_splines_swrite': 45, 'lam_swrite': 0.047023457538219085, 'spline_order_swrite': 2, 'n_splines_fork': 89, 'lam_fork': 70.31089700037474, 'spline_order_fork': 4, 'n_splines_exec': 11, 'lam_exec': 0.25921582255006664, 'spline_order_exec': 3, 'n_splines_rchar': 69, 'lam_rchar': 131.62951798040808, 'spline_order_rchar': 5, 'n_splines_wchar': 84, 'lam_wchar': 0.3953024763394232, 'spline_order_wchar': 1, 'n_splines_pgout': 39, 'lam_pgout': 0.37803384060701045, 'spline_order_pgout': 3, 'n_splines_ppgout': 28, 'lam_ppgout': 0.0052564046429012115, 'spline_order_ppgout': 2, 'n_splines_pgfree': 60, 'lam_pgfree': 0.3632014360393156, 'spline_order_pgfree': 3, 'n_splines_pgscan': 78, 'lam_pgscan': 4.1851479492549215, 'spline_order_pgscan': 2, 'n_splines_atch': 38, 'lam_atch': 0.30562461916531347, 'spline_order_atch': 2, 'n_splines_pgin': 52, 'lam_pgin': 0.005294717777938952, 'spline_order_pgin': 5, 'n_splines_ppgin': 89, 'lam_ppgin': 139.76617436876924, 'spline_order_ppgin': 1, 'n_splines_pflt': 99, 'lam_pflt': 0.27757534338186873, 'spline_order_pflt': 1, 'n_splines_vflt': 69, 'lam_vflt': 4.75687939151724, 'spline_order_vflt': 4, 'n_splines_runqsz': 12, 'lam_runqsz': 53.5441130173971, 'spline_order_runqsz': 3, 'n_splines_freemem': 88, 'lam_freemem': 0.1434895753825371, 'spline_order_freemem': 4, 'n_splines_freeswap': 98, 'lam_freeswap': 0.34644617424001034, 'spline_order_freeswap': 3}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:31:03,686] Trial 23 finished with value: 42.9745681700686 and parameters: {'n_splines_lread': 15, 'lam_lread': 0.07625919701722679, 'spline_order_lread': 5, 'n_splines_lwrite': 91, 'lam_lwrite': 946.4306246446337, 'spline_order_lwrite': 1, 'n_splines_scall': 71, 'lam_scall': 0.3943955211296358, 'spline_order_scall': 1, 'n_splines_sread': 48, 'lam_sread': 0.023299118174667816, 'spline_order_sread': 4, 'n_splines_swrite': 40, 'lam_swrite': 0.003298038451589538, 'spline_order_swrite': 2, 'n_splines_fork': 89, 'lam_fork': 797.7018800906768, 'spline_order_fork': 4, 'n_splines_exec': 10, 'lam_exec': 0.005675416727398919, 'spline_order_exec': 4, 'n_splines_rchar': 68, 'lam_rchar': 791.3778453885458, 'spline_order_rchar': 5, 'n_splines_wchar': 79, 'lam_wchar': 0.6700979028378898, 'spline_order_wchar': 2, 'n_splines_pgout': 26, 'lam_pgout': 0.5534416706367642, 'spline_order_pgout': 3, 'n_splines_ppgout': 47, 'lam_ppgout': 0.03136348748655526, 'spline_order_ppgout': 2, 'n_splines_pgfree': 79, 'lam_pgfree': 0.22056034650262715, 'spline_order_pgfree': 4, 'n_splines_pgscan': 77, 'lam_pgscan': 9.13628530032472, 'spline_order_pgscan': 3, 'n_splines_atch': 26, 'lam_atch': 0.0651617407464079, 'spline_order_atch': 2, 'n_splines_pgin': 61, 'lam_pgin': 0.005128486769431396, 'spline_order_pgin': 5, 'n_splines_ppgin': 90, 'lam_ppgin': 140.5799341191361, 'spline_order_ppgin': 2, 'n_splines_pflt': 68, 'lam_pflt': 0.31017330336627535, 'spline_order_pflt': 1, 'n_splines_vflt': 60, 'lam_vflt': 7.168611047269612, 'spline_order_vflt': 4, 'n_splines_runqsz': 39, 'lam_runqsz': 3.5339425477139805, 'spline_order_runqsz': 3, 'n_splines_freemem': 84, 'lam_freemem': 0.03747629063681122, 'spline_order_freemem': 4, 'n_splines_freeswap': 91, 'lam_freeswap': 0.28746610229095737, 'spline_order_freeswap': 2}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:31:15,600] Trial 24 finished with value: 8.711259827550268 and parameters: {'n_splines_lread': 38, 'lam_lread': 0.003784364825688498, 'spline_order_lread': 5, 'n_splines_lwrite': 77, 'lam_lwrite': 102.11185948073336, 'spline_order_lwrite': 2, 'n_splines_scall': 59, 'lam_scall': 0.03614158578409332, 'spline_order_scall': 2, 'n_splines_sread': 22, 'lam_sread': 0.05711571409037631, 'spline_order_sread': 4, 'n_splines_swrite': 52, 'lam_swrite': 0.0570313040243423, 'spline_order_swrite': 3, 'n_splines_fork': 88, 'lam_fork': 5.892133713379041, 'spline_order_fork': 4, 'n_splines_exec': 22, 'lam_exec': 0.3057553652206579, 'spline_order_exec': 3, 'n_splines_rchar': 47, 'lam_rchar': 62.226712752560786, 'spline_order_rchar': 5, 'n_splines_wchar': 90, 'lam_wchar': 10.117014220729569, 'spline_order_wchar': 1, 'n_splines_pgout': 48, 'lam_pgout': 0.010147586976157159, 'spline_order_pgout': 2, 'n_splines_ppgout': 25, 'lam_ppgout': 0.004200691284780758, 'spline_order_ppgout': 2, 'n_splines_pgfree': 59, 'lam_pgfree': 0.020329030214712576, 'spline_order_pgfree': 4, 'n_splines_pgscan': 65, 'lam_pgscan': 0.16899754468509887, 'spline_order_pgscan': 2, 'n_splines_atch': 41, 'lam_atch': 0.1757368665699332, 'spline_order_atch': 2, 'n_splines_pgin': 69, 'lam_pgin': 0.33000787264459036, 'spline_order_pgin': 5, 'n_splines_ppgin': 91, 'lam_ppgin': 42.82109067977796, 'spline_order_ppgin': 1, 'n_splines_pflt': 92, 'lam_pflt': 0.07217420826740037, 'spline_order_pflt': 1, 'n_splines_vflt': 75, 'lam_vflt': 9.669394254214295, 'spline_order_vflt': 4, 'n_splines_runqsz': 22, 'lam_runqsz': 117.88145661347593, 'spline_order_runqsz': 3, 'n_splines_freemem': 84, 'lam_freemem': 0.015608544271504005, 'spline_order_freemem': 4, 'n_splines_freeswap': 82, 'lam_freeswap': 0.0045736920894631685, 'spline_order_freeswap': 3}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:31:28,553] Trial 25 finished with value: 85.10240777422139 and parameters: {'n_splines_lread': 48, 'lam_lread': 1.0304609678573045, 'spline_order_lread': 5, 'n_splines_lwrite': 58, 'lam_lwrite': 673.4927470692832, 'spline_order_lwrite': 1, 'n_splines_scall': 87, 'lam_scall': 11.74240029188143, 'spline_order_scall': 1, 'n_splines_sread': 48, 'lam_sread': 0.004470218607045884, 'spline_order_sread': 3, 'n_splines_swrite': 28, 'lam_swrite': 0.5384378853928224, 'spline_order_swrite': 2, 'n_splines_fork': 98, 'lam_fork': 1.004263440351527, 'spline_order_fork': 3, 'n_splines_exec': 17, 'lam_exec': 0.10853183118907558, 'spline_order_exec': 4, 'n_splines_rchar': 85, 'lam_rchar': 158.85448979416967, 'spline_order_rchar': 5, 'n_splines_wchar': 70, 'lam_wchar': 1.0944703415150379, 'spline_order_wchar': 2, 'n_splines_pgout': 31, 'lam_pgout': 0.14614037275833866, 'spline_order_pgout': 2, 'n_splines_ppgout': 48, 'lam_ppgout': 0.0011060441637456913, 'spline_order_ppgout': 2, 'n_splines_pgfree': 71, 'lam_pgfree': 0.19115290651047928, 'spline_order_pgfree': 3, 'n_splines_pgscan': 80, 'lam_pgscan': 28.354910082145945, 'spline_order_pgscan': 4, 'n_splines_atch': 42, 'lam_atch': 0.6280271747795283, 'spline_order_atch': 2, 'n_splines_pgin': 49, 'lam_pgin': 0.006371634346348047, 'spline_order_pgin': 4, 'n_splines_ppgin': 71, 'lam_ppgin': 16.242381296344707, 'spline_order_ppgin': 2, 'n_splines_pflt': 90, 'lam_pflt': 0.5786659757452736, 'spline_order_pflt': 1, 'n_splines_vflt': 92, 'lam_vflt': 0.8175056239911332, 'spline_order_vflt': 5, 'n_splines_runqsz': 64, 'lam_runqsz': 0.7035870769836441, 'spline_order_runqsz': 2, 'n_splines_freemem': 81, 'lam_freemem': 0.20886520308133374, 'spline_order_freemem': 4, 'n_splines_freeswap': 100, 'lam_freeswap': 1.4829455987938625, 'spline_order_freeswap': 4}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:31:39,185] Trial 26 finished with value: 11.334957431287998 and parameters: {'n_splines_lread': 33, 'lam_lread': 60.41437393351059, 'spline_order_lread': 4, 'n_splines_lwrite': 81, 'lam_lwrite': 38.340476705703345, 'spline_order_lwrite': 2, 'n_splines_scall': 70, 'lam_scall': 6.536379273123907, 'spline_order_scall': 2, 'n_splines_sread': 33, 'lam_sread': 0.0914443134742772, 'spline_order_sread': 4, 'n_splines_swrite': 74, 'lam_swrite': 0.010617246229873159, 'spline_order_swrite': 2, 'n_splines_fork': 81, 'lam_fork': 44.8941518867368, 'spline_order_fork': 4, 'n_splines_exec': 26, 'lam_exec': 0.01116034562297032, 'spline_order_exec': 2, 'n_splines_rchar': 69, 'lam_rchar': 18.62373913788968, 'spline_order_rchar': 4, 'n_splines_wchar': 94, 'lam_wchar': 0.03694007048573157, 'spline_order_wchar': 1, 'n_splines_pgout': 21, 'lam_pgout': 0.029258597439422055, 'spline_order_pgout': 3, 'n_splines_ppgout': 11, 'lam_ppgout': 0.016906750473074675, 'spline_order_ppgout': 3, 'n_splines_pgfree': 55, 'lam_pgfree': 0.019018987756589105, 'spline_order_pgfree': 3, 'n_splines_pgscan': 100, 'lam_pgscan': 132.79924995833153, 'spline_order_pgscan': 2, 'n_splines_atch': 52, 'lam_atch': 0.058634562517653914, 'spline_order_atch': 1, 'n_splines_pgin': 34, 'lam_pgin': 0.001748841837843934, 'spline_order_pgin': 5, 'n_splines_ppgin': 83, 'lam_ppgin': 274.29726199079636, 'spline_order_ppgin': 1, 'n_splines_pflt': 100, 'lam_pflt': 2.096731753426846, 'spline_order_pflt': 2, 'n_splines_vflt': 60, 'lam_vflt': 302.5700290308665, 'spline_order_vflt': 4, 'n_splines_runqsz': 10, 'lam_runqsz': 35.18433623428116, 'spline_order_runqsz': 4, 'n_splines_freemem': 63, 'lam_freemem': 0.08434400924695819, 'spline_order_freemem': 3, 'n_splines_freeswap': 82, 'lam_freeswap': 11.98390734416778, 'spline_order_freeswap': 2}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:31:48,913] Trial 27 finished with value: 10.652157106314062 and parameters: {'n_splines_lread': 20, 'lam_lread': 0.004646114732409169, 'spline_order_lread': 5, 'n_splines_lwrite': 91, 'lam_lwrite': 0.009137589970658989, 'spline_order_lwrite': 1, 'n_splines_scall': 55, 'lam_scall': 0.3051697541642199, 'spline_order_scall': 1, 'n_splines_sread': 12, 'lam_sread': 0.011692353292899036, 'spline_order_sread': 3, 'n_splines_swrite': 45, 'lam_swrite': 0.004396450772630056, 'spline_order_swrite': 3, 'n_splines_fork': 60, 'lam_fork': 355.27697839954055, 'spline_order_fork': 3, 'n_splines_exec': 10, 'lam_exec': 5.008094038530759, 'spline_order_exec': 3, 'n_splines_rchar': 96, 'lam_rchar': 896.8275246450402, 'spline_order_rchar': 4, 'n_splines_wchar': 83, 'lam_wchar': 0.016859037498866192, 'spline_order_wchar': 2, 'n_splines_pgout': 37, 'lam_pgout': 1.2891282629653533, 'spline_order_pgout': 3, 'n_splines_ppgout': 36, 'lam_ppgout': 0.056204309064456746, 'spline_order_ppgout': 3, 'n_splines_pgfree': 77, 'lam_pgfree': 0.5670945572744307, 'spline_order_pgfree': 4, 'n_splines_pgscan': 89, 'lam_pgscan': 7.287047765995831, 'spline_order_pgscan': 3, 'n_splines_atch': 28, 'lam_atch': 46.165977400095436, 'spline_order_atch': 3, 'n_splines_pgin': 57, 'lam_pgin': 0.06680485573672154, 'spline_order_pgin': 4, 'n_splines_ppgin': 94, 'lam_ppgin': 75.50732947627701, 'spline_order_ppgin': 2, 'n_splines_pflt': 23, 'lam_pflt': 0.11828957022452852, 'spline_order_pflt': 1, 'n_splines_vflt': 63, 'lam_vflt': 0.22324015955310583, 'spline_order_vflt': 4, 'n_splines_runqsz': 16, 'lam_runqsz': 201.3532064912807, 'spline_order_runqsz': 3, 'n_splines_freemem': 89, 'lam_freemem': 0.4276145148042888, 'spline_order_freemem': 5, 'n_splines_freeswap': 91, 'lam_freeswap': 0.4629360166290329, 'spline_order_freeswap': 3}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:31:58,846] Trial 28 finished with value: 31.589654925622295 and parameters: {'n_splines_lread': 39, 'lam_lread': 0.5357954227081512, 'spline_order_lread': 4, 'n_splines_lwrite': 64, 'lam_lwrite': 2.2776789484023667, 'spline_order_lwrite': 2, 'n_splines_scall': 73, 'lam_scall': 0.036685323024656685, 'spline_order_scall': 2, 'n_splines_sread': 20, 'lam_sread': 0.0013040977923849044, 'spline_order_sread': 3, 'n_splines_swrite': 54, 'lam_swrite': 48.695362664706245, 'spline_order_swrite': 2, 'n_splines_fork': 91, 'lam_fork': 0.10273638193305934, 'spline_order_fork': 4, 'n_splines_exec': 17, 'lam_exec': 0.5368139023804337, 'spline_order_exec': 1, 'n_splines_rchar': 78, 'lam_rchar': 1.2792954093017581, 'spline_order_rchar': 5, 'n_splines_wchar': 76, 'lam_wchar': 0.1910881311548335, 'spline_order_wchar': 1, 'n_splines_pgout': 48, 'lam_pgout': 0.2680861673461686, 'spline_order_pgout': 2, 'n_splines_ppgout': 41, 'lam_ppgout': 0.00873053746722992, 'spline_order_ppgout': 2, 'n_splines_pgfree': 88, 'lam_pgfree': 4.547772968157617, 'spline_order_pgfree': 3, 'n_splines_pgscan': 63, 'lam_pgscan': 1.5930358832059586, 'spline_order_pgscan': 1, 'n_splines_atch': 24, 'lam_atch': 0.00135152119363538, 'spline_order_atch': 2, 'n_splines_pgin': 47, 'lam_pgin': 0.5371891459352492, 'spline_order_pgin': 5, 'n_splines_ppgin': 51, 'lam_ppgin': 8.740291538641506, 'spline_order_ppgin': 1, 'n_splines_pflt': 61, 'lam_pflt': 0.7938625064454095, 'spline_order_pflt': 4, 'n_splines_vflt': 47, 'lam_vflt': 2.0502581798277157, 'spline_order_vflt': 5, 'n_splines_runqsz': 38, 'lam_runqsz': 11.513381172133132, 'spline_order_runqsz': 2, 'n_splines_freemem': 79, 'lam_freemem': 0.007163523056129903, 'spline_order_freemem': 4, 'n_splines_freeswap': 73, 'lam_freeswap': 0.24378651414366984, 'spline_order_freeswap': 1}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:32:07,631] Trial 29 finished with value: 19.23743092130858 and parameters: {'n_splines_lread': 29, 'lam_lread': 0.028074086802529912, 'spline_order_lread': 5, 'n_splines_lwrite': 52, 'lam_lwrite': 16.08842433983242, 'spline_order_lwrite': 2, 'n_splines_scall': 91, 'lam_scall': 49.52993348876601, 'spline_order_scall': 1, 'n_splines_sread': 24, 'lam_sread': 0.38157495573203887, 'spline_order_sread': 5, 'n_splines_swrite': 28, 'lam_swrite': 0.0014242207429576664, 'spline_order_swrite': 3, 'n_splines_fork': 75, 'lam_fork': 45.38719074965081, 'spline_order_fork': 1, 'n_splines_exec': 30, 'lam_exec': 0.07418682204588402, 'spline_order_exec': 3, 'n_splines_rchar': 44, 'lam_rchar': 38.08203091766046, 'spline_order_rchar': 4, 'n_splines_wchar': 57, 'lam_wchar': 0.4258227075046533, 'spline_order_wchar': 2, 'n_splines_pgout': 47, 'lam_pgout': 8.418560960547262, 'spline_order_pgout': 3, 'n_splines_ppgout': 26, 'lam_ppgout': 0.4761129798761849, 'spline_order_ppgout': 5, 'n_splines_pgfree': 43, 'lam_pgfree': 0.0013513077477428058, 'spline_order_pgfree': 2, 'n_splines_pgscan': 47, 'lam_pgscan': 64.22302226189798, 'spline_order_pgscan': 1, 'n_splines_atch': 72, 'lam_atch': 1.168951562471412, 'spline_order_atch': 3, 'n_splines_pgin': 65, 'lam_pgin': 0.05352808413367677, 'spline_order_pgin': 4, 'n_splines_ppgin': 82, 'lam_ppgin': 414.1276576328228, 'spline_order_ppgin': 2, 'n_splines_pflt': 37, 'lam_pflt': 81.56526939677602, 'spline_order_pflt': 2, 'n_splines_vflt': 77, 'lam_vflt': 15.129924515770716, 'spline_order_vflt': 3, 'n_splines_runqsz': 25, 'lam_runqsz': 315.2457045030004, 'spline_order_runqsz': 3, 'n_splines_freemem': 50, 'lam_freemem': 0.06480690717412899, 'spline_order_freemem': 3, 'n_splines_freeswap': 58, 'lam_freeswap': 0.004863284213737886, 'spline_order_freeswap': 2}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:32:18,796] Trial 30 finished with value: 35.889870650042425 and parameters: {'n_splines_lread': 49, 'lam_lread': 0.08053567168656467, 'spline_order_lread': 4, 'n_splines_lwrite': 80, 'lam_lwrite': 242.21086348225904, 'spline_order_lwrite': 3, 'n_splines_scall': 60, 'lam_scall': 1.0596967040181502, 'spline_order_scall': 3, 'n_splines_sread': 10, 'lam_sread': 0.1643658120187721, 'spline_order_sread': 2, 'n_splines_swrite': 40, 'lam_swrite': 0.7604496089612299, 'spline_order_swrite': 3, 'n_splines_fork': 100, 'lam_fork': 0.9832235231533301, 'spline_order_fork': 3, 'n_splines_exec': 23, 'lam_exec': 1.1350901953692007, 'spline_order_exec': 2, 'n_splines_rchar': 64, 'lam_rchar': 10.596172248138126, 'spline_order_rchar': 5, 'n_splines_wchar': 66, 'lam_wchar': 88.91614821898364, 'spline_order_wchar': 1, 'n_splines_pgout': 64, 'lam_pgout': 0.019408213612131978, 'spline_order_pgout': 1, 'n_splines_ppgout': 52, 'lam_ppgout': 0.004180813989768609, 'spline_order_ppgout': 4, 'n_splines_pgfree': 65, 'lam_pgfree': 0.028352409591717878, 'spline_order_pgfree': 4, 'n_splines_pgscan': 76, 'lam_pgscan': 0.03870591627706203, 'spline_order_pgscan': 3, 'n_splines_atch': 37, 'lam_atch': 7.645343027753007, 'spline_order_atch': 1, 'n_splines_pgin': 34, 'lam_pgin': 0.004961051027467326, 'spline_order_pgin': 3, 'n_splines_ppgin': 94, 'lam_ppgin': 138.3070567839, 'spline_order_ppgin': 1, 'n_splines_pflt': 52, 'lam_pflt': 0.11852290663303718, 'spline_order_pflt': 2, 'n_splines_vflt': 86, 'lam_vflt': 109.84484309074311, 'spline_order_vflt': 3, 'n_splines_runqsz': 42, 'lam_runqsz': 46.54117726182208, 'spline_order_runqsz': 1, 'n_splines_freemem': 47, 'lam_freemem': 0.4448426487562184, 'spline_order_freemem': 5, 'n_splines_freeswap': 94, 'lam_freeswap': 0.03737205812639961, 'spline_order_freeswap': 4}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:32:31,964] Trial 31 finished with value: 5.070058876075268 and parameters: {'n_splines_lread': 33, 'lam_lread': 0.008500381304172794, 'spline_order_lread': 5, 'n_splines_lwrite': 73, 'lam_lwrite': 56.49280111345093, 'spline_order_lwrite': 1, 'n_splines_scall': 63, 'lam_scall': 0.8017798172081181, 'spline_order_scall': 1, 'n_splines_sread': 33, 'lam_sread': 0.9890130309084291, 'spline_order_sread': 3, 'n_splines_swrite': 44, 'lam_swrite': 0.03843104278638273, 'spline_order_swrite': 3, 'n_splines_fork': 77, 'lam_fork': 71.07522057126528, 'spline_order_fork': 4, 'n_splines_exec': 10, 'lam_exec': 0.16896944173312525, 'spline_order_exec': 3, 'n_splines_rchar': 75, 'lam_rchar': 77.50194052801504, 'spline_order_rchar': 4, 'n_splines_wchar': 84, 'lam_wchar': 0.18519209306368103, 'spline_order_wchar': 1, 'n_splines_pgout': 43, 'lam_pgout': 0.7603594103534067, 'spline_order_pgout': 3, 'n_splines_ppgout': 30, 'lam_ppgout': 0.004890365391893868, 'spline_order_ppgout': 3, 'n_splines_pgfree': 56, 'lam_pgfree': 1.0329136683567848, 'spline_order_pgfree': 3, 'n_splines_pgscan': 92, 'lam_pgscan': 1.7429317243570843, 'spline_order_pgscan': 2, 'n_splines_atch': 60, 'lam_atch': 0.26340598451982317, 'spline_order_atch': 2, 'n_splines_pgin': 100, 'lam_pgin': 0.012521572760699949, 'spline_order_pgin': 5, 'n_splines_ppgin': 79, 'lam_ppgin': 655.2264699116399, 'spline_order_ppgin': 1, 'n_splines_pflt': 42, 'lam_pflt': 0.30889602702409397, 'spline_order_pflt': 1, 'n_splines_vflt': 71, 'lam_vflt': 4.559908121007474, 'spline_order_vflt': 4, 'n_splines_runqsz': 10, 'lam_runqsz': 60.4193750824214, 'spline_order_runqsz': 2, 'n_splines_freemem': 67, 'lam_freemem': 0.24827404685878274, 'spline_order_freemem': 5, 'n_splines_freeswap': 99, 'lam_freeswap': 0.12127295793972921, 'spline_order_freeswap': 3}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:32:42,793] Trial 32 finished with value: 3.448732815234561 and parameters: {'n_splines_lread': 35, 'lam_lread': 0.001886558815611251, 'spline_order_lread': 5, 'n_splines_lwrite': 76, 'lam_lwrite': 56.82232525492704, 'spline_order_lwrite': 1, 'n_splines_scall': 78, 'lam_scall': 0.4742793296425831, 'spline_order_scall': 1, 'n_splines_sread': 26, 'lam_sread': 0.12067715685315702, 'spline_order_sread': 4, 'n_splines_swrite': 40, 'lam_swrite': 0.06146756510357965, 'spline_order_swrite': 3, 'n_splines_fork': 85, 'lam_fork': 11.7757551798656, 'spline_order_fork': 4, 'n_splines_exec': 15, 'lam_exec': 0.6691398113838427, 'spline_order_exec': 3, 'n_splines_rchar': 72, 'lam_rchar': 140.5377929479533, 'spline_order_rchar': 4, 'n_splines_wchar': 85, 'lam_wchar': 0.6633574118109666, 'spline_order_wchar': 1, 'n_splines_pgout': 38, 'lam_pgout': 5.263870893944615, 'spline_order_pgout': 3, 'n_splines_ppgout': 20, 'lam_ppgout': 0.010838264191928272, 'spline_order_ppgout': 3, 'n_splines_pgfree': 66, 'lam_pgfree': 0.19005139175130492, 'spline_order_pgfree': 3, 'n_splines_pgscan': 91, 'lam_pgscan': 0.27797683497202735, 'spline_order_pgscan': 2, 'n_splines_atch': 58, 'lam_atch': 0.47355732827503916, 'spline_order_atch': 2, 'n_splines_pgin': 50, 'lam_pgin': 0.05500322409208187, 'spline_order_pgin': 5, 'n_splines_ppgin': 74, 'lam_ppgin': 91.53099057123902, 'spline_order_ppgin': 1, 'n_splines_pflt': 54, 'lam_pflt': 0.02012962060786142, 'spline_order_pflt': 1, 'n_splines_vflt': 66, 'lam_vflt': 1.8877836434957316, 'spline_order_vflt': 3, 'n_splines_runqsz': 16, 'lam_runqsz': 265.29400918673497, 'spline_order_runqsz': 2, 'n_splines_freemem': 59, 'lam_freemem': 0.028735838122237248, 'spline_order_freemem': 5, 'n_splines_freeswap': 85, 'lam_freeswap': 0.7625426908010692, 'spline_order_freeswap': 3}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:32:55,812] Trial 33 finished with value: 6.25829385354419 and parameters: {'n_splines_lread': 43, 'lam_lread': 0.0021835397983336093, 'spline_order_lread': 4, 'n_splines_lwrite': 82, 'lam_lwrite': 0.08353448256646094, 'spline_order_lwrite': 1, 'n_splines_scall': 73, 'lam_scall': 0.09776150868848375, 'spline_order_scall': 1, 'n_splines_sread': 43, 'lam_sread': 0.021778704228340447, 'spline_order_sread': 4, 'n_splines_swrite': 57, 'lam_swrite': 2.6060272789704357, 'spline_order_swrite': 4, 'n_splines_fork': 85, 'lam_fork': 7.54601069934909, 'spline_order_fork': 5, 'n_splines_exec': 16, 'lam_exec': 5.870643109742414, 'spline_order_exec': 4, 'n_splines_rchar': 86, 'lam_rchar': 325.9745334649359, 'spline_order_rchar': 3, 'n_splines_wchar': 95, 'lam_wchar': 2.1271192853120007, 'spline_order_wchar': 1, 'n_splines_pgout': 20, 'lam_pgout': 6.138312282469885, 'spline_order_pgout': 4, 'n_splines_ppgout': 18, 'lam_ppgout': 0.04892984238705879, 'spline_order_ppgout': 3, 'n_splines_pgfree': 75, 'lam_pgfree': 0.18180648440042815, 'spline_order_pgfree': 3, 'n_splines_pgscan': 86, 'lam_pgscan': 0.2410064390574171, 'spline_order_pgscan': 2, 'n_splines_atch': 44, 'lam_atch': 0.6893604101772115, 'spline_order_atch': 2, 'n_splines_pgin': 55, 'lam_pgin': 0.05657175312208301, 'spline_order_pgin': 5, 'n_splines_ppgin': 72, 'lam_ppgin': 75.67883385592575, 'spline_order_ppgin': 1, 'n_splines_pflt': 54, 'lam_pflt': 0.025124114273614967, 'spline_order_pflt': 1, 'n_splines_vflt': 64, 'lam_vflt': 0.23469082833584712, 'spline_order_vflt': 3, 'n_splines_runqsz': 25, 'lam_runqsz': 263.6163577350462, 'spline_order_runqsz': 2, 'n_splines_freemem': 58, 'lam_freemem': 0.028419982015985824, 'spline_order_freemem': 4, 'n_splines_freeswap': 85, 'lam_freeswap': 0.8495702771898532, 'spline_order_freeswap': 3}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:33:08,564] Trial 34 finished with value: 4.758652408306981 and parameters: {'n_splines_lread': 18, 'lam_lread': 0.002306089958285297, 'spline_order_lread': 5, 'n_splines_lwrite': 92, 'lam_lwrite': 453.28534597272784, 'spline_order_lwrite': 1, 'n_splines_scall': 79, 'lam_scall': 4.326726859237782, 'spline_order_scall': 1, 'n_splines_sread': 36, 'lam_sread': 0.13762189859100696, 'spline_order_sread': 5, 'n_splines_swrite': 51, 'lam_swrite': 0.41102509183487335, 'spline_order_swrite': 2, 'n_splines_fork': 65, 'lam_fork': 12.449365179338875, 'spline_order_fork': 4, 'n_splines_exec': 55, 'lam_exec': 0.6752746230765461, 'spline_order_exec': 3, 'n_splines_rchar': 56, 'lam_rchar': 148.18870255144319, 'spline_order_rchar': 4, 'n_splines_wchar': 85, 'lam_wchar': 0.5580081892237025, 'spline_order_wchar': 2, 'n_splines_pgout': 35, 'lam_pgout': 1.3494041033085364, 'spline_order_pgout': 3, 'n_splines_ppgout': 10, 'lam_ppgout': 0.012467345665228129, 'spline_order_ppgout': 1, 'n_splines_pgfree': 53, 'lam_pgfree': 0.44999179787560367, 'spline_order_pgfree': 2, 'n_splines_pgscan': 92, 'lam_pgscan': 0.6290405667024405, 'spline_order_pgscan': 5, 'n_splines_atch': 62, 'lam_atch': 2.0902377672451347, 'spline_order_atch': 3, 'n_splines_pgin': 71, 'lam_pgin': 1.8822198553496767, 'spline_order_pgin': 4, 'n_splines_ppgin': 63, 'lam_ppgin': 3.279717548036422, 'spline_order_ppgin': 2, 'n_splines_pflt': 37, 'lam_pflt': 0.0038581035032860273, 'spline_order_pflt': 1, 'n_splines_vflt': 91, 'lam_vflt': 1.4817171867793584, 'spline_order_vflt': 4, 'n_splines_runqsz': 16, 'lam_runqsz': 142.45564686720857, 'spline_order_runqsz': 1, 'n_splines_freemem': 63, 'lam_freemem': 50.8991041670513, 'spline_order_freemem': 5, 'n_splines_freeswap': 76, 'lam_freeswap': 0.2527361455668816, 'spline_order_freeswap': 3}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:33:24,589] Trial 35 finished with value: 3.495550192085576 and parameters: {'n_splines_lread': 24, 'lam_lread': 0.008078276849404428, 'spline_order_lread': 4, 'n_splines_lwrite': 72, 'lam_lwrite': 99.8884396500184, 'spline_order_lwrite': 2, 'n_splines_scall': 83, 'lam_scall': 0.21105583933487654, 'spline_order_scall': 2, 'n_splines_sread': 18, 'lam_sread': 0.0108810632490019, 'spline_order_sread': 4, 'n_splines_swrite': 39, 'lam_swrite': 0.07300880804102634, 'spline_order_swrite': 3, 'n_splines_fork': 94, 'lam_fork': 3.131335625309982, 'spline_order_fork': 5, 'n_splines_exec': 35, 'lam_exec': 2.4701898569024903, 'spline_order_exec': 4, 'n_splines_rchar': 65, 'lam_rchar': 1.7267130996311213, 'spline_order_rchar': 5, 'n_splines_wchar': 73, 'lam_wchar': 1.6128544044869335, 'spline_order_wchar': 2, 'n_splines_pgout': 55, 'lam_pgout': 0.1505701015733015, 'spline_order_pgout': 4, 'n_splines_ppgout': 19, 'lam_ppgout': 1.686113048669984, 'spline_order_ppgout': 5, 'n_splines_pgfree': 66, 'lam_pgfree': 0.04109119058746687, 'spline_order_pgfree': 3, 'n_splines_pgscan': 35, 'lam_pgscan': 0.017100255743506605, 'spline_order_pgscan': 2, 'n_splines_atch': 56, 'lam_atch': 0.15148081287178486, 'spline_order_atch': 1, 'n_splines_pgin': 62, 'lam_pgin': 0.010188233094031425, 'spline_order_pgin': 5, 'n_splines_ppgin': 88, 'lam_ppgin': 13.314605374907602, 'spline_order_ppgin': 2, 'n_splines_pflt': 60, 'lam_pflt': 0.004507822154047148, 'spline_order_pflt': 1, 'n_splines_vflt': 54, 'lam_vflt': 0.566353542885498, 'spline_order_vflt': 4, 'n_splines_runqsz': 20, 'lam_runqsz': 496.1410257916093, 'spline_order_runqsz': 2, 'n_splines_freemem': 76, 'lam_freemem': 0.00973654746749941, 'spline_order_freemem': 4, 'n_splines_freeswap': 93, 'lam_freeswap': 3.0165857048503866, 'spline_order_freeswap': 2}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:33:35,248] Trial 36 finished with value: 5.707297805840814 and parameters: {'n_splines_lread': 26, 'lam_lread': 0.0010283311055200703, 'spline_order_lread': 4, 'n_splines_lwrite': 72, 'lam_lwrite': 125.81425429542281, 'spline_order_lwrite': 3, 'n_splines_scall': 99, 'lam_scall': 0.010301286235363744, 'spline_order_scall': 3, 'n_splines_sread': 18, 'lam_sread': 0.007330069878637134, 'spline_order_sread': 5, 'n_splines_swrite': 38, 'lam_swrite': 276.4645348761592, 'spline_order_swrite': 3, 'n_splines_fork': 92, 'lam_fork': 1.0817746844204539, 'spline_order_fork': 5, 'n_splines_exec': 32, 'lam_exec': 1.717834792940026, 'spline_order_exec': 4, 'n_splines_rchar': 61, 'lam_rchar': 0.010183720655788145, 'spline_order_rchar': 4, 'n_splines_wchar': 72, 'lam_wchar': 1.666455990004566, 'spline_order_wchar': 3, 'n_splines_pgout': 51, 'lam_pgout': 125.69710616463693, 'spline_order_pgout': 4, 'n_splines_ppgout': 21, 'lam_ppgout': 1.5231854209486508, 'spline_order_ppgout': 5, 'n_splines_pgfree': 91, 'lam_pgfree': 0.039071385325761036, 'spline_order_pgfree': 2, 'n_splines_pgscan': 35, 'lam_pgscan': 0.006315115069353168, 'spline_order_pgscan': 1, 'n_splines_atch': 56, 'lam_atch': 0.03229013515962036, 'spline_order_atch': 1, 'n_splines_pgin': 62, 'lam_pgin': 0.07169722170231395, 'spline_order_pgin': 4, 'n_splines_ppgin': 55, 'lam_ppgin': 11.148184552566972, 'spline_order_ppgin': 2, 'n_splines_pflt': 63, 'lam_pflt': 0.001026451050579766, 'spline_order_pflt': 2, 'n_splines_vflt': 22, 'lam_vflt': 0.47839486169950124, 'spline_order_vflt': 3, 'n_splines_runqsz': 20, 'lam_runqsz': 532.9156524488985, 'spline_order_runqsz': 2, 'n_splines_freemem': 47, 'lam_freemem': 0.006056849533063441, 'spline_order_freemem': 5, 'n_splines_freeswap': 78, 'lam_freeswap': 158.05421365432127, 'spline_order_freeswap': 1}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:33:45,853] Trial 37 finished with value: 6.899486270407959 and parameters: {'n_splines_lread': 59, 'lam_lread': 0.01265068753233493, 'spline_order_lread': 4, 'n_splines_lwrite': 29, 'lam_lwrite': 15.241120960028896, 'spline_order_lwrite': 3, 'n_splines_scall': 85, 'lam_scall': 0.17097767673248987, 'spline_order_scall': 3, 'n_splines_sread': 17, 'lam_sread': 0.0014439251051248186, 'spline_order_sread': 4, 'n_splines_swrite': 19, 'lam_swrite': 0.09440372115041576, 'spline_order_swrite': 3, 'n_splines_fork': 81, 'lam_fork': 2.4715001917260566, 'spline_order_fork': 5, 'n_splines_exec': 37, 'lam_exec': 10.378534150742215, 'spline_order_exec': 5, 'n_splines_rchar': 51, 'lam_rchar': 2.6092727679633447, 'spline_order_rchar': 3, 'n_splines_wchar': 76, 'lam_wchar': 4.226091111141496, 'spline_order_wchar': 3, 'n_splines_pgout': 58, 'lam_pgout': 10.010980266184651, 'spline_order_pgout': 1, 'n_splines_ppgout': 19, 'lam_ppgout': 0.22771754193706908, 'spline_order_ppgout': 5, 'n_splines_pgfree': 66, 'lam_pgfree': 0.01171034790537974, 'spline_order_pgfree': 4, 'n_splines_pgscan': 15, 'lam_pgscan': 0.001491837486532864, 'spline_order_pgscan': 4, 'n_splines_atch': 67, 'lam_atch': 0.1347074431890895, 'spline_order_atch': 1, 'n_splines_pgin': 79, 'lam_pgin': 971.7124722761498, 'spline_order_pgin': 5, 'n_splines_ppgin': 81, 'lam_ppgin': 1.6271442106198528, 'spline_order_ppgin': 2, 'n_splines_pflt': 57, 'lam_pflt': 0.0039726550216682135, 'spline_order_pflt': 1, 'n_splines_vflt': 56, 'lam_vflt': 0.1194041686087998, 'spline_order_vflt': 3, 'n_splines_runqsz': 28, 'lam_runqsz': 401.1945700631768, 'spline_order_runqsz': 1, 'n_splines_freemem': 75, 'lam_freemem': 0.006253397099240439, 'spline_order_freemem': 4, 'n_splines_freeswap': 92, 'lam_freeswap': 2.997673474902676, 'spline_order_freeswap': 2}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:33:56,970] Trial 38 finished with value: 34.04312733914845 and parameters: {'n_splines_lread': 11, 'lam_lread': 0.0033000896266670438, 'spline_order_lread': 3, 'n_splines_lwrite': 62, 'lam_lwrite': 1.9926974517126124, 'spline_order_lwrite': 2, 'n_splines_scall': 79, 'lam_scall': 22.325693073906372, 'spline_order_scall': 2, 'n_splines_sread': 45, 'lam_sread': 2.430447795240288, 'spline_order_sread': 1, 'n_splines_swrite': 31, 'lam_swrite': 0.25563945809372174, 'spline_order_swrite': 4, 'n_splines_fork': 94, 'lam_fork': 0.45331734007553565, 'spline_order_fork': 5, 'n_splines_exec': 53, 'lam_exec': 30.604908543970346, 'spline_order_exec': 4, 'n_splines_rchar': 75, 'lam_rchar': 0.006161980836075733, 'spline_order_rchar': 5, 'n_splines_wchar': 64, 'lam_wchar': 14.193044516150373, 'spline_order_wchar': 2, 'n_splines_pgout': 79, 'lam_pgout': 0.16245651400650288, 'spline_order_pgout': 4, 'n_splines_ppgout': 63, 'lam_ppgout': 2.830600388857082, 'spline_order_ppgout': 5, 'n_splines_pgfree': 49, 'lam_pgfree': 0.08143694282747588, 'spline_order_pgfree': 1, 'n_splines_pgscan': 36, 'lam_pgscan': 0.013571393768806807, 'spline_order_pgscan': 5, 'n_splines_atch': 78, 'lam_atch': 0.10224597820438365, 'spline_order_atch': 1, 'n_splines_pgin': 44, 'lam_pgin': 0.4196484763726268, 'spline_order_pgin': 4, 'n_splines_ppgin': 22, 'lam_ppgin': 43.88435297139917, 'spline_order_ppgin': 2, 'n_splines_pflt': 73, 'lam_pflt': 0.005755529270950902, 'spline_order_pflt': 2, 'n_splines_vflt': 39, 'lam_vflt': 0.5140778850004701, 'spline_order_vflt': 5, 'n_splines_runqsz': 60, 'lam_runqsz': 142.4320694786601, 'spline_order_runqsz': 2, 'n_splines_freemem': 55, 'lam_freemem': 0.015662795213444557, 'spline_order_freemem': 3, 'n_splines_freeswap': 79, 'lam_freeswap': 21.78032437667511, 'spline_order_freeswap': 2}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:34:06,386] Trial 39 finished with value: 13.488390150958221 and parameters: {'n_splines_lread': 23, 'lam_lread': 0.0019856471332804723, 'spline_order_lread': 5, 'n_splines_lwrite': 49, 'lam_lwrite': 0.010411533021209245, 'spline_order_lwrite': 2, 'n_splines_scall': 91, 'lam_scall': 0.0703016011328195, 'spline_order_scall': 5, 'n_splines_sread': 57, 'lam_sread': 9.110896855745757, 'spline_order_sread': 5, 'n_splines_swrite': 68, 'lam_swrite': 0.019536316365377913, 'spline_order_swrite': 3, 'n_splines_fork': 84, 'lam_fork': 0.011951781629413552, 'spline_order_fork': 5, 'n_splines_exec': 24, 'lam_exec': 2.193251010922158, 'spline_order_exec': 1, 'n_splines_rchar': 43, 'lam_rchar': 0.2910129208751239, 'spline_order_rchar': 3, 'n_splines_wchar': 45, 'lam_wchar': 0.9954383223163497, 'spline_order_wchar': 2, 'n_splines_pgout': 53, 'lam_pgout': 2.5991424598123425, 'spline_order_pgout': 2, 'n_splines_ppgout': 33, 'lam_ppgout': 0.08009155948037935, 'spline_order_ppgout': 4, 'n_splines_pgfree': 73, 'lam_pgfree': 0.002061298946257698, 'spline_order_pgfree': 3, 'n_splines_pgscan': 58, 'lam_pgscan': 0.04354666852868333, 'spline_order_pgscan': 1, 'n_splines_atch': 52, 'lam_atch': 0.013063546123941339, 'spline_order_atch': 1, 'n_splines_pgin': 73, 'lam_pgin': 0.009003069790445191, 'spline_order_pgin': 5, 'n_splines_ppgin': 65, 'lam_ppgin': 4.621017018039256, 'spline_order_ppgin': 1, 'n_splines_pflt': 51, 'lam_pflt': 0.025875054777869666, 'spline_order_pflt': 1, 'n_splines_vflt': 52, 'lam_vflt': 16.352347131845207, 'spline_order_vflt': 4, 'n_splines_runqsz': 35, 'lam_runqsz': 396.87985218910444, 'spline_order_runqsz': 1, 'n_splines_freemem': 60, 'lam_freemem': 0.0027710170703782394, 'spline_order_freemem': 5, 'n_splines_freeswap': 87, 'lam_freeswap': 5.633932806736744, 'spline_order_freeswap': 1}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:34:14,351] Trial 40 finished with value: 5.079993205365816 and parameters: {'n_splines_lread': 28, 'lam_lread': 7.653806235132285, 'spline_order_lread': 4, 'n_splines_lwrite': 69, 'lam_lwrite': 0.07401668349932039, 'spline_order_lwrite': 2, 'n_splines_scall': 21, 'lam_scall': 0.0052873296122862515, 'spline_order_scall': 2, 'n_splines_sread': 25, 'lam_sread': 4.256166634307947, 'spline_order_sread': 3, 'n_splines_swrite': 32, 'lam_swrite': 28.571433096178836, 'spline_order_swrite': 3, 'n_splines_fork': 50, 'lam_fork': 3.6235186729592916, 'spline_order_fork': 2, 'n_splines_exec': 50, 'lam_exec': 9.125183202407634, 'spline_order_exec': 5, 'n_splines_rchar': 90, 'lam_rchar': 21.458280951785973, 'spline_order_rchar': 2, 'n_splines_wchar': 54, 'lam_wchar': 34.562037747752726, 'spline_order_wchar': 2, 'n_splines_pgout': 67, 'lam_pgout': 0.0038768748398402266, 'spline_order_pgout': 4, 'n_splines_ppgout': 42, 'lam_ppgout': 0.0280598702839461, 'spline_order_ppgout': 5, 'n_splines_pgfree': 67, 'lam_pgfree': 0.006649099438730999, 'spline_order_pgfree': 3, 'n_splines_pgscan': 40, 'lam_pgscan': 0.0032716773828491, 'spline_order_pgscan': 3, 'n_splines_atch': 61, 'lam_atch': 1.164043949050456, 'spline_order_atch': 3, 'n_splines_pgin': 64, 'lam_pgin': 0.002291129947496472, 'spline_order_pgin': 4, 'n_splines_ppgin': 71, 'lam_ppgin': 28.02379967830519, 'spline_order_ppgin': 2, 'n_splines_pflt': 67, 'lam_pflt': 0.002117865759973289, 'spline_order_pflt': 4, 'n_splines_vflt': 77, 'lam_vflt': 0.01947789981863993, 'spline_order_vflt': 3, 'n_splines_runqsz': 15, 'lam_runqsz': 977.3808213328782, 'spline_order_runqsz': 2, 'n_splines_freemem': 10, 'lam_freemem': 0.047273145926084725, 'spline_order_freemem': 3, 'n_splines_freeswap': 72, 'lam_freeswap': 44.42116612895931, 'spline_order_freeswap': 2}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:34:25,117] Trial 41 finished with value: 9.302137587914252 and parameters: {'n_splines_lread': 36, 'lam_lread': 0.0065874896785387115, 'spline_order_lread': 5, 'n_splines_lwrite': 84, 'lam_lwrite': 66.62212737148607, 'spline_order_lwrite': 1, 'n_splines_scall': 67, 'lam_scall': 0.315499988011509, 'spline_order_scall': 1, 'n_splines_sread': 24, 'lam_sread': 0.04936993466869477, 'spline_order_sread': 4, 'n_splines_swrite': 41, 'lam_swrite': 0.06678135332203476, 'spline_order_swrite': 1, 'n_splines_fork': 88, 'lam_fork': 234.68558010758514, 'spline_order_fork': 4, 'n_splines_exec': 15, 'lam_exec': 0.7206925342366657, 'spline_order_exec': 3, 'n_splines_rchar': 66, 'lam_rchar': 237.1506247347055, 'spline_order_rchar': 5, 'n_splines_wchar': 89, 'lam_wchar': 0.09553330931743811, 'spline_order_wchar': 1, 'n_splines_pgout': 40, 'lam_pgout': 0.35478451577422715, 'spline_order_pgout': 3, 'n_splines_ppgout': 15, 'lam_ppgout': 0.002246104006213339, 'spline_order_ppgout': 3, 'n_splines_pgfree': 59, 'lam_pgfree': 0.24730245983443633, 'spline_order_pgfree': 3, 'n_splines_pgscan': 53, 'lam_pgscan': 0.3306352299710618, 'spline_order_pgscan': 2, 'n_splines_atch': 35, 'lam_atch': 0.4825401526865408, 'spline_order_atch': 2, 'n_splines_pgin': 54, 'lam_pgin': 0.03173116355386789, 'spline_order_pgin': 5, 'n_splines_ppgin': 89, 'lam_ppgin': 107.85002016034288, 'spline_order_ppgin': 1, 'n_splines_pflt': 48, 'lam_pflt': 0.008794945089901857, 'spline_order_pflt': 1, 'n_splines_vflt': 67, 'lam_vflt': 5.996308661714054, 'spline_order_vflt': 4, 'n_splines_runqsz': 24, 'lam_runqsz': 93.13059786066276, 'spline_order_runqsz': 3, 'n_splines_freemem': 93, 'lam_freemem': 0.016983752213497207, 'spline_order_freemem': 4, 'n_splines_freeswap': 93, 'lam_freeswap': 0.5453002094006517, 'spline_order_freeswap': 3}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:34:35,615] Trial 42 finished with value: 11.517398999855946 and parameters: {'n_splines_lread': 41, 'lam_lread': 0.020878314740927138, 'spline_order_lread': 5, 'n_splines_lwrite': 94, 'lam_lwrite': 278.42466530595937, 'spline_order_lwrite': 1, 'n_splines_scall': 67, 'lam_scall': 1.432540464528129, 'spline_order_scall': 1, 'n_splines_sread': 15, 'lam_sread': 0.014810245602077788, 'spline_order_sread': 4, 'n_splines_swrite': 50, 'lam_swrite': 0.0074759112262149695, 'spline_order_swrite': 2, 'n_splines_fork': 94, 'lam_fork': 45.151761170003496, 'spline_order_fork': 5, 'n_splines_exec': 21, 'lam_exec': 0.148447817096781, 'spline_order_exec': 3, 'n_splines_rchar': 59, 'lam_rchar': 482.7953461736381, 'spline_order_rchar': 5, 'n_splines_wchar': 95, 'lam_wchar': 0.28164798541653735, 'spline_order_wchar': 1, 'n_splines_pgout': 30, 'lam_pgout': 0.05473830212005631, 'spline_order_pgout': 3, 'n_splines_ppgout': 30, 'lam_ppgout': 0.002774528406262438, 'spline_order_ppgout': 3, 'n_splines_pgfree': 43, 'lam_pgfree': 0.13396904212119487, 'spline_order_pgfree': 3, 'n_splines_pgscan': 85, 'lam_pgscan': 0.014452306932180612, 'spline_order_pgscan': 2, 'n_splines_atch': 51, 'lam_atch': 0.30415260561897833, 'spline_order_atch': 2, 'n_splines_pgin': 48, 'lam_pgin': 0.011488776810631821, 'spline_order_pgin': 5, 'n_splines_ppgin': 95, 'lam_ppgin': 265.94298726986864, 'spline_order_ppgin': 1, 'n_splines_pflt': 77, 'lam_pflt': 0.02262091509774835, 'spline_order_pflt': 1, 'n_splines_vflt': 57, 'lam_vflt': 0.9361234812281722, 'spline_order_vflt': 4, 'n_splines_runqsz': 16, 'lam_runqsz': 219.00410706565458, 'spline_order_runqsz': 3, 'n_splines_freemem': 78, 'lam_freemem': 0.12879209637036884, 'spline_order_freemem': 4, 'n_splines_freeswap': 95, 'lam_freeswap': 2.0639419536822246, 'spline_order_freeswap': 3}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:34:45,494] Trial 43 finished with value: 8.694537524619562 and parameters: {'n_splines_lread': 19, 'lam_lread': 0.0052191153362575885, 'spline_order_lread': 5, 'n_splines_lwrite': 77, 'lam_lwrite': 29.680412699763366, 'spline_order_lwrite': 1, 'n_splines_scall': 51, 'lam_scall': 0.4860322884502016, 'spline_order_scall': 2, 'n_splines_sread': 35, 'lam_sread': 0.27537245817341754, 'spline_order_sread': 4, 'n_splines_swrite': 46, 'lam_swrite': 0.02308941853772356, 'spline_order_swrite': 1, 'n_splines_fork': 87, 'lam_fork': 13.72026745902908, 'spline_order_fork': 4, 'n_splines_exec': 65, 'lam_exec': 0.026237251580087616, 'spline_order_exec': 4, 'n_splines_rchar': 72, 'lam_rchar': 38.88171404969051, 'spline_order_rchar': 4, 'n_splines_wchar': 75, 'lam_wchar': 2.644736923497422, 'spline_order_wchar': 1, 'n_splines_pgout': 56, 'lam_pgout': 0.8581634807202677, 'spline_order_pgout': 3, 'n_splines_ppgout': 21, 'lam_ppgout': 1.8199102701908942, 'spline_order_ppgout': 1, 'n_splines_pgfree': 62, 'lam_pgfree': 0.051626790561651606, 'spline_order_pgfree': 3, 'n_splines_pgscan': 79, 'lam_pgscan': 0.09862300614394802, 'spline_order_pgscan': 2, 'n_splines_atch': 46, 'lam_atch': 0.03494280498952862, 'spline_order_atch': 2, 'n_splines_pgin': 59, 'lam_pgin': 0.002611420069654428, 'spline_order_pgin': 5, 'n_splines_ppgin': 86, 'lam_ppgin': 49.68025351457667, 'spline_order_ppgin': 1, 'n_splines_pflt': 82, 'lam_pflt': 0.042631161417912816, 'spline_order_pflt': 1, 'n_splines_vflt': 32, 'lam_vflt': 2.2287353781393135, 'spline_order_vflt': 5, 'n_splines_runqsz': 13, 'lam_runqsz': 22.748808095948103, 'spline_order_runqsz': 2, 'n_splines_freemem': 88, 'lam_freemem': 0.0031815130718558966, 'spline_order_freemem': 5, 'n_splines_freeswap': 84, 'lam_freeswap': 0.9065512473159993, 'spline_order_freeswap': 4}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:34:53,778] Trial 44 finished with value: 8.135148001638518 and parameters: {'n_splines_lread': 32, 'lam_lread': 0.0019680048142199425, 'spline_order_lread': 4, 'n_splines_lwrite': 57, 'lam_lwrite': 7.608864143069275, 'spline_order_lwrite': 2, 'n_splines_scall': 82, 'lam_scall': 0.20201235065445453, 'spline_order_scall': 1, 'n_splines_sread': 20, 'lam_sread': 0.7374954838463784, 'spline_order_sread': 5, 'n_splines_swrite': 38, 'lam_swrite': 441.1326560915754, 'spline_order_swrite': 4, 'n_splines_fork': 79, 'lam_fork': 0.18019796257129564, 'spline_order_fork': 3, 'n_splines_exec': 15, 'lam_exec': 2.1689868309714235, 'spline_order_exec': 2, 'n_splines_rchar': 64, 'lam_rchar': 9.517487993733615, 'spline_order_rchar': 5, 'n_splines_wchar': 82, 'lam_wchar': 0.7615633278930856, 'spline_order_wchar': 3, 'n_splines_pgout': 60, 'lam_pgout': 0.0990851718162301, 'spline_order_pgout': 2, 'n_splines_ppgout': 13, 'lam_ppgout': 0.0010549522459543448, 'spline_order_ppgout': 4, 'n_splines_pgfree': 49, 'lam_pgfree': 1.4481553144289436, 'spline_order_pgfree': 4, 'n_splines_pgscan': 27, 'lam_pgscan': 6.372406015906485, 'spline_order_pgscan': 2, 'n_splines_atch': 57, 'lam_atch': 2.8883632424757573, 'spline_order_atch': 3, 'n_splines_pgin': 37, 'lam_pgin': 0.001134442290311427, 'spline_order_pgin': 5, 'n_splines_ppgin': 75, 'lam_ppgin': 156.50994219931871, 'spline_order_ppgin': 1, 'n_splines_pflt': 58, 'lam_pflt': 12.43948376744833, 'spline_order_pflt': 1, 'n_splines_vflt': 48, 'lam_vflt': 0.10820767926831978, 'spline_order_vflt': 4, 'n_splines_runqsz': 21, 'lam_runqsz': 97.86946404041453, 'spline_order_runqsz': 3, 'n_splines_freemem': 96, 'lam_freemem': 0.0011837878919315377, 'spline_order_freemem': 4, 'n_splines_freeswap': 96, 'lam_freeswap': 8.898706524933022, 'spline_order_freeswap': 3}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:35:04,374] Trial 45 finished with value: 11.198794212668368 and parameters: {'n_splines_lread': 51, 'lam_lread': 0.05259034768254584, 'spline_order_lread': 5, 'n_splines_lwrite': 79, 'lam_lwrite': 99.54353658640554, 'spline_order_lwrite': 2, 'n_splines_scall': 77, 'lam_scall': 0.08640798397037414, 'spline_order_scall': 2, 'n_splines_sread': 27, 'lam_sread': 0.06448505085444195, 'spline_order_sread': 4, 'n_splines_swrite': 57, 'lam_swrite': 0.14221067050316152, 'spline_order_swrite': 2, 'n_splines_fork': 100, 'lam_fork': 27.985755780833884, 'spline_order_fork': 5, 'n_splines_exec': 20, 'lam_exec': 0.387199580792179, 'spline_order_exec': 3, 'n_splines_rchar': 81, 'lam_rchar': 0.02486726716588662, 'spline_order_rchar': 4, 'n_splines_wchar': 87, 'lam_wchar': 7.825446224592556, 'spline_order_wchar': 1, 'n_splines_pgout': 45, 'lam_pgout': 2.1823555921591407, 'spline_order_pgout': 3, 'n_splines_ppgout': 22, 'lam_ppgout': 9.863631873964408, 'spline_order_ppgout': 4, 'n_splines_pgfree': 57, 'lam_pgfree': 0.5475620880816912, 'spline_order_pgfree': 3, 'n_splines_pgscan': 100, 'lam_pgscan': 3.7674890259446525, 'spline_order_pgscan': 1, 'n_splines_atch': 42, 'lam_atch': 0.47063070941268936, 'spline_order_atch': 1, 'n_splines_pgin': 50, 'lam_pgin': 0.22165773953451665, 'spline_order_pgin': 5, 'n_splines_ppgin': 92, 'lam_ppgin': 0.023514616625141354, 'spline_order_ppgin': 1, 'n_splines_pflt': 35, 'lam_pflt': 0.001994540504440633, 'spline_order_pflt': 1, 'n_splines_vflt': 65, 'lam_vflt': 30.421340605844826, 'spline_order_vflt': 3, 'n_splines_runqsz': 30, 'lam_runqsz': 419.216539089371, 'spline_order_runqsz': 2, 'n_splines_freemem': 68, 'lam_freemem': 0.7311524335466405, 'spline_order_freemem': 3, 'n_splines_freeswap': 90, 'lam_freeswap': 0.14801376236243788, 'spline_order_freeswap': 2}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:35:13,219] Trial 46 finished with value: 28.075289690253392 and parameters: {'n_splines_lread': 61, 'lam_lread': 0.012109330934972124, 'spline_order_lread': 1, 'n_splines_lwrite': 71, 'lam_lwrite': 354.58107129931636, 'spline_order_lwrite': 1, 'n_splines_scall': 57, 'lam_scall': 3.72778803136958, 'spline_order_scall': 3, 'n_splines_sread': 15, 'lam_sread': 0.0029261247375419654, 'spline_order_sread': 3, 'n_splines_swrite': 23, 'lam_swrite': 0.2890010558522643, 'spline_order_swrite': 3, 'n_splines_fork': 74, 'lam_fork': 1.8598213604461997, 'spline_order_fork': 4, 'n_splines_exec': 39, 'lam_exec': 0.05347430505147638, 'spline_order_exec': 3, 'n_splines_rchar': 54, 'lam_rchar': 2.0662894461821235, 'spline_order_rchar': 5, 'n_splines_wchar': 79, 'lam_wchar': 0.09635895148390712, 'spline_order_wchar': 2, 'n_splines_pgout': 38, 'lam_pgout': 0.5582425013940976, 'spline_order_pgout': 4, 'n_splines_ppgout': 34, 'lam_ppgout': 0.02402398832314301, 'spline_order_ppgout': 5, 'n_splines_pgfree': 68, 'lam_pgfree': 0.10042245535262735, 'spline_order_pgfree': 4, 'n_splines_pgscan': 72, 'lam_pgscan': 1.1191756482884554, 'spline_order_pgscan': 3, 'n_splines_atch': 19, 'lam_atch': 12.987990801253043, 'spline_order_atch': 4, 'n_splines_pgin': 67, 'lam_pgin': 0.011489671405138808, 'spline_order_pgin': 4, 'n_splines_ppgin': 87, 'lam_ppgin': 456.2824041555846, 'spline_order_ppgin': 2, 'n_splines_pflt': 41, 'lam_pflt': 0.12362152674929412, 'spline_order_pflt': 2, 'n_splines_vflt': 73, 'lam_vflt': 0.002368880724132355, 'spline_order_vflt': 4, 'n_splines_runqsz': 19, 'lam_runqsz': 7.920346633003208, 'spline_order_runqsz': 1, 'n_splines_freemem': 73, 'lam_freemem': 0.10396138976535985, 'spline_order_freemem': 3, 'n_splines_freeswap': 96, 'lam_freeswap': 0.05177417051779894, 'spline_order_freeswap': 5}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:35:21,416] Trial 47 finished with value: 28.16133300944966 and parameters: {'n_splines_lread': 45, 'lam_lread': 0.003055340635831848, 'spline_order_lread': 3, 'n_splines_lwrite': 96, 'lam_lwrite': 0.02443454838622602, 'spline_order_lwrite': 1, 'n_splines_scall': 88, 'lam_scall': 1.4816392150122342, 'spline_order_scall': 1, 'n_splines_sread': 39, 'lam_sread': 1.2687460498676653, 'spline_order_sread': 1, 'n_splines_swrite': 34, 'lam_swrite': 1.1614828992672361, 'spline_order_swrite': 3, 'n_splines_fork': 34, 'lam_fork': 487.04235637827196, 'spline_order_fork': 3, 'n_splines_exec': 27, 'lam_exec': 1.3784335732479192, 'spline_order_exec': 4, 'n_splines_rchar': 28, 'lam_rchar': 118.4305134599497, 'spline_order_rchar': 5, 'n_splines_wchar': 95, 'lam_wchar': 0.41439443025197464, 'spline_order_wchar': 5, 'n_splines_pgout': 33, 'lam_pgout': 0.03321987153856488, 'spline_order_pgout': 5, 'n_splines_ppgout': 95, 'lam_ppgout': 0.09297554082707607, 'spline_order_ppgout': 4, 'n_splines_pgfree': 73, 'lam_pgfree': 0.3405689928777447, 'spline_order_pgfree': 2, 'n_splines_pgscan': 94, 'lam_pgscan': 36.83483465529538, 'spline_order_pgscan': 2, 'n_splines_atch': 33, 'lam_atch': 0.01061700320785157, 'spline_order_atch': 2, 'n_splines_pgin': 40, 'lam_pgin': 0.0901994550134932, 'spline_order_pgin': 1, 'n_splines_ppgin': 83, 'lam_ppgin': 12.76609143443387, 'spline_order_ppgin': 4, 'n_splines_pflt': 26, 'lam_pflt': 0.007247234120428032, 'spline_order_pflt': 1, 'n_splines_vflt': 80, 'lam_vflt': 0.4155999902291157, 'spline_order_vflt': 1, 'n_splines_runqsz': 13, 'lam_runqsz': 0.41010141877363204, 'spline_order_runqsz': 2, 'n_splines_freemem': 31, 'lam_freemem': 0.022111264526759218, 'spline_order_freemem': 4, 'n_splines_freeswap': 28, 'lam_freeswap': 0.012853249305226501, 'spline_order_freeswap': 3}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:35:32,764] Trial 48 finished with value: 101.10513460987269 and parameters: {'n_splines_lread': 23, 'lam_lread': 22.542668588133427, 'spline_order_lread': 4, 'n_splines_lwrite': 88, 'lam_lwrite': 173.34593155417878, 'spline_order_lwrite': 3, 'n_splines_scall': 44, 'lam_scall': 0.5794294157752656, 'spline_order_scall': 1, 'n_splines_sread': 32, 'lam_sread': 0.031052079283564717, 'spline_order_sread': 2, 'n_splines_swrite': 43, 'lam_swrite': 0.03621533435881299, 'spline_order_swrite': 4, 'n_splines_fork': 62, 'lam_fork': 7.429739962926879, 'spline_order_fork': 5, 'n_splines_exec': 100, 'lam_exec': 262.5577092010273, 'spline_order_exec': 2, 'n_splines_rchar': 73, 'lam_rchar': 0.597805707861788, 'spline_order_rchar': 1, 'n_splines_wchar': 37, 'lam_wchar': 0.002061591224991626, 'spline_order_wchar': 1, 'n_splines_pgout': 51, 'lam_pgout': 14.718260924386545, 'spline_order_pgout': 2, 'n_splines_ppgout': 58, 'lam_ppgout': 0.008556299040241536, 'spline_order_ppgout': 2, 'n_splines_pgfree': 83, 'lam_pgfree': 2.198895339204786, 'spline_order_pgfree': 3, 'n_splines_pgscan': 50, 'lam_pgscan': 17.424438371611316, 'spline_order_pgscan': 3, 'n_splines_atch': 66, 'lam_atch': 0.12750515224875994, 'spline_order_atch': 4, 'n_splines_pgin': 45, 'lam_pgin': 0.03775984715985276, 'spline_order_pgin': 3, 'n_splines_ppgin': 97, 'lam_ppgin': 5.694908192212081, 'spline_order_ppgin': 1, 'n_splines_pflt': 100, 'lam_pflt': 0.8747216023358567, 'spline_order_pflt': 2, 'n_splines_vflt': 68, 'lam_vflt': 3.4560663588589318, 'spline_order_vflt': 2, 'n_splines_runqsz': 53, 'lam_runqsz': 0.016183077574085, 'spline_order_runqsz': 3, 'n_splines_freemem': 85, 'lam_freemem': 268.3296126024493, 'spline_order_freemem': 5, 'n_splines_freeswap': 43, 'lam_freeswap': 0.5566705930211274, 'spline_order_freeswap': 2}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:35:40,971] Trial 49 finished with value: 27.116138570669726 and parameters: {'n_splines_lread': 40, 'lam_lread': 0.006680238970399513, 'spline_order_lread': 2, 'n_splines_lwrite': 43, 'lam_lwrite': 0.1549074544999401, 'spline_order_lwrite': 4, 'n_splines_scall': 36, 'lam_scall': 0.2447537027218971, 'spline_order_scall': 2, 'n_splines_sread': 71, 'lam_sread': 0.10940967663052993, 'spline_order_sread': 3, 'n_splines_swrite': 48, 'lam_swrite': 0.01565783108402067, 'spline_order_swrite': 5, 'n_splines_fork': 71, 'lam_fork': 194.87140594433805, 'spline_order_fork': 4, 'n_splines_exec': 14, 'lam_exec': 0.02274835319405702, 'spline_order_exec': 3, 'n_splines_rchar': 60, 'lam_rchar': 0.10162006402759266, 'spline_order_rchar': 4, 'n_splines_wchar': 73, 'lam_wchar': 4.935684584168889, 'spline_order_wchar': 3, 'n_splines_pgout': 88, 'lam_pgout': 0.26014797973652987, 'spline_order_pgout': 4, 'n_splines_ppgout': 82, 'lam_ppgout': 0.32234802942632107, 'spline_order_ppgout': 3, 'n_splines_pgfree': 51, 'lam_pgfree': 0.7553193311338473, 'spline_order_pgfree': 3, 'n_splines_pgscan': 89, 'lam_pgscan': 0.04814205957167543, 'spline_order_pgscan': 1, 'n_splines_atch': 71, 'lam_atch': 1.0594500191667695, 'spline_order_atch': 1, 'n_splines_pgin': 58, 'lam_pgin': 0.6233679619702068, 'spline_order_pgin': 5, 'n_splines_ppgin': 32, 'lam_ppgin': 216.66528022877978, 'spline_order_ppgin': 3, 'n_splines_pflt': 17, 'lam_pflt': 2.4581157008562036, 'spline_order_pflt': 1, 'n_splines_vflt': 43, 'lam_vflt': 1.3598223122219597, 'spline_order_vflt': 5, 'n_splines_runqsz': 45, 'lam_runqsz': 199.80749873141426, 'spline_order_runqsz': 4, 'n_splines_freemem': 20, 'lam_freemem': 11.988786167352934, 'spline_order_freemem': 2, 'n_splines_freeswap': 81, 'lam_freeswap': 946.6670400308468, 'spline_order_freeswap': 4}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:35:48,699] Trial 50 finished with value: 192.9071381415003 and parameters: {'n_splines_lread': 67, 'lam_lread': 0.0013884509209225431, 'spline_order_lread': 5, 'n_splines_lwrite': 66, 'lam_lwrite': 1.1093473618010345, 'spline_order_lwrite': 2, 'n_splines_scall': 52, 'lam_scall': 0.12602463326401975, 'spline_order_scall': 4, 'n_splines_sread': 22, 'lam_sread': 0.24373830490388318, 'spline_order_sread': 4, 'n_splines_swrite': 25, 'lam_swrite': 0.17027698700394264, 'spline_order_swrite': 4, 'n_splines_fork': 67, 'lam_fork': 15.892293296972033, 'spline_order_fork': 3, 'n_splines_exec': 87, 'lam_exec': 977.2383974813349, 'spline_order_exec': 2, 'n_splines_rchar': 100, 'lam_rchar': 0.0022224730971407954, 'spline_order_rchar': 3, 'n_splines_wchar': 22, 'lam_wchar': 1.7752716736065373, 'spline_order_wchar': 2, 'n_splines_pgout': 15, 'lam_pgout': 42.5730480391473, 'spline_order_pgout': 3, 'n_splines_ppgout': 77, 'lam_ppgout': 449.4945657148339, 'spline_order_ppgout': 2, 'n_splines_pgfree': 62, 'lam_pgfree': 8.183007428684787, 'spline_order_pgfree': 2, 'n_splines_pgscan': 19, 'lam_pgscan': 280.0038950193843, 'spline_order_pgscan': 4, 'n_splines_atch': 47, 'lam_atch': 2.527324220394291, 'spline_order_atch': 2, 'n_splines_pgin': 30, 'lam_pgin': 0.01594917790061077, 'spline_order_pgin': 2, 'n_splines_ppgin': 60, 'lam_ppgin': 1.709946049811779, 'spline_order_ppgin': 5, 'n_splines_pflt': 33, 'lam_pflt': 0.011481257339645362, 'spline_order_pflt': 1, 'n_splines_vflt': 60, 'lam_vflt': 14.944844443767995, 'spline_order_vflt': 2, 'n_splines_runqsz': 27, 'lam_runqsz': 0.0016306291964971627, 'spline_order_runqsz': 1, 'n_splines_freemem': 64, 'lam_freemem': 1.2796727374542198, 'spline_order_freemem': 4, 'n_splines_freeswap': 88, 'lam_freeswap': 0.0023222866651335184, 'spline_order_freeswap': 3}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:35:58,881] Trial 51 finished with value: 3.5429407208815324 and parameters: {'n_splines_lread': 34, 'lam_lread': 0.00924543707046654, 'spline_order_lread': 5, 'n_splines_lwrite': 75, 'lam_lwrite': 47.12415860699348, 'spline_order_lwrite': 1, 'n_splines_scall': 63, 'lam_scall': 0.7985209683935666, 'spline_order_scall': 1, 'n_splines_sread': 27, 'lam_sread': 0.3506485669462383, 'spline_order_sread': 3, 'n_splines_swrite': 42, 'lam_swrite': 0.05335172177470078, 'spline_order_swrite': 3, 'n_splines_fork': 78, 'lam_fork': 134.80144753575672, 'spline_order_fork': 4, 'n_splines_exec': 12, 'lam_exec': 0.2864673923899098, 'spline_order_exec': 3, 'n_splines_rchar': 70, 'lam_rchar': 81.68392127366909, 'spline_order_rchar': 4, 'n_splines_wchar': 79, 'lam_wchar': 0.1750827401839234, 'spline_order_wchar': 1, 'n_splines_pgout': 41, 'lam_pgout': 5.7404586791714065, 'spline_order_pgout': 3, 'n_splines_ppgout': 29, 'lam_ppgout': 0.005591531898178261, 'spline_order_ppgout': 3, 'n_splines_pgfree': 64, 'lam_pgfree': 1.0992425290072363, 'spline_order_pgfree': 3, 'n_splines_pgscan': 96, 'lam_pgscan': 2.1755469191963317, 'spline_order_pgscan': 2, 'n_splines_atch': 58, 'lam_atch': 0.25575792986836876, 'spline_order_atch': 2, 'n_splines_pgin': 52, 'lam_pgin': 0.033001610719506656, 'spline_order_pgin': 5, 'n_splines_ppgin': 74, 'lam_ppgin': 649.9294216495421, 'spline_order_ppgin': 1, 'n_splines_pflt': 48, 'lam_pflt': 0.1829279834487934, 'spline_order_pflt': 1, 'n_splines_vflt': 75, 'lam_vflt': 1.8326797615992976, 'spline_order_vflt': 3, 'n_splines_runqsz': 13, 'lam_runqsz': 67.41714712772665, 'spline_order_runqsz': 2, 'n_splines_freemem': 68, 'lam_freemem': 0.2484992533693368, 'spline_order_freemem': 5, 'n_splines_freeswap': 100, 'lam_freeswap': 0.06670204540851433, 'spline_order_freeswap': 3}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:36:09,060] Trial 52 finished with value: 4.47836833282971 and parameters: {'n_splines_lread': 31, 'lam_lread': 0.021753901745507304, 'spline_order_lread': 5, 'n_splines_lwrite': 74, 'lam_lwrite': 24.019781266091513, 'spline_order_lwrite': 1, 'n_splines_scall': 65, 'lam_scall': 0.9686776819550436, 'spline_order_scall': 1, 'n_splines_sread': 97, 'lam_sread': 0.5806358290060037, 'spline_order_sread': 3, 'n_splines_swrite': 37, 'lam_swrite': 0.0659602253519059, 'spline_order_swrite': 3, 'n_splines_fork': 83, 'lam_fork': 138.39719828271578, 'spline_order_fork': 4, 'n_splines_exec': 19, 'lam_exec': 0.1327416231717884, 'spline_order_exec': 3, 'n_splines_rchar': 68, 'lam_rchar': 56.47109625611201, 'spline_order_rchar': 4, 'n_splines_wchar': 63, 'lam_wchar': 0.14370258303391872, 'spline_order_wchar': 1, 'n_splines_pgout': 40, 'lam_pgout': 5.252811462441229, 'spline_order_pgout': 3, 'n_splines_ppgout': 27, 'lam_ppgout': 0.01380577500161169, 'spline_order_ppgout': 3, 'n_splines_pgfree': 70, 'lam_pgfree': 2.309123440439367, 'spline_order_pgfree': 3, 'n_splines_pgscan': 96, 'lam_pgscan': 2.791675777655825, 'spline_order_pgscan': 2, 'n_splines_atch': 59, 'lam_atch': 0.2079497453680229, 'spline_order_atch': 2, 'n_splines_pgin': 54, 'lam_pgin': 0.11142084931534164, 'spline_order_pgin': 5, 'n_splines_ppgin': 68, 'lam_ppgin': 524.8045824769403, 'spline_order_ppgin': 1, 'n_splines_pflt': 54, 'lam_pflt': 0.44655968091062764, 'spline_order_pflt': 1, 'n_splines_vflt': 53, 'lam_vflt': 0.31615465550268995, 'spline_order_vflt': 3, 'n_splines_runqsz': 14, 'lam_runqsz': 602.0304996178307, 'spline_order_runqsz': 2, 'n_splines_freemem': 76, 'lam_freemem': 0.34388679798982924, 'spline_order_freemem': 5, 'n_splines_freeswap': 98, 'lam_freeswap': 0.20229333446001102, 'spline_order_freeswap': 3}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:36:18,959] Trial 53 finished with value: 13.909261403739881 and parameters: {'n_splines_lread': 36, 'lam_lread': 0.03970935195601377, 'spline_order_lread': 5, 'n_splines_lwrite': 61, 'lam_lwrite': 75.18512406359899, 'spline_order_lwrite': 5, 'n_splines_scall': 75, 'lam_scall': 0.05894286569709234, 'spline_order_scall': 1, 'n_splines_sread': 29, 'lam_sread': 0.0424083072155046, 'spline_order_sread': 2, 'n_splines_swrite': 48, 'lam_swrite': 0.008780216945841317, 'spline_order_swrite': 3, 'n_splines_fork': 95, 'lam_fork': 96.56052535543502, 'spline_order_fork': 4, 'n_splines_exec': 12, 'lam_exec': 0.21912642129297238, 'spline_order_exec': 3, 'n_splines_rchar': 78, 'lam_rchar': 234.1509697617017, 'spline_order_rchar': 3, 'n_splines_wchar': 68, 'lam_wchar': 0.05903543518822721, 'spline_order_wchar': 1, 'n_splines_pgout': 27, 'lam_pgout': 1.2914722754189998, 'spline_order_pgout': 3, 'n_splines_ppgout': 37, 'lam_ppgout': 0.0018867394658081825, 'spline_order_ppgout': 3, 'n_splines_pgfree': 66, 'lam_pgfree': 0.1437473187935372, 'spline_order_pgfree': 3, 'n_splines_pgscan': 90, 'lam_pgscan': 0.7124914201310325, 'spline_order_pgscan': 2, 'n_splines_atch': 55, 'lam_atch': 0.09820098673169364, 'spline_order_atch': 2, 'n_splines_pgin': 51, 'lam_pgin': 0.006684775401656892, 'spline_order_pgin': 5, 'n_splines_ppgin': 75, 'lam_ppgin': 95.32605443015139, 'spline_order_ppgin': 1, 'n_splines_pflt': 46, 'lam_pflt': 0.22033934157555254, 'spline_order_pflt': 1, 'n_splines_vflt': 70, 'lam_vflt': 0.79201001254903, 'spline_order_vflt': 3, 'n_splines_runqsz': 21, 'lam_runqsz': 76.85780876924595, 'spline_order_runqsz': 2, 'n_splines_freemem': 71, 'lam_freemem': 0.0578254233652664, 'spline_order_freemem': 5, 'n_splines_freeswap': 100, 'lam_freeswap': 0.05939037659693705, 'spline_order_freeswap': 3}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:36:30,380] Trial 54 finished with value: 61.266006663625845 and parameters: {'n_splines_lread': 26, 'lam_lread': 2.7273605077556526, 'spline_order_lread': 5, 'n_splines_lwrite': 83, 'lam_lwrite': 0.5124795459834701, 'spline_order_lwrite': 1, 'n_splines_scall': 69, 'lam_scall': 1.887301692590171, 'spline_order_scall': 2, 'n_splines_sread': 25, 'lam_sread': 0.09525889084487328, 'spline_order_sread': 3, 'n_splines_swrite': 42, 'lam_swrite': 0.004953949858157107, 'spline_order_swrite': 3, 'n_splines_fork': 91, 'lam_fork': 32.154119291462685, 'spline_order_fork': 4, 'n_splines_exec': 24, 'lam_exec': 3.2556871672331904, 'spline_order_exec': 3, 'n_splines_rchar': 65, 'lam_rchar': 529.3973268831759, 'spline_order_rchar': 4, 'n_splines_wchar': 79, 'lam_wchar': 0.2665522620646948, 'spline_order_wchar': 1, 'n_splines_pgout': 44, 'lam_pgout': 2.679087079456597, 'spline_order_pgout': 2, 'n_splines_ppgout': 87, 'lam_ppgout': 0.0029142891989058315, 'spline_order_ppgout': 3, 'n_splines_pgfree': 59, 'lam_pgfree': 0.32313517230160843, 'spline_order_pgfree': 3, 'n_splines_pgscan': 84, 'lam_pgscan': 0.3349960551268792, 'spline_order_pgscan': 2, 'n_splines_atch': 50, 'lam_atch': 0.37706306757228714, 'spline_order_atch': 3, 'n_splines_pgin': 43, 'lam_pgin': 0.03717936337960781, 'spline_order_pgin': 5, 'n_splines_ppgin': 81, 'lam_ppgin': 24.907089895670655, 'spline_order_ppgin': 1, 'n_splines_pflt': 61, 'lam_pflt': 0.04352242612912246, 'spline_order_pflt': 1, 'n_splines_vflt': 79, 'lam_vflt': 9.824977888591444, 'spline_order_vflt': 2, 'n_splines_runqsz': 75, 'lam_runqsz': 19.697801718621033, 'spline_order_runqsz': 2, 'n_splines_freemem': 53, 'lam_freemem': 0.010336229893641254, 'spline_order_freemem': 5, 'n_splines_freeswap': 89, 'lam_freeswap': 0.021701565825392455, 'spline_order_freeswap': 2}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:36:42,881] Trial 55 finished with value: 6.663940546882398 and parameters: {'n_splines_lread': 47, 'lam_lread': 0.009154118428450634, 'spline_order_lread': 5, 'n_splines_lwrite': 86, 'lam_lwrite': 10.570218051723087, 'spline_order_lwrite': 1, 'n_splines_scall': 60, 'lam_scall': 6.265306001291361, 'spline_order_scall': 1, 'n_splines_sread': 36, 'lam_sread': 0.23677913124130326, 'spline_order_sread': 4, 'n_splines_swrite': 61, 'lam_swrite': 0.026869383085240277, 'spline_order_swrite': 1, 'n_splines_fork': 80, 'lam_fork': 972.3007744343408, 'spline_order_fork': 5, 'n_splines_exec': 32, 'lam_exec': 0.4704161835461201, 'spline_order_exec': 2, 'n_splines_rchar': 81, 'lam_rchar': 23.74679239141763, 'spline_order_rchar': 4, 'n_splines_wchar': 88, 'lam_wchar': 0.019637781703509067, 'spline_order_wchar': 1, 'n_splines_pgout': 52, 'lam_pgout': 0.16822150840737782, 'spline_order_pgout': 4, 'n_splines_ppgout': 24, 'lam_ppgout': 0.005863947709105979, 'spline_order_ppgout': 4, 'n_splines_pgfree': 76, 'lam_pgfree': 1.6615529663613318, 'spline_order_pgfree': 3, 'n_splines_pgscan': 96, 'lam_pgscan': 4.513595471198017, 'spline_order_pgscan': 2, 'n_splines_atch': 64, 'lam_atch': 0.0673906236606049, 'spline_order_atch': 2, 'n_splines_pgin': 61, 'lam_pgin': 0.22682763883331922, 'spline_order_pgin': 5, 'n_splines_ppgin': 100, 'lam_ppgin': 233.16070133810587, 'spline_order_ppgin': 3, 'n_splines_pflt': 44, 'lam_pflt': 7.1020481005616105, 'spline_order_pflt': 2, 'n_splines_vflt': 100, 'lam_vflt': 0.057130801955704316, 'spline_order_vflt': 4, 'n_splines_runqsz': 17, 'lam_runqsz': 703.2063479692521, 'spline_order_runqsz': 2, 'n_splines_freemem': 66, 'lam_freemem': 2.3756452578174354, 'spline_order_freemem': 5, 'n_splines_freeswap': 95, 'lam_freeswap': 0.009524590018404494, 'spline_order_freeswap': 4}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:36:51,494] Trial 56 finished with value: 14.772849824413413 and parameters: {'n_splines_lread': 15, 'lam_lread': 0.003917156644271269, 'spline_order_lread': 3, 'n_splines_lwrite': 67, 'lam_lwrite': 630.0921837188539, 'spline_order_lwrite': 1, 'n_splines_scall': 47, 'lam_scall': 0.4877235167672557, 'spline_order_scall': 1, 'n_splines_sread': 31, 'lam_sread': 0.006425789290066652, 'spline_order_sread': 2, 'n_splines_swrite': 32, 'lam_swrite': 0.06463977187376083, 'spline_order_swrite': 2, 'n_splines_fork': 86, 'lam_fork': 54.440485718813115, 'spline_order_fork': 4, 'n_splines_exec': 13, 'lam_exec': 0.073071770554136, 'spline_order_exec': 4, 'n_splines_rchar': 70, 'lam_rchar': 0.33324175932441624, 'spline_order_rchar': 5, 'n_splines_wchar': 93, 'lam_wchar': 1.1734031244528946, 'spline_order_wchar': 1, 'n_splines_pgout': 70, 'lam_pgout': 28.188223975319893, 'spline_order_pgout': 3, 'n_splines_ppgout': 32, 'lam_ppgout': 0.041795667538534996, 'spline_order_ppgout': 3, 'n_splines_pgfree': 54, 'lam_pgfree': 0.8096274094750799, 'spline_order_pgfree': 4, 'n_splines_pgscan': 81, 'lam_pgscan': 459.3978121991389, 'spline_order_pgscan': 3, 'n_splines_atch': 40, 'lam_atch': 0.8952767895617592, 'spline_order_atch': 5, 'n_splines_pgin': 39, 'lam_pgin': 0.003848916877911476, 'spline_order_pgin': 4, 'n_splines_ppgin': 89, 'lam_ppgin': 53.102219668831566, 'spline_order_ppgin': 1, 'n_splines_pflt': 50, 'lam_pflt': 0.17488426677467955, 'spline_order_pflt': 3, 'n_splines_vflt': 96, 'lam_vflt': 64.7944755026532, 'spline_order_vflt': 3, 'n_splines_runqsz': 13, 'lam_runqsz': 3.892616849296311, 'spline_order_runqsz': 3, 'n_splines_freemem': 59, 'lam_freemem': 0.17347803191151753, 'spline_order_freemem': 3, 'n_splines_freeswap': 84, 'lam_freeswap': 0.3629838420764094, 'spline_order_freeswap': 3}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:37:08,642] Trial 57 finished with value: 25.63142150688137 and parameters: {'n_splines_lread': 29, 'lam_lread': 0.14938302401141, 'spline_order_lread': 1, 'n_splines_lwrite': 77, 'lam_lwrite': 157.46322658684065, 'spline_order_lwrite': 2, 'n_splines_scall': 57, 'lam_scall': 2.767223306623368, 'spline_order_scall': 1, 'n_splines_sread': 18, 'lam_sread': 1.578841750333818, 'spline_order_sread': 3, 'n_splines_swrite': 54, 'lam_swrite': 0.44731791723957987, 'spline_order_swrite': 3, 'n_splines_fork': 73, 'lam_fork': 339.56346658351157, 'spline_order_fork': 4, 'n_splines_exec': 61, 'lam_exec': 0.818122798779234, 'spline_order_exec': 3, 'n_splines_rchar': 51, 'lam_rchar': 0.03288877470037339, 'spline_order_rchar': 3, 'n_splines_wchar': 100, 'lam_wchar': 0.5599691048295875, 'spline_order_wchar': 2, 'n_splines_pgout': 35, 'lam_pgout': 4.051188592203153, 'spline_order_pgout': 2, 'n_splines_ppgout': 17, 'lam_ppgout': 0.01082381497639165, 'spline_order_ppgout': 2, 'n_splines_pgfree': 64, 'lam_pgfree': 4.084443845791769, 'spline_order_pgfree': 2, 'n_splines_pgscan': 100, 'lam_pgscan': 1.098469498766807, 'spline_order_pgscan': 2, 'n_splines_atch': 78, 'lam_atch': 0.2210542404048112, 'spline_order_atch': 3, 'n_splines_pgin': 20, 'lam_pgin': 52.096532468484995, 'spline_order_pgin': 5, 'n_splines_ppgin': 78, 'lam_ppgin': 0.0014876063098076089, 'spline_order_ppgin': 1, 'n_splines_pflt': 92, 'lam_pflt': 1.383619022903694, 'spline_order_pflt': 1, 'n_splines_vflt': 74, 'lam_vflt': 1.5875461450047734, 'spline_order_vflt': 4, 'n_splines_runqsz': 31, 'lam_runqsz': 34.75068738903606, 'spline_order_runqsz': 2, 'n_splines_freemem': 81, 'lam_freemem': 0.6155541219827669, 'spline_order_freemem': 5, 'n_splines_freeswap': 97, 'lam_freeswap': 1.2053245146712803, 'spline_order_freeswap': 2}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:37:20,054] Trial 58 finished with value: 10.768050856049708 and parameters: {'n_splines_lread': 74, 'lam_lread': 0.001635084568587713, 'spline_order_lread': 2, 'n_splines_lwrite': 55, 'lam_lwrite': 37.47976875032022, 'spline_order_lwrite': 1, 'n_splines_scall': 63, 'lam_scall': 12.097613326267554, 'spline_order_scall': 2, 'n_splines_sread': 52, 'lam_sread': 18.54277808792635, 'spline_order_sread': 4, 'n_splines_swrite': 52, 'lam_swrite': 0.09868652361796397, 'spline_order_swrite': 2, 'n_splines_fork': 97, 'lam_fork': 0.6435673387154277, 'spline_order_fork': 2, 'n_splines_exec': 19, 'lam_exec': 53.8246532046974, 'spline_order_exec': 5, 'n_splines_rchar': 37, 'lam_rchar': 200.0802090101472, 'spline_order_rchar': 4, 'n_splines_wchar': 80, 'lam_wchar': 0.3317868763068254, 'spline_order_wchar': 1, 'n_splines_pgout': 77, 'lam_pgout': 0.4929194114086685, 'spline_order_pgout': 3, 'n_splines_ppgout': 69, 'lam_ppgout': 202.97301936992312, 'spline_order_ppgout': 4, 'n_splines_pgfree': 71, 'lam_pgfree': 0.0605328009622969, 'spline_order_pgfree': 3, 'n_splines_pgscan': 94, 'lam_pgscan': 13.272648658100369, 'spline_order_pgscan': 2, 'n_splines_atch': 45, 'lam_atch': 5.009633736582371, 'spline_order_atch': 1, 'n_splines_pgin': 55, 'lam_pgin': 0.02092938472820889, 'spline_order_pgin': 2, 'n_splines_ppgin': 84, 'lam_ppgin': 614.9997933257, 'spline_order_ppgin': 2, 'n_splines_pflt': 64, 'lam_pflt': 0.016165954236025102, 'spline_order_pflt': 1, 'n_splines_vflt': 82, 'lam_vflt': 3.1472787443616075, 'spline_order_vflt': 3, 'n_splines_runqsz': 24, 'lam_runqsz': 172.5879277387331, 'spline_order_runqsz': 1, 'n_splines_freemem': 38, 'lam_freemem': 0.030370144148686364, 'spline_order_freemem': 4, 'n_splines_freeswap': 92, 'lam_freeswap': 2.516337254384739, 'spline_order_freeswap': 3}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:37:29,306] Trial 59 finished with value: 29.453532201457243 and parameters: {'n_splines_lread': 36, 'lam_lread': 0.07997859412092986, 'spline_order_lread': 4, 'n_splines_lwrite': 89, 'lam_lwrite': 44.61545251307013, 'spline_order_lwrite': 1, 'n_splines_scall': 81, 'lam_scall': 0.14127816767293616, 'spline_order_scall': 2, 'n_splines_sread': 42, 'lam_sread': 0.01594307791137878, 'spline_order_sread': 4, 'n_splines_swrite': 99, 'lam_swrite': 0.002935939465599892, 'spline_order_swrite': 4, 'n_splines_fork': 91, 'lam_fork': 3.2672977677934307, 'spline_order_fork': 4, 'n_splines_exec': 29, 'lam_exec': 0.033836650485953475, 'spline_order_exec': 3, 'n_splines_rchar': 57, 'lam_rchar': 107.1887415125871, 'spline_order_rchar': 2, 'n_splines_wchar': 52, 'lam_wchar': 0.036618913996290275, 'spline_order_wchar': 2, 'n_splines_pgout': 23, 'lam_pgout': 0.07130065340544273, 'spline_order_pgout': 5, 'n_splines_ppgout': 43, 'lam_ppgout': 23.706848098094895, 'spline_order_ppgout': 3, 'n_splines_pgfree': 34, 'lam_pgfree': 9.079643872765063, 'spline_order_pgfree': 4, 'n_splines_pgscan': 61, 'lam_pgscan': 106.16741135897668, 'spline_order_pgscan': 3, 'n_splines_atch': 54, 'lam_atch': 121.46530153517631, 'spline_order_atch': 2, 'n_splines_pgin': 47, 'lam_pgin': 0.008447188619807231, 'spline_order_pgin': 4, 'n_splines_ppgin': 72, 'lam_ppgin': 0.45742119440064577, 'spline_order_ppgin': 1, 'n_splines_pflt': 69, 'lam_pflt': 0.0743818922746039, 'spline_order_pflt': 2, 'n_splines_vflt': 61, 'lam_vflt': 6.095501310290305, 'spline_order_vflt': 2, 'n_splines_runqsz': 10, 'lam_runqsz': 277.69671688208507, 'spline_order_runqsz': 3, 'n_splines_freemem': 72, 'lam_freemem': 0.07699622575617526, 'spline_order_freemem': 2, 'n_splines_freeswap': 86, 'lam_freeswap': 0.04281871890648318, 'spline_order_freeswap': 1}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:37:39,669] Trial 60 finished with value: 52.07152246582313 and parameters: {'n_splines_lread': 57, 'lam_lread': 0.0031123087870626146, 'spline_order_lread': 5, 'n_splines_lwrite': 100, 'lam_lwrite': 220.5998681348056, 'spline_order_lwrite': 2, 'n_splines_scall': 70, 'lam_scall': 0.783377443694785, 'spline_order_scall': 1, 'n_splines_sread': 13, 'lam_sread': 0.37895424851657167, 'spline_order_sread': 2, 'n_splines_swrite': 35, 'lam_swrite': 0.011708247651180803, 'spline_order_swrite': 3, 'n_splines_fork': 59, 'lam_fork': 22.22197891396176, 'spline_order_fork': 3, 'n_splines_exec': 19, 'lam_exec': 0.011397599233174904, 'spline_order_exec': 2, 'n_splines_rchar': 63, 'lam_rchar': 43.05699167144907, 'spline_order_rchar': 5, 'n_splines_wchar': 47, 'lam_wchar': 2.62841208438735, 'spline_order_wchar': 3, 'n_splines_pgout': 31, 'lam_pgout': 9.280069318254345, 'spline_order_pgout': 1, 'n_splines_ppgout': 13, 'lam_ppgout': 987.2296337548233, 'spline_order_ppgout': 3, 'n_splines_pgfree': 60, 'lam_pgfree': 50.9828597028192, 'spline_order_pgfree': 3, 'n_splines_pgscan': 86, 'lam_pgscan': 0.14021684713258034, 'spline_order_pgscan': 2, 'n_splines_atch': 70, 'lam_atch': 1.5622880923584839, 'spline_order_atch': 4, 'n_splines_pgin': 58, 'lam_pgin': 0.9119890139110552, 'spline_order_pgin': 1, 'n_splines_ppgin': 92, 'lam_ppgin': 378.74591788624036, 'spline_order_ppgin': 2, 'n_splines_pflt': 29, 'lam_pflt': 0.48615843299585326, 'spline_order_pflt': 5, 'n_splines_vflt': 89, 'lam_vflt': 0.17786946266693404, 'spline_order_vflt': 4, 'n_splines_runqsz': 98, 'lam_runqsz': 60.39703238052404, 'spline_order_runqsz': 2, 'n_splines_freemem': 42, 'lam_freemem': 6.15759061312644, 'spline_order_freemem': 5, 'n_splines_freeswap': 69, 'lam_freeswap': 4.934429099521052, 'spline_order_freeswap': 2}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:37:49,387] Trial 61 finished with value: 14.195773383123363 and parameters: {'n_splines_lread': 35, 'lam_lread': 0.007456733487804497, 'spline_order_lread': 5, 'n_splines_lwrite': 74, 'lam_lwrite': 88.78136134121041, 'spline_order_lwrite': 1, 'n_splines_scall': 61, 'lam_scall': 0.3813039711876613, 'spline_order_scall': 1, 'n_splines_sread': 28, 'lam_sread': 0.08371136462641209, 'spline_order_sread': 3, 'n_splines_swrite': 44, 'lam_swrite': 0.03750163156831748, 'spline_order_swrite': 3, 'n_splines_fork': 77, 'lam_fork': 82.33347103658292, 'spline_order_fork': 4, 'n_splines_exec': 14, 'lam_exec': 0.2944465829292652, 'spline_order_exec': 3, 'n_splines_rchar': 72, 'lam_rchar': 77.43080519798266, 'spline_order_rchar': 4, 'n_splines_wchar': 86, 'lam_wchar': 0.13716256324041787, 'spline_order_wchar': 1, 'n_splines_pgout': 42, 'lam_pgout': 3.2837215240209923, 'spline_order_pgout': 3, 'n_splines_ppgout': 27, 'lam_ppgout': 0.006891267568764301, 'spline_order_ppgout': 3, 'n_splines_pgfree': 63, 'lam_pgfree': 0.8938171725655472, 'spline_order_pgfree': 3, 'n_splines_pgscan': 92, 'lam_pgscan': 2.315405019441271, 'spline_order_pgscan': 2, 'n_splines_atch': 57, 'lam_atch': 0.37419341398229006, 'spline_order_atch': 2, 'n_splines_pgin': 52, 'lam_pgin': 0.02096137754366751, 'spline_order_pgin': 5, 'n_splines_ppgin': 79, 'lam_ppgin': 963.6615616983298, 'spline_order_ppgin': 1, 'n_splines_pflt': 48, 'lam_pflt': 0.16368767999451092, 'spline_order_pflt': 1, 'n_splines_vflt': 72, 'lam_vflt': 2.550956661169281, 'spline_order_vflt': 3, 'n_splines_runqsz': 18, 'lam_runqsz': 27.579957957427023, 'spline_order_runqsz': 2, 'n_splines_freemem': 70, 'lam_freemem': 0.2600047147844778, 'spline_order_freemem': 5, 'n_splines_freeswap': 100, 'lam_freeswap': 0.09284087011950225, 'spline_order_freeswap': 3}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:37:58,298] Trial 62 finished with value: 6.305175603940569 and parameters: {'n_splines_lread': 42, 'lam_lread': 0.010894773497234932, 'spline_order_lread': 5, 'n_splines_lwrite': 76, 'lam_lwrite': 19.952365212378247, 'spline_order_lwrite': 1, 'n_splines_scall': 54, 'lam_scall': 0.5983368303896139, 'spline_order_scall': 1, 'n_splines_sread': 22, 'lam_sread': 0.4118849235824201, 'spline_order_sread': 3, 'n_splines_swrite': 46, 'lam_swrite': 0.05023436657446324, 'spline_order_swrite': 3, 'n_splines_fork': 69, 'lam_fork': 161.56388657266658, 'spline_order_fork': 4, 'n_splines_exec': 10, 'lam_exec': 0.08246371626314195, 'spline_order_exec': 3, 'n_splines_rchar': 70, 'lam_rchar': 97.7133152061967, 'spline_order_rchar': 4, 'n_splines_wchar': 82, 'lam_wchar': 0.6979679378392097, 'spline_order_wchar': 1, 'n_splines_pgout': 48, 'lam_pgout': 0.9686532516850989, 'spline_order_pgout': 3, 'n_splines_ppgout': 29, 'lam_ppgout': 0.003463386121444077, 'spline_order_ppgout': 2, 'n_splines_pgfree': 80, 'lam_pgfree': 0.3344446678669992, 'spline_order_pgfree': 3, 'n_splines_pgscan': 69, 'lam_pgscan': 1.806398430582639, 'spline_order_pgscan': 2, 'n_splines_atch': 63, 'lam_atch': 0.6573227137643227, 'spline_order_atch': 2, 'n_splines_pgin': 52, 'lam_pgin': 0.04298000020760335, 'spline_order_pgin': 5, 'n_splines_ppgin': 86, 'lam_ppgin': 657.3056449054606, 'spline_order_ppgin': 1, 'n_splines_pflt': 43, 'lam_pflt': 0.27537249739126324, 'spline_order_pflt': 1, 'n_splines_vflt': 66, 'lam_vflt': 0.7895178422478321, 'spline_order_vflt': 3, 'n_splines_runqsz': 10, 'lam_runqsz': 47.890748850739094, 'spline_order_runqsz': 2, 'n_splines_freemem': 78, 'lam_freemem': 0.1729929494571715, 'spline_order_freemem': 5, 'n_splines_freeswap': 93, 'lam_freeswap': 0.13525634986980595, 'spline_order_freeswap': 3}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:38:07,821] Trial 63 finished with value: 6.885866461462627 and parameters: {'n_splines_lread': 22, 'lam_lread': 0.005429918916106059, 'spline_order_lread': 5, 'n_splines_lwrite': 81, 'lam_lwrite': 54.437664400878155, 'spline_order_lwrite': 1, 'n_splines_scall': 74, 'lam_scall': 1.2555679508634974, 'spline_order_scall': 1, 'n_splines_sread': 26, 'lam_sread': 0.17056687030432494, 'spline_order_sread': 3, 'n_splines_swrite': 40, 'lam_swrite': 0.16285464603725258, 'spline_order_swrite': 3, 'n_splines_fork': 77, 'lam_fork': 102.95506742915887, 'spline_order_fork': 4, 'n_splines_exec': 13, 'lam_exec': 0.00131598584031566, 'spline_order_exec': 3, 'n_splines_rchar': 75, 'lam_rchar': 347.75085047481923, 'spline_order_rchar': 3, 'n_splines_wchar': 76, 'lam_wchar': 0.05853628462461922, 'spline_order_wchar': 1, 'n_splines_pgout': 37, 'lam_pgout': 5.617653864970339, 'spline_order_pgout': 3, 'n_splines_ppgout': 23, 'lam_ppgout': 0.02432180975505355, 'spline_order_ppgout': 3, 'n_splines_pgfree': 68, 'lam_pgfree': 1.1119635664386742, 'spline_order_pgfree': 3, 'n_splines_pgscan': 89, 'lam_pgscan': 9.121918182771548, 'spline_order_pgscan': 2, 'n_splines_atch': 59, 'lam_atch': 0.051751635691190546, 'spline_order_atch': 2, 'n_splines_pgin': 63, 'lam_pgin': 0.02696208659143488, 'spline_order_pgin': 5, 'n_splines_ppgin': 76, 'lam_ppgin': 187.76731832914442, 'spline_order_ppgin': 1, 'n_splines_pflt': 39, 'lam_pflt': 0.7611584378915138, 'spline_order_pflt': 1, 'n_splines_vflt': 57, 'lam_vflt': 4.2827895707920005, 'spline_order_vflt': 3, 'n_splines_runqsz': 13, 'lam_runqsz': 614.356823868149, 'spline_order_runqsz': 2, 'n_splines_freemem': 64, 'lam_freemem': 0.04196078668857404, 'spline_order_freemem': 5, 'n_splines_freeswap': 97, 'lam_freeswap': 0.027745191590556634, 'spline_order_freeswap': 3}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:38:20,192] Trial 64 finished with value: 27.843942790706034 and parameters: {'n_splines_lread': 91, 'lam_lread': 0.0298894980212778, 'spline_order_lread': 5, 'n_splines_lwrite': 69, 'lam_lwrite': 504.6117860717339, 'spline_order_lwrite': 1, 'n_splines_scall': 65, 'lam_scall': 0.2597204091078677, 'spline_order_scall': 4, 'n_splines_sread': 30, 'lam_sread': 80.33243780168769, 'spline_order_sread': 3, 'n_splines_swrite': 38, 'lam_swrite': 0.02735211888513064, 'spline_order_swrite': 2, 'n_splines_fork': 84, 'lam_fork': 352.9825307911938, 'spline_order_fork': 4, 'n_splines_exec': 17, 'lam_exec': 0.20189783317821391, 'spline_order_exec': 4, 'n_splines_rchar': 67, 'lam_rchar': 13.484232526865433, 'spline_order_rchar': 4, 'n_splines_wchar': 92, 'lam_wchar': 0.13833683512606976, 'spline_order_wchar': 4, 'n_splines_pgout': 46, 'lam_pgout': 13.375208045654592, 'spline_order_pgout': 4, 'n_splines_ppgout': 39, 'lam_ppgout': 0.0014298794248776199, 'spline_order_ppgout': 3, 'n_splines_pgfree': 57, 'lam_pgfree': 0.1328839671909128, 'spline_order_pgfree': 5, 'n_splines_pgscan': 97, 'lam_pgscan': 3.2168007919054737, 'spline_order_pgscan': 1, 'n_splines_atch': 53, 'lam_atch': 0.19816448320140018, 'spline_order_atch': 2, 'n_splines_pgin': 47, 'lam_pgin': 0.003341548148602239, 'spline_order_pgin': 5, 'n_splines_ppgin': 69, 'lam_ppgin': 356.822818446244, 'spline_order_ppgin': 1, 'n_splines_pflt': 53, 'lam_pflt': 0.07310655388841575, 'spline_order_pflt': 1, 'n_splines_vflt': 76, 'lam_vflt': 9.487568996827505, 'spline_order_vflt': 2, 'n_splines_runqsz': 22, 'lam_runqsz': 10.956865890047196, 'spline_order_runqsz': 2, 'n_splines_freemem': 69, 'lam_freemem': 1.222134740144172, 'spline_order_freemem': 5, 'n_splines_freeswap': 90, 'lam_freeswap': 0.17064246662360624, 'spline_order_freeswap': 4}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:38:30,112] Trial 65 finished with value: 5.846537551915734 and parameters: {'n_splines_lread': 31, 'lam_lread': 0.0011961368090476127, 'spline_order_lread': 4, 'n_splines_lwrite': 85, 'lam_lwrite': 2.784943116961916, 'spline_order_lwrite': 1, 'n_splines_scall': 58, 'lam_scall': 0.04400834178144758, 'spline_order_scall': 1, 'n_splines_sread': 34, 'lam_sread': 0.030812567764992735, 'spline_order_sread': 4, 'n_splines_swrite': 48, 'lam_swrite': 1.995696051507368, 'spline_order_swrite': 4, 'n_splines_fork': 82, 'lam_fork': 8.163106222124004, 'spline_order_fork': 3, 'n_splines_exec': 25, 'lam_exec': 0.41521219318314556, 'spline_order_exec': 3, 'n_splines_rchar': 61, 'lam_rchar': 6.1195015321857005, 'spline_order_rchar': 4, 'n_splines_wchar': 59, 'lam_wchar': 0.42192374197401955, 'spline_order_wchar': 1, 'n_splines_pgout': 39, 'lam_pgout': 1.839334229394013, 'spline_order_pgout': 3, 'n_splines_ppgout': 19, 'lam_ppgout': 0.005155742080297909, 'spline_order_ppgout': 4, 'n_splines_pgfree': 73, 'lam_pgfree': 0.540213257075199, 'spline_order_pgfree': 3, 'n_splines_pgscan': 77, 'lam_pgscan': 0.4190894649141838, 'spline_order_pgscan': 2, 'n_splines_atch': 48, 'lam_atch': 0.0224547220812467, 'spline_order_atch': 3, 'n_splines_pgin': 69, 'lam_pgin': 0.016612523270431504, 'spline_order_pgin': 5, 'n_splines_ppgin': 60, 'lam_ppgin': 109.17607779967221, 'spline_order_ppgin': 1, 'n_splines_pflt': 59, 'lam_pflt': 24.996222214981604, 'spline_order_pflt': 1, 'n_splines_vflt': 68, 'lam_vflt': 1.3329525233471444, 'spline_order_vflt': 4, 'n_splines_runqsz': 18, 'lam_runqsz': 114.14355057782291, 'spline_order_runqsz': 4, 'n_splines_freemem': 56, 'lam_freemem': 0.3833537933573599, 'spline_order_freemem': 5, 'n_splines_freeswap': 100, 'lam_freeswap': 0.07331319911164452, 'spline_order_freeswap': 3}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:38:39,091] Trial 66 finished with value: 4.9092654001966345 and parameters: {'n_splines_lread': 39, 'lam_lread': 0.016061648679832806, 'spline_order_lread': 5, 'n_splines_lwrite': 64, 'lam_lwrite': 128.35003138035228, 'spline_order_lwrite': 2, 'n_splines_scall': 62, 'lam_scall': 2.6020855763308446, 'spline_order_scall': 1, 'n_splines_sread': 38, 'lam_sread': 0.7915226212255475, 'spline_order_sread': 3, 'n_splines_swrite': 54, 'lam_swrite': 0.08975123237589511, 'spline_order_swrite': 3, 'n_splines_fork': 11, 'lam_fork': 62.16136530518332, 'spline_order_fork': 4, 'n_splines_exec': 10, 'lam_exec': 0.11122237655190212, 'spline_order_exec': 2, 'n_splines_rchar': 80, 'lam_rchar': 690.280219547731, 'spline_order_rchar': 5, 'n_splines_wchar': 97, 'lam_wchar': 1.350792841199397, 'spline_order_wchar': 1, 'n_splines_pgout': 43, 'lam_pgout': 24.529295502185416, 'spline_order_pgout': 3, 'n_splines_ppgout': 27, 'lam_ppgout': 0.01831255413326895, 'spline_order_ppgout': 2, 'n_splines_pgfree': 64, 'lam_pgfree': 0.24709168150343602, 'spline_order_pgfree': 3, 'n_splines_pgscan': 92, 'lam_pgscan': 6.278150193437011, 'spline_order_pgscan': 2, 'n_splines_atch': 30, 'lam_atch': 0.3458634956171492, 'spline_order_atch': 1, 'n_splines_pgin': 42, 'lam_pgin': 0.1016503343762554, 'spline_order_pgin': 5, 'n_splines_ppgin': 80, 'lam_ppgin': 899.1199331978053, 'spline_order_ppgin': 1, 'n_splines_pflt': 46, 'lam_pflt': 3.1600844740972693, 'spline_order_pflt': 1, 'n_splines_vflt': 85, 'lam_vflt': 19.757165549948255, 'spline_order_vflt': 3, 'n_splines_runqsz': 13, 'lam_runqsz': 269.4673503300277, 'spline_order_runqsz': 2, 'n_splines_freemem': 87, 'lam_freemem': 0.11916015975302278, 'spline_order_freemem': 3, 'n_splines_freeswap': 95, 'lam_freeswap': 0.8895275373229946, 'spline_order_freeswap': 3}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:38:48,047] Trial 67 finished with value: 17.794568589407284 and parameters: {'n_splines_lread': 34, 'lam_lread': 0.002685034532105168, 'spline_order_lread': 5, 'n_splines_lwrite': 79, 'lam_lwrite': 10.168683654071007, 'spline_order_lwrite': 1, 'n_splines_scall': 48, 'lam_scall': 0.02213595705602273, 'spline_order_scall': 1, 'n_splines_sread': 19, 'lam_sread': 0.01034101919576286, 'spline_order_sread': 4, 'n_splines_swrite': 30, 'lam_swrite': 0.8615669610083537, 'spline_order_swrite': 3, 'n_splines_fork': 75, 'lam_fork': 1.744898782246153, 'spline_order_fork': 5, 'n_splines_exec': 22, 'lam_exec': 1.2493369166684596, 'spline_order_exec': 3, 'n_splines_rchar': 87, 'lam_rchar': 27.516915617155814, 'spline_order_rchar': 3, 'n_splines_wchar': 69, 'lam_wchar': 0.24072211596691845, 'spline_order_wchar': 1, 'n_splines_pgout': 50, 'lam_pgout': 0.22644742879232801, 'spline_order_pgout': 3, 'n_splines_ppgout': 31, 'lam_ppgout': 1.016802306816884, 'spline_order_ppgout': 3, 'n_splines_pgfree': 53, 'lam_pgfree': 3.6393502434394915, 'spline_order_pgfree': 3, 'n_splines_pgscan': 82, 'lam_pgscan': 0.022304201588523878, 'spline_order_pgscan': 3, 'n_splines_atch': 37, 'lam_atch': 0.14267874195274544, 'spline_order_atch': 2, 'n_splines_pgin': 57, 'lam_pgin': 4.743186074937133, 'spline_order_pgin': 3, 'n_splines_ppgin': 65, 'lam_ppgin': 331.6427369223697, 'spline_order_ppgin': 2, 'n_splines_pflt': 55, 'lam_pflt': 439.6100566849935, 'spline_order_pflt': 1, 'n_splines_vflt': 63, 'lam_vflt': 0.03424446071863186, 'spline_order_vflt': 3, 'n_splines_runqsz': 24, 'lam_runqsz': 59.31434946651276, 'spline_order_runqsz': 3, 'n_splines_freemem': 82, 'lam_freemem': 0.010779010150963055, 'spline_order_freemem': 4, 'n_splines_freeswap': 89, 'lam_freeswap': 0.6549846647762715, 'spline_order_freeswap': 2}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:38:56,998] Trial 68 finished with value: 44.20994381583165 and parameters: {'n_splines_lread': 26, 'lam_lread': 0.00483931830090633, 'spline_order_lread': 5, 'n_splines_lwrite': 10, 'lam_lwrite': 28.873332604013985, 'spline_order_lwrite': 3, 'n_splines_scall': 77, 'lam_scall': 0.6244361393382527, 'spline_order_scall': 2, 'n_splines_sread': 10, 'lam_sread': 0.06543652411829692, 'spline_order_sread': 2, 'n_splines_swrite': 44, 'lam_swrite': 0.017452504874518163, 'spline_order_swrite': 2, 'n_splines_fork': 89, 'lam_fork': 567.2153160105346, 'spline_order_fork': 1, 'n_splines_exec': 74, 'lam_exec': 3.322103721843106, 'spline_order_exec': 4, 'n_splines_rchar': 77, 'lam_rchar': 57.56384899626939, 'spline_order_rchar': 5, 'n_splines_wchar': 91, 'lam_wchar': 10.988538694718057, 'spline_order_wchar': 2, 'n_splines_pgout': 58, 'lam_pgout': 45.10948551184408, 'spline_order_pgout': 2, 'n_splines_ppgout': 45, 'lam_ppgout': 0.16635690863951066, 'spline_order_ppgout': 1, 'n_splines_pgfree': 61, 'lam_pgfree': 722.15141500412, 'spline_order_pgfree': 2, 'n_splines_pgscan': 87, 'lam_pgscan': 0.8237720301175495, 'spline_order_pgscan': 5, 'n_splines_atch': 58, 'lam_atch': 0.0940612615887565, 'spline_order_atch': 3, 'n_splines_pgin': 61, 'lam_pgin': 0.05886237209180868, 'spline_order_pgin': 4, 'n_splines_ppgin': 88, 'lam_ppgin': 31.12625640106275, 'spline_order_ppgin': 1, 'n_splines_pflt': 37, 'lam_pflt': 0.46878440356863105, 'spline_order_pflt': 2, 'n_splines_vflt': 49, 'lam_vflt': 1.8966103455042054, 'spline_order_vflt': 1, 'n_splines_runqsz': 16, 'lam_runqsz': 0.21607527388957393, 'spline_order_runqsz': 2, 'n_splines_freemem': 49, 'lam_freemem': 0.09460599255426133, 'spline_order_freemem': 4, 'n_splines_freeswap': 76, 'lam_freeswap': 0.37669242031235556, 'spline_order_freeswap': 4}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:39:05,095] Trial 69 finished with value: 34.31201511311288 and parameters: {'n_splines_lread': 44, 'lam_lread': 0.0509092164211184, 'spline_order_lread': 4, 'n_splines_lwrite': 75, 'lam_lwrite': 0.04013455796447374, 'spline_order_lwrite': 4, 'n_splines_scall': 72, 'lam_scall': 1.8399962298141264, 'spline_order_scall': 1, 'n_splines_sread': 16, 'lam_sread': 1.5325147412216547, 'spline_order_sread': 3, 'n_splines_swrite': 64, 'lam_swrite': 6.046705618702824, 'spline_order_swrite': 4, 'n_splines_fork': 79, 'lam_fork': 31.749859332996014, 'spline_order_fork': 5, 'n_splines_exec': 17, 'lam_exec': 140.09528851849072, 'spline_order_exec': 1, 'n_splines_rchar': 19, 'lam_rchar': 180.6666565803372, 'spline_order_rchar': 4, 'n_splines_wchar': 74, 'lam_wchar': 0.839095353390374, 'spline_order_wchar': 1, 'n_splines_pgout': 55, 'lam_pgout': 1.5515744525910964, 'spline_order_pgout': 2, 'n_splines_ppgout': 36, 'lam_ppgout': 5.497318822285941, 'spline_order_ppgout': 4, 'n_splines_pgfree': 47, 'lam_pgfree': 0.038260152056106965, 'spline_order_pgfree': 3, 'n_splines_pgscan': 42, 'lam_pgscan': 0.18927456455280722, 'spline_order_pgscan': 4, 'n_splines_atch': 91, 'lam_atch': 3.537650280060519, 'spline_order_atch': 1, 'n_splines_pgin': 46, 'lam_pgin': 0.007039232683667997, 'spline_order_pgin': 5, 'n_splines_ppgin': 44, 'lam_ppgin': 17.65134602705016, 'spline_order_ppgin': 1, 'n_splines_pflt': 49, 'lam_pflt': 0.041501345737002426, 'spline_order_pflt': 1, 'n_splines_vflt': 72, 'lam_vflt': 0.002757634037803042, 'spline_order_vflt': 3, 'n_splines_runqsz': 19, 'lam_runqsz': 1.4123644647747788, 'spline_order_runqsz': 1, 'n_splines_freemem': 62, 'lam_freemem': 52.20800906900672, 'spline_order_freemem': 5, 'n_splines_freeswap': 92, 'lam_freeswap': 0.021274477072326676, 'spline_order_freeswap': 3}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:39:17,297] Trial 70 finished with value: 3.7286250426875216 and parameters: {'n_splines_lread': 51, 'lam_lread': 627.2051289042374, 'spline_order_lread': 5, 'n_splines_lwrite': 71, 'lam_lwrite': 4.30654176079667, 'spline_order_lwrite': 1, 'n_splines_scall': 68, 'lam_scall': 5.025535797595325, 'spline_order_scall': 3, 'n_splines_sread': 23, 'lam_sread': 568.8558023896392, 'spline_order_sread': 3, 'n_splines_swrite': 50, 'lam_swrite': 116.07008174996346, 'spline_order_swrite': 3, 'n_splines_fork': 67, 'lam_fork': 140.23941273247823, 'spline_order_fork': 4, 'n_splines_exec': 13, 'lam_exec': 0.8562770910076632, 'spline_order_exec': 3, 'n_splines_rchar': 72, 'lam_rchar': 284.74519788101213, 'spline_order_rchar': 3, 'n_splines_wchar': 85, 'lam_wchar': 5.815686683763105, 'spline_order_wchar': 2, 'n_splines_pgout': 33, 'lam_pgout': 0.5847364732492485, 'spline_order_pgout': 3, 'n_splines_ppgout': 21, 'lam_ppgout': 0.062228763733686805, 'spline_order_ppgout': 3, 'n_splines_pgfree': 77, 'lam_pgfree': 0.013205173402085383, 'spline_order_pgfree': 4, 'n_splines_pgscan': 97, 'lam_pgscan': 1.2517638490214646, 'spline_order_pgscan': 1, 'n_splines_atch': 75, 'lam_atch': 0.8002494190970227, 'spline_order_atch': 2, 'n_splines_pgin': 50, 'lam_pgin': 0.004856317322497393, 'spline_order_pgin': 5, 'n_splines_ppgin': 84, 'lam_ppgin': 160.96652708268593, 'spline_order_ppgin': 2, 'n_splines_pflt': 32, 'lam_pflt': 1.1602754053277167, 'spline_order_pflt': 1, 'n_splines_vflt': 79, 'lam_vflt': 0.0133757135485527, 'spline_order_vflt': 5, 'n_splines_runqsz': 10, 'lam_runqsz': 368.8778567262327, 'spline_order_runqsz': 5, 'n_splines_freemem': 92, 'lam_freemem': 0.5566543213914869, 'spline_order_freemem': 5, 'n_splines_freeswap': 98, 'lam_freeswap': 1.3865114921215718, 'spline_order_freeswap': 2}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:39:28,695] Trial 71 finished with value: 3.890362565357245 and parameters: {'n_splines_lread': 51, 'lam_lread': 493.1158829024078, 'spline_order_lread': 5, 'n_splines_lwrite': 72, 'lam_lwrite': 3.979666334262029, 'spline_order_lwrite': 1, 'n_splines_scall': 68, 'lam_scall': 4.240211202968776, 'spline_order_scall': 3, 'n_splines_sread': 22, 'lam_sread': 411.49755894986043, 'spline_order_sread': 3, 'n_splines_swrite': 50, 'lam_swrite': 172.68432321089813, 'spline_order_swrite': 3, 'n_splines_fork': 68, 'lam_fork': 227.26473720682347, 'spline_order_fork': 4, 'n_splines_exec': 12, 'lam_exec': 0.5596873921692396, 'spline_order_exec': 3, 'n_splines_rchar': 72, 'lam_rchar': 266.1460097714341, 'spline_order_rchar': 3, 'n_splines_wchar': 85, 'lam_wchar': 24.996640963352906, 'spline_order_wchar': 2, 'n_splines_pgout': 30, 'lam_pgout': 0.6042807433802247, 'spline_order_pgout': 3, 'n_splines_ppgout': 20, 'lam_ppgout': 0.053051669982135896, 'spline_order_ppgout': 3, 'n_splines_pgfree': 78, 'lam_pgfree': 0.013381566654713503, 'spline_order_pgfree': 5, 'n_splines_pgscan': 97, 'lam_pgscan': 1.7872071602236106, 'spline_order_pgscan': 1, 'n_splines_atch': 75, 'lam_atch': 1.4655316589686416, 'spline_order_atch': 2, 'n_splines_pgin': 51, 'lam_pgin': 0.005324585037951954, 'spline_order_pgin': 5, 'n_splines_ppgin': 96, 'lam_ppgin': 70.61506593314915, 'spline_order_ppgin': 3, 'n_splines_pflt': 33, 'lam_pflt': 1.0893036242540386, 'spline_order_pflt': 1, 'n_splines_vflt': 79, 'lam_vflt': 0.006757161664459736, 'spline_order_vflt': 5, 'n_splines_runqsz': 10, 'lam_runqsz': 467.89290264446083, 'spline_order_runqsz': 4, 'n_splines_freemem': 91, 'lam_freemem': 0.6686950854568335, 'spline_order_freemem': 5, 'n_splines_freeswap': 98, 'lam_freeswap': 1.4450056306149839, 'spline_order_freeswap': 2}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:39:42,201] Trial 72 finished with value: 3.962936829863154 and parameters: {'n_splines_lread': 51, 'lam_lread': 805.8019863890532, 'spline_order_lread': 5, 'n_splines_lwrite': 71, 'lam_lwrite': 6.165023469205932, 'spline_order_lwrite': 1, 'n_splines_scall': 67, 'lam_scall': 4.958160493610965, 'spline_order_scall': 3, 'n_splines_sread': 22, 'lam_sread': 474.4777514608002, 'spline_order_sread': 3, 'n_splines_swrite': 51, 'lam_swrite': 144.98743830319606, 'spline_order_swrite': 3, 'n_splines_fork': 67, 'lam_fork': 232.4707914165927, 'spline_order_fork': 4, 'n_splines_exec': 12, 'lam_exec': 0.8541150556252247, 'spline_order_exec': 3, 'n_splines_rchar': 74, 'lam_rchar': 315.12665008069587, 'spline_order_rchar': 3, 'n_splines_wchar': 84, 'lam_wchar': 20.885768700458886, 'spline_order_wchar': 2, 'n_splines_pgout': 34, 'lam_pgout': 0.6759651028715287, 'spline_order_pgout': 3, 'n_splines_ppgout': 20, 'lam_ppgout': 0.05610147751535181, 'spline_order_ppgout': 3, 'n_splines_pgfree': 94, 'lam_pgfree': 0.009991496300084968, 'spline_order_pgfree': 5, 'n_splines_pgscan': 98, 'lam_pgscan': 1.3178667384590927, 'spline_order_pgscan': 1, 'n_splines_atch': 75, 'lam_atch': 1.5472288262670246, 'spline_order_atch': 2, 'n_splines_pgin': 50, 'lam_pgin': 0.004540857045785588, 'spline_order_pgin': 5, 'n_splines_ppgin': 95, 'lam_ppgin': 75.60849307917789, 'spline_order_ppgin': 3, 'n_splines_pflt': 32, 'lam_pflt': 1.4035969838692015, 'spline_order_pflt': 1, 'n_splines_vflt': 82, 'lam_vflt': 0.011975559413731005, 'spline_order_vflt': 5, 'n_splines_runqsz': 12, 'lam_runqsz': 973.0619736121464, 'spline_order_runqsz': 5, 'n_splines_freemem': 91, 'lam_freemem': 1.608888419042483, 'spline_order_freemem': 5, 'n_splines_freeswap': 97, 'lam_freeswap': 1.471198054969916, 'spline_order_freeswap': 2}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:39:53,743] Trial 73 finished with value: 4.470388278408576 and parameters: {'n_splines_lread': 47, 'lam_lread': 336.13745510662966, 'spline_order_lread': 5, 'n_splines_lwrite': 71, 'lam_lwrite': 0.4810309748556132, 'spline_order_lwrite': 1, 'n_splines_scall': 84, 'lam_scall': 2.9659744558621455, 'spline_order_scall': 3, 'n_splines_sread': 25, 'lam_sread': 493.72649220574425, 'spline_order_sread': 3, 'n_splines_swrite': 58, 'lam_swrite': 493.36666883633654, 'spline_order_swrite': 3, 'n_splines_fork': 58, 'lam_fork': 161.38559978947006, 'spline_order_fork': 4, 'n_splines_exec': 16, 'lam_exec': 0.5527071652927054, 'spline_order_exec': 3, 'n_splines_rchar': 67, 'lam_rchar': 141.92630780669052, 'spline_order_rchar': 3, 'n_splines_wchar': 78, 'lam_wchar': 66.59467683544753, 'spline_order_wchar': 2, 'n_splines_pgout': 28, 'lam_pgout': 0.2922425277528266, 'spline_order_pgout': 3, 'n_splines_ppgout': 15, 'lam_ppgout': 0.07819328145470822, 'spline_order_ppgout': 3, 'n_splines_pgfree': 78, 'lam_pgfree': 0.0033483651393604198, 'spline_order_pgfree': 5, 'n_splines_pgscan': 94, 'lam_pgscan': 0.5255389961694562, 'spline_order_pgscan': 1, 'n_splines_atch': 85, 'lam_atch': 0.7752677125828376, 'spline_order_atch': 2, 'n_splines_pgin': 56, 'lam_pgin': 0.0014569771413483876, 'spline_order_pgin': 5, 'n_splines_ppgin': 97, 'lam_ppgin': 154.56643642624073, 'spline_order_ppgin': 4, 'n_splines_pflt': 27, 'lam_pflt': 5.385929304703581, 'spline_order_pflt': 1, 'n_splines_vflt': 78, 'lam_vflt': 0.0030852721629254115, 'spline_order_vflt': 5, 'n_splines_runqsz': 15, 'lam_runqsz': 414.68627450354575, 'spline_order_runqsz': 5, 'n_splines_freemem': 96, 'lam_freemem': 0.5513560691956731, 'spline_order_freemem': 5, 'n_splines_freeswap': 94, 'lam_freeswap': 1.6486674010538742, 'spline_order_freeswap': 2}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:40:04,384] Trial 74 finished with value: 5.067619462970032 and parameters: {'n_splines_lread': 55, 'lam_lread': 123.96940820068023, 'spline_order_lread': 5, 'n_splines_lwrite': 67, 'lam_lwrite': 4.668599730491335, 'spline_order_lwrite': 1, 'n_splines_scall': 68, 'lam_scall': 10.30390915609743, 'spline_order_scall': 4, 'n_splines_sread': 13, 'lam_sread': 177.50916497308455, 'spline_order_sread': 4, 'n_splines_swrite': 41, 'lam_swrite': 69.53461742569644, 'spline_order_swrite': 5, 'n_splines_fork': 51, 'lam_fork': 124.3519841312634, 'spline_order_fork': 4, 'n_splines_exec': 21, 'lam_exec': 1.054827637683344, 'spline_order_exec': 3, 'n_splines_rchar': 70, 'lam_rchar': 516.0611478727812, 'spline_order_rchar': 2, 'n_splines_wchar': 82, 'lam_wchar': 6.341349663788389, 'spline_order_wchar': 2, 'n_splines_pgout': 22, 'lam_pgout': 0.3845092063710706, 'spline_order_pgout': 3, 'n_splines_ppgout': 13, 'lam_ppgout': 0.1582485297046213, 'spline_order_ppgout': 3, 'n_splines_pgfree': 87, 'lam_pgfree': 0.01679388640540769, 'spline_order_pgfree': 4, 'n_splines_pgscan': 100, 'lam_pgscan': 0.0047994551332750535, 'spline_order_pgscan': 1, 'n_splines_atch': 96, 'lam_atch': 7.963779094557141, 'spline_order_atch': 2, 'n_splines_pgin': 54, 'lam_pgin': 0.012012350564019048, 'spline_order_pgin': 5, 'n_splines_ppgin': 90, 'lam_ppgin': 41.17826170874477, 'spline_order_ppgin': 3, 'n_splines_pflt': 21, 'lam_pflt': 1.075801462379615, 'spline_order_pflt': 1, 'n_splines_vflt': 88, 'lam_vflt': 0.01503458480623732, 'spline_order_vflt': 5, 'n_splines_runqsz': 10, 'lam_runqsz': 159.5147612220411, 'spline_order_runqsz': 4, 'n_splines_freemem': 94, 'lam_freemem': 2.471717049607565, 'spline_order_freemem': 5, 'n_splines_freeswap': 85, 'lam_freeswap': 4.040904616795703, 'spline_order_freeswap': 2}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:40:16,183] Trial 75 finished with value: 7.659147278125114 and parameters: {'n_splines_lread': 52, 'lam_lread': 120.51325309462374, 'spline_order_lread': 5, 'n_splines_lwrite': 82, 'lam_lwrite': 1.413857746871654, 'spline_order_lwrite': 1, 'n_splines_scall': 65, 'lam_scall': 22.13743809501488, 'spline_order_scall': 3, 'n_splines_sread': 23, 'lam_sread': 598.5750016276595, 'spline_order_sread': 2, 'n_splines_swrite': 49, 'lam_swrite': 980.7523999573318, 'spline_order_swrite': 3, 'n_splines_fork': 72, 'lam_fork': 0.04513500709704591, 'spline_order_fork': 4, 'n_splines_exec': 12, 'lam_exec': 0.31206832295132453, 'spline_order_exec': 3, 'n_splines_rchar': 63, 'lam_rchar': 0.15476107919291632, 'spline_order_rchar': 3, 'n_splines_wchar': 87, 'lam_wchar': 57.602275523146915, 'spline_order_wchar': 3, 'n_splines_pgout': 17, 'lam_pgout': 0.10605139332698255, 'spline_order_pgout': 4, 'n_splines_ppgout': 23, 'lam_ppgout': 0.5370598373893698, 'spline_order_ppgout': 3, 'n_splines_pgfree': 74, 'lam_pgfree': 0.004979513667628468, 'spline_order_pgfree': 5, 'n_splines_pgscan': 96, 'lam_pgscan': 1.9647018537479903, 'spline_order_pgscan': 1, 'n_splines_atch': 68, 'lam_atch': 31.037704850340816, 'spline_order_atch': 2, 'n_splines_pgin': 50, 'lam_pgin': 0.0025143202059776473, 'spline_order_pgin': 5, 'n_splines_ppgin': 83, 'lam_ppgin': 7.97370500048527, 'spline_order_ppgin': 4, 'n_splines_pflt': 31, 'lam_pflt': 1.7010360910874662, 'spline_order_pflt': 1, 'n_splines_vflt': 92, 'lam_vflt': 0.004374081000019815, 'spline_order_vflt': 5, 'n_splines_runqsz': 20, 'lam_runqsz': 700.8578423517027, 'spline_order_runqsz': 4, 'n_splines_freemem': 97, 'lam_freemem': 0.7659227640516436, 'spline_order_freemem': 5, 'n_splines_freeswap': 98, 'lam_freeswap': 2.092262654522067, 'spline_order_freeswap': 1}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:40:34,622] Trial 76 finished with value: 31.424280464327897 and parameters: {'n_splines_lread': 63, 'lam_lread': 45.62029800491824, 'spline_order_lread': 4, 'n_splines_lwrite': 34, 'lam_lwrite': 3.340219211821308, 'spline_order_lwrite': 2, 'n_splines_scall': 71, 'lam_scall': 7.424464507858163, 'spline_order_scall': 3, 'n_splines_sread': 20, 'lam_sread': 943.8462143510396, 'spline_order_sread': 3, 'n_splines_swrite': 46, 'lam_swrite': 216.67099596849016, 'spline_order_swrite': 3, 'n_splines_fork': 64, 'lam_fork': 446.90718521242786, 'spline_order_fork': 4, 'n_splines_exec': 27, 'lam_exec': 8.478471906928423, 'spline_order_exec': 3, 'n_splines_rchar': 83, 'lam_rchar': 287.07392636942745, 'spline_order_rchar': 3, 'n_splines_wchar': 97, 'lam_wchar': 3.1374802189764703, 'spline_order_wchar': 2, 'n_splines_pgout': 33, 'lam_pgout': 1.0851489838512558, 'spline_order_pgout': 3, 'n_splines_ppgout': 51, 'lam_ppgout': 0.03494940823480446, 'spline_order_ppgout': 2, 'n_splines_pgfree': 83, 'lam_pgfree': 0.023388005584058343, 'spline_order_pgfree': 4, 'n_splines_pgscan': 91, 'lam_pgscan': 3.581048655324021, 'spline_order_pgscan': 1, 'n_splines_atch': 75, 'lam_atch': 1.6508334584741717, 'spline_order_atch': 2, 'n_splines_pgin': 66, 'lam_pgin': 0.0018533066117654576, 'spline_order_pgin': 5, 'n_splines_ppgin': 92, 'lam_ppgin': 72.33501379679377, 'spline_order_ppgin': 3, 'n_splines_pflt': 39, 'lam_pflt': 10.620433869070851, 'spline_order_pflt': 1, 'n_splines_vflt': 96, 'lam_vflt': 0.00808691597428642, 'spline_order_vflt': 5, 'n_splines_runqsz': 87, 'lam_runqsz': 327.0908612152042, 'spline_order_runqsz': 5, 'n_splines_freemem': 99, 'lam_freemem': 0.26422848082091127, 'spline_order_freemem': 3, 'n_splines_freeswap': 91, 'lam_freeswap': 15.870379207471007, 'spline_order_freeswap': 2}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:40:50,595] Trial 77 finished with value: 3.0957505300052772 and parameters: {'n_splines_lread': 59, 'lam_lread': 438.82199434294307, 'spline_order_lread': 5, 'n_splines_lwrite': 63, 'lam_lwrite': 0.12572794914906124, 'spline_order_lwrite': 1, 'n_splines_scall': 77, 'lam_scall': 1.1291389228864683, 'spline_order_scall': 3, 'n_splines_sread': 27, 'lam_sread': 311.0572522245083, 'spline_order_sread': 4, 'n_splines_swrite': 39, 'lam_swrite': 36.755691836869794, 'spline_order_swrite': 3, 'n_splines_fork': 86, 'lam_fork': 0.24851222884451937, 'spline_order_fork': 3, 'n_splines_exec': 18, 'lam_exec': 1.8680497861586236, 'spline_order_exec': 2, 'n_splines_rchar': 75, 'lam_rchar': 707.3626044690592, 'spline_order_rchar': 3, 'n_splines_wchar': 71, 'lam_wchar': 21.515756495062895, 'spline_order_wchar': 2, 'n_splines_pgout': 29, 'lam_pgout': 0.04281922372494406, 'spline_order_pgout': 3, 'n_splines_ppgout': 18, 'lam_ppgout': 0.008697282740321636, 'spline_order_ppgout': 5, 'n_splines_pgfree': 70, 'lam_pgfree': 0.009593954729815411, 'spline_order_pgfree': 4, 'n_splines_pgscan': 10, 'lam_pgscan': 0.24217755500155208, 'spline_order_pgscan': 1, 'n_splines_atch': 80, 'lam_atch': 0.4936191904326665, 'spline_order_atch': 3, 'n_splines_pgin': 37, 'lam_pgin': 0.005693165837547675, 'spline_order_pgin': 4, 'n_splines_ppgin': 85, 'lam_ppgin': 60.30392697409965, 'spline_order_ppgin': 2, 'n_splines_pflt': 96, 'lam_pflt': 0.6874324344074219, 'spline_order_pflt': 2, 'n_splines_vflt': 75, 'lam_vflt': 0.004989263901073365, 'spline_order_vflt': 5, 'n_splines_runqsz': 27, 'lam_runqsz': 486.18484610575285, 'spline_order_runqsz': 4, 'n_splines_freemem': 85, 'lam_freemem': 0.16749529772519653, 'spline_order_freemem': 4, 'n_splines_freeswap': 81, 'lam_freeswap': 8.40681083742742, 'spline_order_freeswap': 2}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:41:04,218] Trial 78 finished with value: 20.871376841349953 and parameters: {'n_splines_lread': 59, 'lam_lread': 224.85450674047098, 'spline_order_lread': 3, 'n_splines_lwrite': 65, 'lam_lwrite': 0.2284774070618867, 'spline_order_lwrite': 1, 'n_splines_scall': 90, 'lam_scall': 1.0548921613714872, 'spline_order_scall': 3, 'n_splines_sread': 30, 'lam_sread': 142.45854755338104, 'spline_order_sread': 4, 'n_splines_swrite': 34, 'lam_swrite': 34.12830338599019, 'spline_order_swrite': 2, 'n_splines_fork': 87, 'lam_fork': 0.15104477310688838, 'spline_order_fork': 3, 'n_splines_exec': 18, 'lam_exec': 5.5027531269988295, 'spline_order_exec': 2, 'n_splines_rchar': 89, 'lam_rchar': 893.147811934623, 'spline_order_rchar': 2, 'n_splines_wchar': 64, 'lam_wchar': 14.312377138224825, 'spline_order_wchar': 2, 'n_splines_pgout': 25, 'lam_pgout': 0.036887967154581765, 'spline_order_pgout': 2, 'n_splines_ppgout': 16, 'lam_ppgout': 0.009081026948835913, 'spline_order_ppgout': 5, 'n_splines_pgfree': 71, 'lam_pgfree': 0.008250267208086947, 'spline_order_pgfree': 4, 'n_splines_pgscan': 12, 'lam_pgscan': 0.08797868452272467, 'spline_order_pgscan': 1, 'n_splines_atch': 81, 'lam_atch': 0.4913904751526855, 'spline_order_atch': 4, 'n_splines_pgin': 33, 'lam_pgin': 0.009810907029226887, 'spline_order_pgin': 4, 'n_splines_ppgin': 85, 'lam_ppgin': 123.19044527555417, 'spline_order_ppgin': 2, 'n_splines_pflt': 98, 'lam_pflt': 56.583916277596686, 'spline_order_pflt': 4, 'n_splines_vflt': 70, 'lam_vflt': 0.03539263336632551, 'spline_order_vflt': 5, 'n_splines_runqsz': 37, 'lam_runqsz': 86.24489216646967, 'spline_order_runqsz': 5, 'n_splines_freemem': 84, 'lam_freemem': 0.06455063797392821, 'spline_order_freemem': 4, 'n_splines_freeswap': 81, 'lam_freeswap': 9.070223017469258, 'spline_order_freeswap': 2}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:41:15,554] Trial 79 finished with value: 12.001036217476988 and parameters: {'n_splines_lread': 57, 'lam_lread': 0.455515960891108, 'spline_order_lread': 2, 'n_splines_lwrite': 55, 'lam_lwrite': 0.7305107983717891, 'spline_order_lwrite': 1, 'n_splines_scall': 78, 'lam_scall': 0.41132324001173953, 'spline_order_scall': 4, 'n_splines_sread': 27, 'lam_sread': 250.2730472631526, 'spline_order_sread': 5, 'n_splines_swrite': 40, 'lam_swrite': 15.229900999074365, 'spline_order_swrite': 3, 'n_splines_fork': 92, 'lam_fork': 0.5766710243111262, 'spline_order_fork': 3, 'n_splines_exec': 22, 'lam_exec': 1.876133085816063, 'spline_order_exec': 2, 'n_splines_rchar': 76, 'lam_rchar': 596.7442952317591, 'spline_order_rchar': 3, 'n_splines_wchar': 66, 'lam_wchar': 8.26462943263099, 'spline_order_wchar': 3, 'n_splines_pgout': 37, 'lam_pgout': 0.01635618970312934, 'spline_order_pgout': 4, 'n_splines_ppgout': 10, 'lam_ppgout': 0.01686615404578857, 'spline_order_ppgout': 5, 'n_splines_pgfree': 67, 'lam_pgfree': 0.03333986732805763, 'spline_order_pgfree': 4, 'n_splines_pgscan': 20, 'lam_pgscan': 0.21240978033565946, 'spline_order_pgscan': 1, 'n_splines_atch': 65, 'lam_atch': 0.518400442170093, 'spline_order_atch': 3, 'n_splines_pgin': 44, 'lam_pgin': 0.015379813097737637, 'spline_order_pgin': 4, 'n_splines_ppgin': 74, 'lam_ppgin': 16.796520420616236, 'spline_order_ppgin': 2, 'n_splines_pflt': 96, 'lam_pflt': 0.6520493987471282, 'spline_order_pflt': 2, 'n_splines_vflt': 75, 'lam_vflt': 0.00177067093504912, 'spline_order_vflt': 4, 'n_splines_runqsz': 32, 'lam_runqsz': 209.51322929633702, 'spline_order_runqsz': 4, 'n_splines_freemem': 75, 'lam_freemem': 0.0038830595462253396, 'spline_order_freemem': 4, 'n_splines_freeswap': 58, 'lam_freeswap': 29.38122665955358, 'spline_order_freeswap': 1}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:41:31,417] Trial 80 finished with value: 13.187794254861442 and parameters: {'n_splines_lread': 64, 'lam_lread': 787.9014742371081, 'spline_order_lread': 4, 'n_splines_lwrite': 79, 'lam_lwrite': 0.1379981527100256, 'spline_order_lwrite': 2, 'n_splines_scall': 75, 'lam_scall': 1.963373687910803, 'spline_order_scall': 2, 'n_splines_sread': 33, 'lam_sread': 298.7909471895311, 'spline_order_sread': 4, 'n_splines_swrite': 36, 'lam_swrite': 340.68772544186913, 'spline_order_swrite': 3, 'n_splines_fork': 82, 'lam_fork': 0.24054972067111874, 'spline_order_fork': 3, 'n_splines_exec': 30, 'lam_exec': 633.3786573193746, 'spline_order_exec': 1, 'n_splines_rchar': 66, 'lam_rchar': 2.9058258246514828, 'spline_order_rchar': 3, 'n_splines_wchar': 71, 'lam_wchar': 183.47612154456917, 'spline_order_wchar': 2, 'n_splines_pgout': 62, 'lam_pgout': 0.07258478327504927, 'spline_order_pgout': 3, 'n_splines_ppgout': 66, 'lam_ppgout': 0.004194349351687131, 'spline_order_ppgout': 5, 'n_splines_pgfree': 70, 'lam_pgfree': 0.0031835883639969287, 'spline_order_pgfree': 4, 'n_splines_pgscan': 31, 'lam_pgscan': 0.06778522244312589, 'spline_order_pgscan': 3, 'n_splines_atch': 79, 'lam_atch': 0.2749309261397521, 'spline_order_atch': 4, 'n_splines_pgin': 38, 'lam_pgin': 0.0034825940718887904, 'spline_order_pgin': 3, 'n_splines_ppgin': 82, 'lam_ppgin': 31.884925169623727, 'spline_order_ppgin': 2, 'n_splines_pflt': 86, 'lam_pflt': 0.3937852338419143, 'spline_order_pflt': 2, 'n_splines_vflt': 62, 'lam_vflt': 0.022143995440418906, 'spline_order_vflt': 5, 'n_splines_runqsz': 28, 'lam_runqsz': 126.98333958834401, 'spline_order_runqsz': 5, 'n_splines_freemem': 89, 'lam_freemem': 0.17730227907301926, 'spline_order_freemem': 4, 'n_splines_freeswap': 19, 'lam_freeswap': 7.867163498035772, 'spline_order_freeswap': 2}. Best is trial 13 with value: 2.944528873830982.\n",
      "[I 2024-03-27 21:41:45,058] Trial 81 finished with value: 2.740213584945917 and parameters: {'n_splines_lread': 49, 'lam_lread': 501.61533748418543, 'spline_order_lread': 5, 'n_splines_lwrite': 62, 'lam_lwrite': 0.09685276459402459, 'spline_order_lwrite': 1, 'n_splines_scall': 86, 'lam_scall': 0.8330238593577353, 'spline_order_scall': 3, 'n_splines_sread': 18, 'lam_sread': 345.96256925907755, 'spline_order_sread': 4, 'n_splines_swrite': 42, 'lam_swrite': 83.75688258053826, 'spline_order_swrite': 3, 'n_splines_fork': 85, 'lam_fork': 0.3231257805781241, 'spline_order_fork': 3, 'n_splines_exec': 14, 'lam_exec': 0.6355662599445909, 'spline_order_exec': 2, 'n_splines_rchar': 72, 'lam_rchar': 434.7803030665083, 'spline_order_rchar': 3, 'n_splines_wchar': 81, 'lam_wchar': 5.7703216001626805, 'spline_order_wchar': 2, 'n_splines_pgout': 29, 'lam_pgout': 0.20137980370580547, 'spline_order_pgout': 3, 'n_splines_ppgout': 19, 'lam_ppgout': 0.007264717149162388, 'spline_order_ppgout': 5, 'n_splines_pgfree': 77, 'lam_pgfree': 0.0568066405313421, 'spline_order_pgfree': 5, 'n_splines_pgscan': 97, 'lam_pgscan': 0.2968392679982818, 'spline_order_pgscan': 1, 'n_splines_atch': 90, 'lam_atch': 1.016737346683253, 'spline_order_atch': 2, 'n_splines_pgin': 41, 'lam_pgin': 0.006326480404923288, 'spline_order_pgin': 5, 'n_splines_ppgin': 94, 'lam_ppgin': 61.72447311500173, 'spline_order_ppgin': 3, 'n_splines_pflt': 35, 'lam_pflt': 3.4884908897424958, 'spline_order_pflt': 1, 'n_splines_vflt': 81, 'lam_vflt': 0.005373890883698487, 'spline_order_vflt': 5, 'n_splines_runqsz': 23, 'lam_runqsz': 558.958100606524, 'spline_order_runqsz': 4, 'n_splines_freemem': 86, 'lam_freemem': 0.853155480181184, 'spline_order_freemem': 5, 'n_splines_freeswap': 88, 'lam_freeswap': 3.5318259114931316, 'spline_order_freeswap': 2}. Best is trial 81 with value: 2.740213584945917.\n",
      "[I 2024-03-27 21:42:00,200] Trial 82 finished with value: 3.4684000429273327 and parameters: {'n_splines_lread': 38, 'lam_lread': 374.4676516593981, 'spline_order_lread': 5, 'n_splines_lwrite': 69, 'lam_lwrite': 0.11060589343469648, 'spline_order_lwrite': 1, 'n_splines_scall': 93, 'lam_scall': 0.1998132278355818, 'spline_order_scall': 3, 'n_splines_sread': 17, 'lam_sread': 83.10593692069153, 'spline_order_sread': 4, 'n_splines_swrite': 42, 'lam_swrite': 588.2432716264196, 'spline_order_swrite': 3, 'n_splines_fork': 85, 'lam_fork': 0.10657695378967022, 'spline_order_fork': 3, 'n_splines_exec': 15, 'lam_exec': 2.410049046888283, 'spline_order_exec': 2, 'n_splines_rchar': 15, 'lam_rchar': 917.4255200441033, 'spline_order_rchar': 3, 'n_splines_wchar': 81, 'lam_wchar': 5.348150023834725, 'spline_order_wchar': 2, 'n_splines_pgout': 29, 'lam_pgout': 0.19326103692821636, 'spline_order_pgout': 3, 'n_splines_ppgout': 25, 'lam_ppgout': 0.006688300854328446, 'spline_order_ppgout': 5, 'n_splines_pgfree': 76, 'lam_pgfree': 0.09123772161172013, 'spline_order_pgfree': 5, 'n_splines_pgscan': 94, 'lam_pgscan': 0.44232002943671583, 'spline_order_pgscan': 1, 'n_splines_atch': 86, 'lam_atch': 0.16336822928773487, 'spline_order_atch': 3, 'n_splines_pgin': 41, 'lam_pgin': 0.007408985738640568, 'spline_order_pgin': 5, 'n_splines_ppgin': 93, 'lam_ppgin': 52.50323829882066, 'spline_order_ppgin': 2, 'n_splines_pflt': 96, 'lam_pflt': 3.3062884070825747, 'spline_order_pflt': 1, 'n_splines_vflt': 82, 'lam_vflt': 0.012709139056228431, 'spline_order_vflt': 5, 'n_splines_runqsz': 23, 'lam_runqsz': 744.3921909201146, 'spline_order_runqsz': 4, 'n_splines_freemem': 79, 'lam_freemem': 0.3146061982749924, 'spline_order_freemem': 3, 'n_splines_freeswap': 86, 'lam_freeswap': 5.474828443379365, 'spline_order_freeswap': 2}. Best is trial 81 with value: 2.740213584945917.\n",
      "[I 2024-03-27 21:42:14,747] Trial 83 finished with value: 3.079126107490166 and parameters: {'n_splines_lread': 38, 'lam_lread': 279.45419926313576, 'spline_order_lread': 5, 'n_splines_lwrite': 60, 'lam_lwrite': 0.11133133707564827, 'spline_order_lwrite': 1, 'n_splines_scall': 95, 'lam_scall': 0.19699844742217687, 'spline_order_scall': 3, 'n_splines_sread': 15, 'lam_sread': 50.230697285704025, 'spline_order_sread': 4, 'n_splines_swrite': 38, 'lam_swrite': 396.8231647066886, 'spline_order_swrite': 3, 'n_splines_fork': 89, 'lam_fork': 0.04568797063736862, 'spline_order_fork': 3, 'n_splines_exec': 15, 'lam_exec': 3.259522920091953, 'spline_order_exec': 2, 'n_splines_rchar': 13, 'lam_rchar': 534.8061714716288, 'spline_order_rchar': 3, 'n_splines_wchar': 78, 'lam_wchar': 3.509157146283321, 'spline_order_wchar': 2, 'n_splines_pgout': 99, 'lam_pgout': 0.13760971819671533, 'spline_order_pgout': 3, 'n_splines_ppgout': 25, 'lam_ppgout': 0.0073034831530937655, 'spline_order_ppgout': 5, 'n_splines_pgfree': 66, 'lam_pgfree': 0.055140649292164994, 'spline_order_pgfree': 5, 'n_splines_pgscan': 88, 'lam_pgscan': 0.25978167429923227, 'spline_order_pgscan': 1, 'n_splines_atch': 90, 'lam_atch': 0.15674232412326092, 'spline_order_atch': 3, 'n_splines_pgin': 35, 'lam_pgin': 0.03241028086292183, 'spline_order_pgin': 5, 'n_splines_ppgin': 99, 'lam_ppgin': 11.545639090757929, 'spline_order_ppgin': 2, 'n_splines_pflt': 92, 'lam_pflt': 4.04386808822883, 'spline_order_pflt': 1, 'n_splines_vflt': 83, 'lam_vflt': 0.005572884462426112, 'spline_order_vflt': 5, 'n_splines_runqsz': 25, 'lam_runqsz': 778.9689526662352, 'spline_order_runqsz': 4, 'n_splines_freemem': 79, 'lam_freemem': 0.31950156874193936, 'spline_order_freemem': 3, 'n_splines_freeswap': 79, 'lam_freeswap': 3.513546435611699, 'spline_order_freeswap': 2}. Best is trial 81 with value: 2.740213584945917.\n",
      "[I 2024-03-27 21:42:27,645] Trial 84 finished with value: 2.5407100180963624 and parameters: {'n_splines_lread': 38, 'lam_lread': 290.8115123173335, 'spline_order_lread': 5, 'n_splines_lwrite': 62, 'lam_lwrite': 0.1042366815650523, 'spline_order_lwrite': 1, 'n_splines_scall': 97, 'lam_scall': 0.1192521737579469, 'spline_order_scall': 3, 'n_splines_sread': 17, 'lam_sread': 45.87878208237527, 'spline_order_sread': 4, 'n_splines_swrite': 39, 'lam_swrite': 620.9425410822906, 'spline_order_swrite': 3, 'n_splines_fork': 85, 'lam_fork': 0.06405556156695394, 'spline_order_fork': 3, 'n_splines_exec': 15, 'lam_exec': 2.9411880617806587, 'spline_order_exec': 2, 'n_splines_rchar': 15, 'lam_rchar': 947.9257358633946, 'spline_order_rchar': 3, 'n_splines_wchar': 73, 'lam_wchar': 4.491966599028597, 'spline_order_wchar': 2, 'n_splines_pgout': 100, 'lam_pgout': 0.04546199403336469, 'spline_order_pgout': 3, 'n_splines_ppgout': 26, 'lam_ppgout': 0.007448487523375738, 'spline_order_ppgout': 5, 'n_splines_pgfree': 65, 'lam_pgfree': 0.05110303889478881, 'spline_order_pgfree': 5, 'n_splines_pgscan': 94, 'lam_pgscan': 0.2935077554485958, 'spline_order_pgscan': 1, 'n_splines_atch': 89, 'lam_atch': 0.16860237332440478, 'spline_order_atch': 3, 'n_splines_pgin': 29, 'lam_pgin': 0.026671655636994016, 'spline_order_pgin': 5, 'n_splines_ppgin': 93, 'lam_ppgin': 10.694986646942217, 'spline_order_ppgin': 2, 'n_splines_pflt': 95, 'lam_pflt': 3.251627863430497, 'spline_order_pflt': 1, 'n_splines_vflt': 83, 'lam_vflt': 0.004613710157443668, 'spline_order_vflt': 5, 'n_splines_runqsz': 26, 'lam_runqsz': 713.0772418983304, 'spline_order_runqsz': 4, 'n_splines_freemem': 79, 'lam_freemem': 0.9734845456410806, 'spline_order_freemem': 3, 'n_splines_freeswap': 79, 'lam_freeswap': 45.25135556052211, 'spline_order_freeswap': 2}. Best is trial 84 with value: 2.5407100180963624.\n",
      "[I 2024-03-27 21:42:39,719] Trial 85 finished with value: 4.014728954897685 and parameters: {'n_splines_lread': 42, 'lam_lread': 265.25483500328846, 'spline_order_lread': 5, 'n_splines_lwrite': 62, 'lam_lwrite': 0.09035575979259601, 'spline_order_lwrite': 1, 'n_splines_scall': 95, 'lam_scall': 0.21418179347913446, 'spline_order_scall': 3, 'n_splines_sread': 15, 'lam_sread': 61.20485652180156, 'spline_order_sread': 4, 'n_splines_swrite': 38, 'lam_swrite': 622.7921470542349, 'spline_order_swrite': 3, 'n_splines_fork': 85, 'lam_fork': 0.06582496081780359, 'spline_order_fork': 3, 'n_splines_exec': 35, 'lam_exec': 12.681152660060244, 'spline_order_exec': 2, 'n_splines_rchar': 15, 'lam_rchar': 463.9493487779572, 'spline_order_rchar': 3, 'n_splines_wchar': 72, 'lam_wchar': 3.731380456508206, 'spline_order_wchar': 2, 'n_splines_pgout': 95, 'lam_pgout': 0.046393780290037163, 'spline_order_pgout': 3, 'n_splines_ppgout': 18, 'lam_ppgout': 0.0024839379452351066, 'spline_order_ppgout': 5, 'n_splines_pgfree': 75, 'lam_pgfree': 0.06456170462709342, 'spline_order_pgfree': 5, 'n_splines_pgscan': 94, 'lam_pgscan': 0.1361670591436542, 'spline_order_pgscan': 1, 'n_splines_atch': 91, 'lam_atch': 0.04041895163156378, 'spline_order_atch': 3, 'n_splines_pgin': 30, 'lam_pgin': 0.08428385463072434, 'spline_order_pgin': 5, 'n_splines_ppgin': 99, 'lam_ppgin': 11.3896404705392, 'spline_order_ppgin': 2, 'n_splines_pflt': 95, 'lam_pflt': 4.034943948082299, 'spline_order_pflt': 1, 'n_splines_vflt': 83, 'lam_vflt': 0.004623105789221347, 'spline_order_vflt': 5, 'n_splines_runqsz': 34, 'lam_runqsz': 988.9799562625122, 'spline_order_runqsz': 4, 'n_splines_freemem': 79, 'lam_freemem': 1.6806615872066586, 'spline_order_freemem': 3, 'n_splines_freeswap': 74, 'lam_freeswap': 44.00503788291105, 'spline_order_freeswap': 2}. Best is trial 84 with value: 2.5407100180963624.\n",
      "[I 2024-03-27 21:42:51,254] Trial 86 finished with value: 2.631012684549715 and parameters: {'n_splines_lread': 39, 'lam_lread': 135.4211276826575, 'spline_order_lread': 5, 'n_splines_lwrite': 59, 'lam_lwrite': 0.05292760061417441, 'spline_order_lwrite': 1, 'n_splines_scall': 95, 'lam_scall': 0.10738755997334233, 'spline_order_scall': 3, 'n_splines_sread': 11, 'lam_sread': 105.35783171100316, 'spline_order_sread': 4, 'n_splines_swrite': 29, 'lam_swrite': 695.0953361048271, 'spline_order_swrite': 3, 'n_splines_fork': 90, 'lam_fork': 0.026978460519775965, 'spline_order_fork': 3, 'n_splines_exec': 25, 'lam_exec': 2.8302153586278083, 'spline_order_exec': 2, 'n_splines_rchar': 13, 'lam_rchar': 972.2090482589854, 'spline_order_rchar': 3, 'n_splines_wchar': 77, 'lam_wchar': 9.914521107381962, 'spline_order_wchar': 2, 'n_splines_pgout': 98, 'lam_pgout': 0.024644537492732967, 'spline_order_pgout': 3, 'n_splines_ppgout': 25, 'lam_ppgout': 0.012539434422974827, 'spline_order_ppgout': 5, 'n_splines_pgfree': 69, 'lam_pgfree': 0.04804993890007996, 'spline_order_pgfree': 5, 'n_splines_pgscan': 84, 'lam_pgscan': 0.29834029024183384, 'spline_order_pgscan': 1, 'n_splines_atch': 89, 'lam_atch': 0.15426256269413904, 'spline_order_atch': 3, 'n_splines_pgin': 23, 'lam_pgin': 0.045928170775209914, 'spline_order_pgin': 5, 'n_splines_ppgin': 94, 'lam_ppgin': 3.1517367870879043, 'spline_order_ppgin': 2, 'n_splines_pflt': 88, 'lam_pflt': 13.169782317388385, 'spline_order_pflt': 3, 'n_splines_vflt': 86, 'lam_vflt': 0.0021114839199509756, 'spline_order_vflt': 5, 'n_splines_runqsz': 26, 'lam_runqsz': 578.3232001728213, 'spline_order_runqsz': 4, 'n_splines_freemem': 77, 'lam_freemem': 3.082610104036906, 'spline_order_freemem': 3, 'n_splines_freeswap': 80, 'lam_freeswap': 105.28090278554734, 'spline_order_freeswap': 2}. Best is trial 84 with value: 2.5407100180963624.\n",
      "[I 2024-03-27 21:43:02,801] Trial 87 finished with value: 10.544472628176946 and parameters: {'n_splines_lread': 38, 'lam_lread': 130.82348081687124, 'spline_order_lread': 5, 'n_splines_lwrite': 51, 'lam_lwrite': 0.05614255325966254, 'spline_order_lwrite': 1, 'n_splines_scall': 99, 'lam_scall': 0.14904567941259142, 'spline_order_scall': 3, 'n_splines_sread': 13, 'lam_sread': 101.43285566513605, 'spline_order_sread': 4, 'n_splines_swrite': 28, 'lam_swrite': 761.7462294622884, 'spline_order_swrite': 3, 'n_splines_fork': 90, 'lam_fork': 0.021918375785823972, 'spline_order_fork': 3, 'n_splines_exec': 25, 'lam_exec': 6.925540228617301, 'spline_order_exec': 2, 'n_splines_rchar': 13, 'lam_rchar': 654.2774005321088, 'spline_order_rchar': 3, 'n_splines_wchar': 77, 'lam_wchar': 11.733200076843174, 'spline_order_wchar': 2, 'n_splines_pgout': 90, 'lam_pgout': 0.0076839872132795405, 'spline_order_pgout': 3, 'n_splines_ppgout': 25, 'lam_ppgout': 0.014513536266564757, 'spline_order_ppgout': 5, 'n_splines_pgfree': 72, 'lam_pgfree': 0.10998135160704642, 'spline_order_pgfree': 5, 'n_splines_pgscan': 88, 'lam_pgscan': 0.3051052571775903, 'spline_order_pgscan': 1, 'n_splines_atch': 89, 'lam_atch': 0.08317568989765255, 'spline_order_atch': 3, 'n_splines_pgin': 17, 'lam_pgin': 0.16517707809406726, 'spline_order_pgin': 4, 'n_splines_ppgin': 93, 'lam_ppgin': 2.7286415686160352, 'spline_order_ppgin': 2, 'n_splines_pflt': 89, 'lam_pflt': 9.73898842255427, 'spline_order_pflt': 3, 'n_splines_vflt': 91, 'lam_vflt': 0.0011893032674513578, 'spline_order_vflt': 5, 'n_splines_runqsz': 41, 'lam_runqsz': 729.1472761525653, 'spline_order_runqsz': 4, 'n_splines_freemem': 83, 'lam_freemem': 0.9351004988421225, 'spline_order_freemem': 3, 'n_splines_freeswap': 79, 'lam_freeswap': 71.02998464382785, 'spline_order_freeswap': 2}. Best is trial 84 with value: 2.5407100180963624.\n",
      "[I 2024-03-27 21:43:15,337] Trial 88 finished with value: 2.8674395778553636 and parameters: {'n_splines_lread': 45, 'lam_lread': 388.39863144208215, 'spline_order_lread': 5, 'n_splines_lwrite': 60, 'lam_lwrite': 0.11133672225551491, 'spline_order_lwrite': 1, 'n_splines_scall': 94, 'lam_scall': 0.10065185546815188, 'spline_order_scall': 3, 'n_splines_sread': 12, 'lam_sread': 24.68536182798306, 'spline_order_sread': 4, 'n_splines_swrite': 19, 'lam_swrite': 304.85873983884534, 'spline_order_swrite': 3, 'n_splines_fork': 87, 'lam_fork': 0.010076625667041423, 'spline_order_fork': 3, 'n_splines_exec': 15, 'lam_exec': 3.7926110248933136, 'spline_order_exec': 2, 'n_splines_rchar': 23, 'lam_rchar': 933.3325838089977, 'spline_order_rchar': 3, 'n_splines_wchar': 66, 'lam_wchar': 34.40546661403147, 'spline_order_wchar': 2, 'n_splines_pgout': 98, 'lam_pgout': 0.10746632387947669, 'spline_order_pgout': 3, 'n_splines_ppgout': 24, 'lam_ppgout': 0.007432419236067146, 'spline_order_ppgout': 5, 'n_splines_pgfree': 69, 'lam_pgfree': 0.07646380027635905, 'spline_order_pgfree': 5, 'n_splines_pgscan': 84, 'lam_pgscan': 0.261341941256147, 'spline_order_pgscan': 1, 'n_splines_atch': 95, 'lam_atch': 0.1614703615282677, 'spline_order_atch': 3, 'n_splines_pgin': 28, 'lam_pgin': 0.04091163194424548, 'spline_order_pgin': 1, 'n_splines_ppgin': 98, 'lam_ppgin': 3.582998969209357, 'spline_order_ppgin': 2, 'n_splines_pflt': 94, 'lam_pflt': 13.972591418293948, 'spline_order_pflt': 3, 'n_splines_vflt': 85, 'lam_vflt': 0.004736507038506735, 'spline_order_vflt': 5, 'n_splines_runqsz': 29, 'lam_runqsz': 610.2987540586081, 'spline_order_runqsz': 4, 'n_splines_freemem': 86, 'lam_freemem': 5.768439820859756, 'spline_order_freemem': 3, 'n_splines_freeswap': 83, 'lam_freeswap': 6.801617954311337, 'spline_order_freeswap': 2}. Best is trial 84 with value: 2.5407100180963624.\n",
      "[I 2024-03-27 21:43:28,145] Trial 89 finished with value: 3.1834895440505946 and parameters: {'n_splines_lread': 48, 'lam_lread': 66.3912418538961, 'spline_order_lread': 5, 'n_splines_lwrite': 59, 'lam_lwrite': 0.26986650549425945, 'spline_order_lwrite': 1, 'n_splines_scall': 97, 'lam_scall': 0.10715350595808203, 'spline_order_scall': 3, 'n_splines_sread': 10, 'lam_sread': 32.170536928100454, 'spline_order_sread': 5, 'n_splines_swrite': 19, 'lam_swrite': 317.4117514724638, 'spline_order_swrite': 3, 'n_splines_fork': 98, 'lam_fork': 0.007889594091757334, 'spline_order_fork': 2, 'n_splines_exec': 20, 'lam_exec': 4.022426284565112, 'spline_order_exec': 2, 'n_splines_rchar': 23, 'lam_rchar': 998.4022287409232, 'spline_order_rchar': 3, 'n_splines_wchar': 58, 'lam_wchar': 31.20361075502185, 'spline_order_wchar': 2, 'n_splines_pgout': 100, 'lam_pgout': 0.021501293102465135, 'spline_order_pgout': 3, 'n_splines_ppgout': 23, 'lam_ppgout': 0.009748331077904892, 'spline_order_ppgout': 5, 'n_splines_pgfree': 69, 'lam_pgfree': 0.05150187069762035, 'spline_order_pgfree': 5, 'n_splines_pgscan': 85, 'lam_pgscan': 0.2719397606050895, 'spline_order_pgscan': 1, 'n_splines_atch': 96, 'lam_atch': 0.046788345875625865, 'spline_order_atch': 3, 'n_splines_pgin': 23, 'lam_pgin': 0.042650534889111004, 'spline_order_pgin': 1, 'n_splines_ppgin': 100, 'lam_ppgin': 0.9638672407751278, 'spline_order_ppgin': 3, 'n_splines_pflt': 87, 'lam_pflt': 20.55644639625423, 'spline_order_pflt': 3, 'n_splines_vflt': 86, 'lam_vflt': 0.004239525956409114, 'spline_order_vflt': 5, 'n_splines_runqsz': 26, 'lam_runqsz': 527.7248115633159, 'spline_order_runqsz': 4, 'n_splines_freemem': 86, 'lam_freemem': 20.606825259986365, 'spline_order_freemem': 3, 'n_splines_freeswap': 69, 'lam_freeswap': 80.56666609529165, 'spline_order_freeswap': 2}. Best is trial 84 with value: 2.5407100180963624.\n",
      "[I 2024-03-27 21:43:41,336] Trial 90 finished with value: 3.2721994399676255 and parameters: {'n_splines_lread': 49, 'lam_lread': 77.31666663274147, 'spline_order_lread': 5, 'n_splines_lwrite': 59, 'lam_lwrite': 0.31452040415029253, 'spline_order_lwrite': 1, 'n_splines_scall': 96, 'lam_scall': 0.02281616501833384, 'spline_order_scall': 3, 'n_splines_sread': 10, 'lam_sread': 32.55078029007131, 'spline_order_sread': 5, 'n_splines_swrite': 16, 'lam_swrite': 304.5145859249441, 'spline_order_swrite': 3, 'n_splines_fork': 93, 'lam_fork': 0.005935326682790535, 'spline_order_fork': 2, 'n_splines_exec': 20, 'lam_exec': 3.8652138959932687, 'spline_order_exec': 2, 'n_splines_rchar': 25, 'lam_rchar': 435.6082780746001, 'spline_order_rchar': 3, 'n_splines_wchar': 57, 'lam_wchar': 33.35185034895016, 'spline_order_wchar': 2, 'n_splines_pgout': 99, 'lam_pgout': 0.01689886251204295, 'spline_order_pgout': 3, 'n_splines_ppgout': 23, 'lam_ppgout': 0.0033221740546862852, 'spline_order_ppgout': 5, 'n_splines_pgfree': 67, 'lam_pgfree': 0.047989722996372323, 'spline_order_pgfree': 5, 'n_splines_pgscan': 75, 'lam_pgscan': 0.07917157799756129, 'spline_order_pgscan': 1, 'n_splines_atch': 100, 'lam_atch': 0.025449537068882366, 'spline_order_atch': 3, 'n_splines_pgin': 22, 'lam_pgin': 0.12868067623865775, 'spline_order_pgin': 1, 'n_splines_ppgin': 98, 'lam_ppgin': 1.091216085725897, 'spline_order_ppgin': 3, 'n_splines_pflt': 79, 'lam_pflt': 16.436694223551093, 'spline_order_pflt': 3, 'n_splines_vflt': 94, 'lam_vflt': 0.0017045292780041219, 'spline_order_vflt': 5, 'n_splines_runqsz': 26, 'lam_runqsz': 562.3361767101817, 'spline_order_runqsz': 4, 'n_splines_freemem': 85, 'lam_freemem': 15.171334445944002, 'spline_order_freemem': 3, 'n_splines_freeswap': 63, 'lam_freeswap': 251.5454325934352, 'spline_order_freeswap': 2}. Best is trial 84 with value: 2.5407100180963624.\n",
      "[I 2024-03-27 21:43:55,222] Trial 91 finished with value: 3.6419693183605943 and parameters: {'n_splines_lread': 49, 'lam_lread': 65.10583412176432, 'spline_order_lread': 5, 'n_splines_lwrite': 59, 'lam_lwrite': 0.3134348592402455, 'spline_order_lwrite': 1, 'n_splines_scall': 96, 'lam_scall': 0.020082825217973995, 'spline_order_scall': 3, 'n_splines_sread': 10, 'lam_sread': 30.694667867162888, 'spline_order_sread': 5, 'n_splines_swrite': 17, 'lam_swrite': 314.2055123959085, 'spline_order_swrite': 3, 'n_splines_fork': 97, 'lam_fork': 0.004305711158460853, 'spline_order_fork': 2, 'n_splines_exec': 20, 'lam_exec': 3.9041235432091903, 'spline_order_exec': 2, 'n_splines_rchar': 27, 'lam_rchar': 996.9968335690589, 'spline_order_rchar': 3, 'n_splines_wchar': 55, 'lam_wchar': 48.48467458248775, 'spline_order_wchar': 2, 'n_splines_pgout': 100, 'lam_pgout': 0.019830690657474625, 'spline_order_pgout': 3, 'n_splines_ppgout': 33, 'lam_ppgout': 0.0015200373990282919, 'spline_order_ppgout': 5, 'n_splines_pgfree': 69, 'lam_pgfree': 0.046750536518969404, 'spline_order_pgfree': 5, 'n_splines_pgscan': 74, 'lam_pgscan': 0.06535011613147987, 'spline_order_pgscan': 1, 'n_splines_atch': 99, 'lam_atch': 0.0277644170565257, 'spline_order_atch': 3, 'n_splines_pgin': 22, 'lam_pgin': 0.045614286517759756, 'spline_order_pgin': 1, 'n_splines_ppgin': 100, 'lam_ppgin': 0.9593736597416825, 'spline_order_ppgin': 3, 'n_splines_pflt': 81, 'lam_pflt': 38.13173197929995, 'spline_order_pflt': 3, 'n_splines_vflt': 88, 'lam_vflt': 0.0017796562175773393, 'spline_order_vflt': 5, 'n_splines_runqsz': 26, 'lam_runqsz': 521.1588677442254, 'spline_order_runqsz': 4, 'n_splines_freemem': 86, 'lam_freemem': 14.906729946212007, 'spline_order_freemem': 3, 'n_splines_freeswap': 65, 'lam_freeswap': 185.164717435823, 'spline_order_freeswap': 2}. Best is trial 84 with value: 2.5407100180963624.\n",
      "[I 2024-03-27 21:44:06,841] Trial 92 finished with value: 7.587486976809056 and parameters: {'n_splines_lread': 45, 'lam_lread': 161.12205948221305, 'spline_order_lread': 5, 'n_splines_lwrite': 51, 'lam_lwrite': 0.025500656438624907, 'spline_order_lwrite': 1, 'n_splines_scall': 97, 'lam_scall': 0.03347729495259354, 'spline_order_scall': 3, 'n_splines_sread': 13, 'lam_sread': 17.090483179670926, 'spline_order_sread': 5, 'n_splines_swrite': 17, 'lam_swrite': 378.6653786157057, 'spline_order_swrite': 3, 'n_splines_fork': 93, 'lam_fork': 0.007386323332132238, 'spline_order_fork': 2, 'n_splines_exec': 23, 'lam_exec': 26.705869352610208, 'spline_order_exec': 2, 'n_splines_rchar': 24, 'lam_rchar': 432.7084727382921, 'spline_order_rchar': 3, 'n_splines_wchar': 57, 'lam_wchar': 36.7367535772578, 'spline_order_wchar': 2, 'n_splines_pgout': 100, 'lam_pgout': 0.004500913470322493, 'spline_order_pgout': 3, 'n_splines_ppgout': 23, 'lam_ppgout': 0.0032825881142405033, 'spline_order_ppgout': 5, 'n_splines_pgfree': 69, 'lam_pgfree': 0.025782001471235137, 'spline_order_pgfree': 5, 'n_splines_pgscan': 84, 'lam_pgscan': 0.02716524784988311, 'spline_order_pgscan': 1, 'n_splines_atch': 95, 'lam_atch': 0.005355734315783542, 'spline_order_atch': 3, 'n_splines_pgin': 26, 'lam_pgin': 0.1208283562087269, 'spline_order_pgin': 1, 'n_splines_ppgin': 98, 'lam_ppgin': 0.7162750486448842, 'spline_order_ppgin': 3, 'n_splines_pflt': 86, 'lam_pflt': 17.963990207022835, 'spline_order_pflt': 3, 'n_splines_vflt': 86, 'lam_vflt': 0.004299921683943983, 'spline_order_vflt': 5, 'n_splines_runqsz': 29, 'lam_runqsz': 317.6892569267431, 'spline_order_runqsz': 4, 'n_splines_freemem': 86, 'lam_freemem': 7.517208321371601, 'spline_order_freemem': 3, 'n_splines_freeswap': 69, 'lam_freeswap': 82.42890499959967, 'spline_order_freeswap': 2}. Best is trial 84 with value: 2.5407100180963624.\n",
      "[I 2024-03-27 21:44:19,072] Trial 93 finished with value: 9.001690837460735 and parameters: {'n_splines_lread': 54, 'lam_lread': 84.0297686528428, 'spline_order_lread': 5, 'n_splines_lwrite': 57, 'lam_lwrite': 0.21462618019986845, 'spline_order_lwrite': 1, 'n_splines_scall': 93, 'lam_scall': 0.00527700109773819, 'spline_order_scall': 3, 'n_splines_sread': 11, 'lam_sread': 40.387977518939735, 'spline_order_sread': 5, 'n_splines_swrite': 11, 'lam_swrite': 236.07609924401135, 'spline_order_swrite': 3, 'n_splines_fork': 100, 'lam_fork': 0.012562171445962715, 'spline_order_fork': 2, 'n_splines_exec': 18, 'lam_exec': 12.936931412419236, 'spline_order_exec': 2, 'n_splines_rchar': 21, 'lam_rchar': 701.457885010766, 'spline_order_rchar': 3, 'n_splines_wchar': 60, 'lam_wchar': 24.144577690848426, 'spline_order_wchar': 2, 'n_splines_pgout': 98, 'lam_pgout': 0.001667748483705234, 'spline_order_pgout': 3, 'n_splines_ppgout': 28, 'lam_ppgout': 0.008081912317516593, 'spline_order_ppgout': 5, 'n_splines_pgfree': 81, 'lam_pgfree': 0.0679984439867156, 'spline_order_pgfree': 5, 'n_splines_pgscan': 80, 'lam_pgscan': 0.09708469245368587, 'spline_order_pgscan': 1, 'n_splines_atch': 94, 'lam_atch': 0.018336006556245128, 'spline_order_atch': 3, 'n_splines_pgin': 22, 'lam_pgin': 0.24674866376485347, 'spline_order_pgin': 1, 'n_splines_ppgin': 95, 'lam_ppgin': 0.18807472627083102, 'spline_order_ppgin': 3, 'n_splines_pflt': 92, 'lam_pflt': 8.10199372951692, 'spline_order_pflt': 3, 'n_splines_vflt': 94, 'lam_vflt': 0.0010130142981920464, 'spline_order_vflt': 5, 'n_splines_runqsz': 36, 'lam_runqsz': 530.475412659326, 'spline_order_runqsz': 4, 'n_splines_freemem': 81, 'lam_freemem': 3.576654237489926, 'spline_order_freemem': 3, 'n_splines_freeswap': 73, 'lam_freeswap': 344.6480673665883, 'spline_order_freeswap': 2}. Best is trial 84 with value: 2.5407100180963624.\n",
      "[I 2024-03-27 21:44:30,319] Trial 94 finished with value: 4.141257893981587 and parameters: {'n_splines_lread': 47, 'lam_lread': 27.847032808697538, 'spline_order_lread': 5, 'n_splines_lwrite': 60, 'lam_lwrite': 0.31786679525022277, 'spline_order_lwrite': 1, 'n_splines_scall': 100, 'lam_scall': 0.05371531797814012, 'spline_order_scall': 3, 'n_splines_sread': 12, 'lam_sread': 7.260790507054981, 'spline_order_sread': 5, 'n_splines_swrite': 14, 'lam_swrite': 69.31203790140792, 'spline_order_swrite': 3, 'n_splines_fork': 95, 'lam_fork': 0.001372851744568155, 'spline_order_fork': 2, 'n_splines_exec': 26, 'lam_exec': 1.4992918960592307, 'spline_order_exec': 2, 'n_splines_rchar': 12, 'lam_rchar': 997.8190643914173, 'spline_order_rchar': 3, 'n_splines_wchar': 66, 'lam_wchar': 125.48705590845037, 'spline_order_wchar': 2, 'n_splines_pgout': 92, 'lam_pgout': 0.02637775607113461, 'spline_order_pgout': 3, 'n_splines_ppgout': 25, 'lam_ppgout': 0.019736034227184163, 'spline_order_ppgout': 5, 'n_splines_pgfree': 65, 'lam_pgfree': 0.016943965232294956, 'spline_order_pgfree': 5, 'n_splines_pgscan': 84, 'lam_pgscan': 0.15265650733608432, 'spline_order_pgscan': 1, 'n_splines_atch': 97, 'lam_atch': 0.06680398551793426, 'spline_order_atch': 3, 'n_splines_pgin': 13, 'lam_pgin': 0.027257583810386392, 'spline_order_pgin': 1, 'n_splines_ppgin': 97, 'lam_ppgin': 2.1126536565611924, 'spline_order_ppgin': 3, 'n_splines_pflt': 79, 'lam_pflt': 14.052495295716644, 'spline_order_pflt': 3, 'n_splines_vflt': 94, 'lam_vflt': 0.001965961675071296, 'spline_order_vflt': 5, 'n_splines_runqsz': 32, 'lam_runqsz': 804.3285305523398, 'spline_order_runqsz': 4, 'n_splines_freemem': 77, 'lam_freemem': 9.083139312719211, 'spline_order_freemem': 3, 'n_splines_freeswap': 67, 'lam_freeswap': 605.165139123774, 'spline_order_freeswap': 2}. Best is trial 84 with value: 2.5407100180963624.\n",
      "[I 2024-03-27 21:44:40,926] Trial 95 finished with value: 8.738287702191315 and parameters: {'n_splines_lread': 43, 'lam_lread': 366.5500058826943, 'spline_order_lread': 5, 'n_splines_lwrite': 53, 'lam_lwrite': 0.05749983879616788, 'spline_order_lwrite': 1, 'n_splines_scall': 87, 'lam_scall': 0.10788925249203667, 'spline_order_scall': 3, 'n_splines_sread': 15, 'lam_sread': 50.07975504767467, 'spline_order_sread': 5, 'n_splines_swrite': 26, 'lam_swrite': 100.33259719375572, 'spline_order_swrite': 3, 'n_splines_fork': 88, 'lam_fork': 0.024364091010107596, 'spline_order_fork': 1, 'n_splines_exec': 21, 'lam_exec': 4.564086524585599, 'spline_order_exec': 2, 'n_splines_rchar': 18, 'lam_rchar': 631.6047551630693, 'spline_order_rchar': 3, 'n_splines_wchar': 62, 'lam_wchar': 98.13722927642952, 'spline_order_wchar': 3, 'n_splines_pgout': 96, 'lam_pgout': 0.012046657506968745, 'spline_order_pgout': 3, 'n_splines_ppgout': 17, 'lam_ppgout': 0.011661020069014873, 'spline_order_ppgout': 5, 'n_splines_pgfree': 57, 'lam_pgfree': 0.03017459079121823, 'spline_order_pgfree': 5, 'n_splines_pgscan': 87, 'lam_pgscan': 0.2546125403964165, 'spline_order_pgscan': 1, 'n_splines_atch': 100, 'lam_atch': 0.008012311295918454, 'spline_order_atch': 3, 'n_splines_pgin': 29, 'lam_pgin': 0.06562133486036315, 'spline_order_pgin': 1, 'n_splines_ppgin': 91, 'lam_ppgin': 4.98300901912173, 'spline_order_ppgin': 3, 'n_splines_pflt': 87, 'lam_pflt': 19.792080401125215, 'spline_order_pflt': 3, 'n_splines_vflt': 85, 'lam_vflt': 0.003661276456374458, 'spline_order_vflt': 5, 'n_splines_runqsz': 26, 'lam_runqsz': 218.105532356996, 'spline_order_runqsz': 4, 'n_splines_freemem': 89, 'lam_freemem': 19.705914232112498, 'spline_order_freemem': 2, 'n_splines_freeswap': 71, 'lam_freeswap': 234.91152645442614, 'spline_order_freeswap': 2}. Best is trial 84 with value: 2.5407100180963624.\n",
      "[I 2024-03-27 21:44:52,658] Trial 96 finished with value: 6.038474589906804 and parameters: {'n_splines_lread': 49, 'lam_lread': 161.59553120104337, 'spline_order_lread': 5, 'n_splines_lwrite': 49, 'lam_lwrite': 0.013049644777400797, 'spline_order_lwrite': 1, 'n_splines_scall': 89, 'lam_scall': 0.013590042341880687, 'spline_order_scall': 3, 'n_splines_sread': 16, 'lam_sread': 20.899449684339928, 'spline_order_sread': 5, 'n_splines_swrite': 22, 'lam_swrite': 468.79764326033626, 'spline_order_swrite': 3, 'n_splines_fork': 98, 'lam_fork': 0.0018876863199876169, 'spline_order_fork': 3, 'n_splines_exec': 28, 'lam_exec': 2.5847001480127156, 'spline_order_exec': 2, 'n_splines_rchar': 32, 'lam_rchar': 427.99771995116424, 'spline_order_rchar': 3, 'n_splines_wchar': 58, 'lam_wchar': 346.63725316891345, 'spline_order_wchar': 2, 'n_splines_pgout': 93, 'lam_pgout': 0.08481512890379798, 'spline_order_pgout': 3, 'n_splines_ppgout': 31, 'lam_ppgout': 0.0037789597234094375, 'spline_order_ppgout': 5, 'n_splines_pgfree': 68, 'lam_pgfree': 0.05176965942169143, 'spline_order_pgfree': 5, 'n_splines_pgscan': 82, 'lam_pgscan': 0.6123481620411341, 'spline_order_pgscan': 1, 'n_splines_atch': 93, 'lam_atch': 0.04092218573388274, 'spline_order_atch': 3, 'n_splines_pgin': 35, 'lam_pgin': 0.01703390215288079, 'spline_order_pgin': 1, 'n_splines_ppgin': 100, 'lam_ppgin': 3.713307021518678, 'spline_order_ppgin': 2, 'n_splines_pflt': 90, 'lam_pflt': 96.48021934401874, 'spline_order_pflt': 3, 'n_splines_vflt': 98, 'lam_vflt': 0.005745954557056265, 'spline_order_vflt': 5, 'n_splines_runqsz': 30, 'lam_runqsz': 490.7675020206611, 'spline_order_runqsz': 4, 'n_splines_freemem': 81, 'lam_freemem': 113.37848718866569, 'spline_order_freemem': 3, 'n_splines_freeswap': 62, 'lam_freeswap': 98.06278852682698, 'spline_order_freeswap': 2}. Best is trial 84 with value: 2.5407100180963624.\n",
      "[I 2024-03-27 21:45:02,968] Trial 97 finished with value: 17.0003483134411 and parameters: {'n_splines_lread': 41, 'lam_lread': 527.8852863063818, 'spline_order_lread': 5, 'n_splines_lwrite': 62, 'lam_lwrite': 0.0058626688128453315, 'spline_order_lwrite': 1, 'n_splines_scall': 94, 'lam_scall': 0.07127031841840945, 'spline_order_scall': 3, 'n_splines_sread': 10, 'lam_sread': 155.5443038829146, 'spline_order_sread': 4, 'n_splines_swrite': 20, 'lam_swrite': 820.1655042302151, 'spline_order_swrite': 3, 'n_splines_fork': 90, 'lam_fork': 0.007032124981635395, 'spline_order_fork': 2, 'n_splines_exec': 16, 'lam_exec': 18.890598886359747, 'spline_order_exec': 2, 'n_splines_rchar': 25, 'lam_rchar': 387.1036110364273, 'spline_order_rchar': 3, 'n_splines_wchar': 51, 'lam_wchar': 17.2517757287012, 'spline_order_wchar': 2, 'n_splines_pgout': 85, 'lam_pgout': 0.04544080931370975, 'spline_order_pgout': 2, 'n_splines_ppgout': 13, 'lam_ppgout': 0.011517475451934411, 'spline_order_ppgout': 5, 'n_splines_pgfree': 62, 'lam_pgfree': 0.13046685287928664, 'spline_order_pgfree': 5, 'n_splines_pgscan': 75, 'lam_pgscan': 0.3775393766501367, 'spline_order_pgscan': 1, 'n_splines_atch': 88, 'lam_atch': 0.04797240290333268, 'spline_order_atch': 3, 'n_splines_pgin': 32, 'lam_pgin': 0.3754132812836615, 'spline_order_pgin': 1, 'n_splines_ppgin': 94, 'lam_ppgin': 1.2324307017575749, 'spline_order_ppgin': 3, 'n_splines_pflt': 95, 'lam_pflt': 30.009969834916667, 'spline_order_pflt': 3, 'n_splines_vflt': 90, 'lam_vflt': 0.008865426566604719, 'spline_order_vflt': 5, 'n_splines_runqsz': 40, 'lam_runqsz': 295.4142918075433, 'spline_order_runqsz': 4, 'n_splines_freemem': 84, 'lam_freemem': 27.66014156082196, 'spline_order_freemem': 3, 'n_splines_freeswap': 76, 'lam_freeswap': 142.23145884971882, 'spline_order_freeswap': 2}. Best is trial 84 with value: 2.5407100180963624.\n",
      "[I 2024-03-27 21:45:13,928] Trial 98 finished with value: 2.934323260128551 and parameters: {'n_splines_lread': 45, 'lam_lread': 283.1110139448458, 'spline_order_lread': 5, 'n_splines_lwrite': 55, 'lam_lwrite': 0.15789036674373344, 'spline_order_lwrite': 1, 'n_splines_scall': 86, 'lam_scall': 0.3166239826292547, 'spline_order_scall': 3, 'n_splines_sread': 20, 'lam_sread': 11.087516782204702, 'spline_order_sread': 5, 'n_splines_swrite': 23, 'lam_swrite': 168.97923230713428, 'spline_order_swrite': 3, 'n_splines_fork': 86, 'lam_fork': 0.015169701750531419, 'spline_order_fork': 3, 'n_splines_exec': 24, 'lam_exec': 37.693718776987275, 'spline_order_exec': 1, 'n_splines_rchar': 21, 'lam_rchar': 180.45737637603204, 'spline_order_rchar': 3, 'n_splines_wchar': 53, 'lam_wchar': 8.90654485186942, 'spline_order_wchar': 2, 'n_splines_pgout': 98, 'lam_pgout': 0.11171044000639832, 'spline_order_pgout': 3, 'n_splines_ppgout': 72, 'lam_ppgout': 0.007187344000626664, 'spline_order_ppgout': 5, 'n_splines_pgfree': 20, 'lam_pgfree': 0.19226886029273102, 'spline_order_pgfree': 5, 'n_splines_pgscan': 90, 'lam_pgscan': 0.1984239285874135, 'spline_order_pgscan': 1, 'n_splines_atch': 82, 'lam_atch': 0.12256351686867191, 'spline_order_atch': 4, 'n_splines_pgin': 18, 'lam_pgin': 0.14645794605090212, 'spline_order_pgin': 2, 'n_splines_ppgin': 10, 'lam_ppgin': 7.4462043292042, 'spline_order_ppgin': 4, 'n_splines_pflt': 84, 'lam_pflt': 4.411478497958091, 'spline_order_pflt': 4, 'n_splines_vflt': 93, 'lam_vflt': 0.0014125003975551038, 'spline_order_vflt': 5, 'n_splines_runqsz': 27, 'lam_runqsz': 603.8956891122926, 'spline_order_runqsz': 4, 'n_splines_freemem': 74, 'lam_freemem': 213.36419871538234, 'spline_order_freemem': 3, 'n_splines_freeswap': 82, 'lam_freeswap': 54.79168621743163, 'spline_order_freeswap': 1}. Best is trial 84 with value: 2.5407100180963624.\n",
      "[I 2024-03-27 21:45:26,931] Trial 99 finished with value: 2.5948248011291017 and parameters: {'n_splines_lread': 46, 'lam_lread': 993.0049471611525, 'spline_order_lread': 5, 'n_splines_lwrite': 54, 'lam_lwrite': 0.12988246443496507, 'spline_order_lwrite': 1, 'n_splines_scall': 86, 'lam_scall': 0.355343630006744, 'spline_order_scall': 3, 'n_splines_sread': 19, 'lam_sread': 11.146365969723606, 'spline_order_sread': 5, 'n_splines_swrite': 24, 'lam_swrite': 154.66551672456862, 'spline_order_swrite': 3, 'n_splines_fork': 96, 'lam_fork': 0.0355272131783762, 'spline_order_fork': 3, 'n_splines_exec': 31, 'lam_exec': 7.185144437645391, 'spline_order_exec': 1, 'n_splines_rchar': 31, 'lam_rchar': 191.2052776073747, 'spline_order_rchar': 3, 'n_splines_wchar': 48, 'lam_wchar': 9.265795796040049, 'spline_order_wchar': 2, 'n_splines_pgout': 97, 'lam_pgout': 0.12751085695200232, 'spline_order_pgout': 3, 'n_splines_ppgout': 85, 'lam_ppgout': 0.0022914499732702705, 'spline_order_ppgout': 5, 'n_splines_pgfree': 73, 'lam_pgfree': 0.18547365788308864, 'spline_order_pgfree': 5, 'n_splines_pgscan': 79, 'lam_pgscan': 0.19570252322600198, 'spline_order_pgscan': 1, 'n_splines_atch': 83, 'lam_atch': 0.1208379287760051, 'spline_order_atch': 4, 'n_splines_pgin': 23, 'lam_pgin': 0.08957392650045637, 'spline_order_pgin': 2, 'n_splines_ppgin': 98, 'lam_ppgin': 8.138803545187203, 'spline_order_ppgin': 4, 'n_splines_pflt': 93, 'lam_pflt': 4.573566979904194, 'spline_order_pflt': 4, 'n_splines_vflt': 86, 'lam_vflt': 0.0014514020648068592, 'spline_order_vflt': 5, 'n_splines_runqsz': 27, 'lam_runqsz': 582.5043669307455, 'spline_order_runqsz': 4, 'n_splines_freemem': 87, 'lam_freemem': 236.66725651019365, 'spline_order_freemem': 3, 'n_splines_freeswap': 82, 'lam_freeswap': 52.66105770105229, 'spline_order_freeswap': 1}. Best is trial 84 with value: 2.5407100180963624.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE GAM:  40.98465473045525\n"
     ]
    }
   ],
   "source": [
    "#### GAM model\n",
    "N_TRIALS=100\n",
    "def gam_model(trial):\n",
    "\n",
    "    n_splines = []\n",
    "    lam = []\n",
    "    spline_order = []\n",
    "\n",
    "    # Iterate over each covariate in X_train_\n",
    "    for col in X_train_.columns:\n",
    "        # Define the search space for n_splines, lam, and spline_order\n",
    "        n_splines.append(trial.suggest_int(f'n_splines_{col}', 10, 100))\n",
    "        lam.append(trial.suggest_float(f'lam_{col}', 1e-3, 1e3, log=True))\n",
    "        spline_order.append(trial.suggest_int(f'spline_order_{col}', 1, 5))\n",
    "    \n",
    "    # Create and train the model\n",
    "    gam = LinearGAM(n_splines=n_splines, spline_order=spline_order, lam=lam).fit(X_train_, y_train_)\n",
    "\n",
    "    # Predict on the validation set and calculate the RMSE\n",
    "    y_val_hat_gam = gam.predict(X_val)\n",
    "    RMSE_gam = np.sqrt(np.mean((y_val - y_val_hat_gam) ** 2))\n",
    "\n",
    "    return RMSE_gam\n",
    "\n",
    "# Create the sampler and study\n",
    "sampler_gam = optuna.samplers.TPESampler(seed=seed)\n",
    "study_gam = optuna.create_study(sampler=sampler_gam, direction='minimize')\n",
    "\n",
    "# Optimize the model\n",
    "study_gam.optimize(gam_model, n_trials=N_TRIALS)\n",
    "\n",
    "n_splines = []\n",
    "lam = []\n",
    "spline_order = []\n",
    "\n",
    "# Create the final model with the best parameters\n",
    "best_params = study_gam.best_params\n",
    "\n",
    "# Iterate over each covariate in X_train_\n",
    "for col in X_train.columns:\n",
    "    # Define the search space for n_splines, lam, and spline_order\n",
    "    n_splines.append(best_params[f'n_splines_{col}'])\n",
    "    lam.append(best_params[f'lam_{col}'])\n",
    "    spline_order.append(best_params[f'spline_order_{col}'])\n",
    "\n",
    "final_gam_model = LinearGAM(n_splines=n_splines, spline_order=spline_order, lam=lam)\n",
    "\n",
    "# Fit the model\n",
    "final_gam_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_hat_gam = final_gam_model.predict(X_test)\n",
    "# Calculate the RMSE\n",
    "RMSE_gam = np.sqrt(np.mean((y_test - y_test_hat_gam) ** 2))\n",
    "print(\"RMSE GAM: \", RMSE_gam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-27 21:55:36,796] A new study created in memory with name: no-name-968777f5-72dc-47ba-b1ec-f8b3a77ff081\n",
      "[I 2024-03-27 21:56:01,184] Trial 0 finished with value: 334.1580013815291 and parameters: {'n_splines': 80, 'lam': 0.0013320229150659071, 'spline_order': 4}. Best is trial 0 with value: 334.1580013815291.\n",
      "[I 2024-03-27 21:56:24,102] Trial 1 finished with value: 113.54695068295823 and parameters: {'n_splines': 78, 'lam': 0.9795848815655198, 'spline_order': 2}. Best is trial 1 with value: 113.54695068295823.\n",
      "[I 2024-03-27 21:56:27,245] Trial 2 finished with value: 20.56405329642756 and parameters: {'n_splines': 28, 'lam': 36.57499481192727, 'spline_order': 1}. Best is trial 2 with value: 20.56405329642756.\n",
      "[I 2024-03-27 21:56:29,219] Trial 3 finished with value: 13.648983422833867 and parameters: {'n_splines': 18, 'lam': 12.94669479886693, 'spline_order': 5}. Best is trial 3 with value: 13.648983422833867.\n",
      "[I 2024-03-27 21:56:30,430] Trial 4 finished with value: 9.7905105625559 and parameters: {'n_splines': 10, 'lam': 1.1834599907542849, 'spline_order': 5}. Best is trial 4 with value: 9.7905105625559.\n",
      "[I 2024-03-27 21:56:45,231] Trial 5 finished with value: 29.12630230011885 and parameters: {'n_splines': 65, 'lam': 21.405821746591823, 'spline_order': 2}. Best is trial 4 with value: 9.7905105625559.\n",
      "[I 2024-03-27 21:57:16,797] Trial 6 finished with value: 43.09365119206896 and parameters: {'n_splines': 93, 'lam': 19.384504329781148, 'spline_order': 3}. Best is trial 4 with value: 9.7905105625559.\n",
      "[I 2024-03-27 21:57:19,231] Trial 7 finished with value: 23.91575403040006 and parameters: {'n_splines': 22, 'lam': 0.173797914304444, 'spline_order': 4}. Best is trial 4 with value: 9.7905105625559.\n",
      "[I 2024-03-27 21:57:29,172] Trial 8 finished with value: 73.03519851502111 and parameters: {'n_splines': 50, 'lam': 0.4018684945839337, 'spline_order': 4}. Best is trial 4 with value: 9.7905105625559.\n",
      "[I 2024-03-27 21:57:42,068] Trial 9 finished with value: 30.037348831825312 and parameters: {'n_splines': 56, 'lam': 7.986989097169455, 'spline_order': 4}. Best is trial 4 with value: 9.7905105625559.\n",
      "[I 2024-03-27 21:57:50,342] Trial 10 finished with value: 5.928043680743211 and parameters: {'n_splines': 37, 'lam': 728.7079614451872, 'spline_order': 5}. Best is trial 10 with value: 5.928043680743211.\n",
      "[I 2024-03-27 21:57:55,797] Trial 11 finished with value: 11.307100418018802 and parameters: {'n_splines': 37, 'lam': 283.95319711913504, 'spline_order': 5}. Best is trial 10 with value: 5.928043680743211.\n",
      "[I 2024-03-27 21:57:56,797] Trial 12 finished with value: 37.54546226102234 and parameters: {'n_splines': 11, 'lam': 0.03637458875316231, 'spline_order': 5}. Best is trial 10 with value: 5.928043680743211.\n",
      "[I 2024-03-27 21:58:01,883] Trial 13 finished with value: 10.702891601033084 and parameters: {'n_splines': 36, 'lam': 283.3499553203668, 'spline_order': 5}. Best is trial 10 with value: 5.928043680743211.\n",
      "[I 2024-03-27 21:58:10,405] Trial 14 finished with value: 117.70963840553172 and parameters: {'n_splines': 45, 'lam': 0.015193716994210123, 'spline_order': 3}. Best is trial 10 with value: 5.928043680743211.\n",
      "[I 2024-03-27 21:58:11,264] Trial 15 finished with value: 3.3784244450809098 and parameters: {'n_splines': 10, 'lam': 989.0517899373325, 'spline_order': 5}. Best is trial 15 with value: 3.3784244450809098.\n",
      "[I 2024-03-27 21:58:15,922] Trial 16 finished with value: 5.416235579812869 and parameters: {'n_splines': 31, 'lam': 490.0477434963423, 'spline_order': 4}. Best is trial 15 with value: 3.3784244450809098.\n",
      "[I 2024-03-27 21:58:19,014] Trial 17 finished with value: 2.640915494774266 and parameters: {'n_splines': 26, 'lam': 881.8412278018175, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 21:58:20,708] Trial 18 finished with value: 8.097373243770628 and parameters: {'n_splines': 21, 'lam': 85.29965505568504, 'spline_order': 3}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 21:58:21,510] Trial 19 finished with value: 3.440524844604893 and parameters: {'n_splines': 10, 'lam': 100.61203973914577, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 21:58:34,116] Trial 20 finished with value: 48.77008409886535 and parameters: {'n_splines': 61, 'lam': 3.934837196289502, 'spline_order': 2}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 21:58:35,199] Trial 21 finished with value: 2.913150063098604 and parameters: {'n_splines': 14, 'lam': 994.7532219878275, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 21:58:36,716] Trial 22 finished with value: 3.353610920176988 and parameters: {'n_splines': 18, 'lam': 935.327191259172, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 21:58:39,149] Trial 23 finished with value: 11.678064710235216 and parameters: {'n_splines': 25, 'lam': 87.10113034734314, 'spline_order': 3}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 21:58:40,563] Trial 24 finished with value: 2.724665237648798 and parameters: {'n_splines': 18, 'lam': 185.24821999126885, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 21:58:47,199] Trial 25 finished with value: 18.347677332846942 and parameters: {'n_splines': 43, 'lam': 171.28903744217448, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 21:58:50,343] Trial 26 finished with value: 17.865314975045354 and parameters: {'n_splines': 29, 'lam': 55.174993782803554, 'spline_order': 3}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 21:58:51,918] Trial 27 finished with value: 2.8354713258260857 and parameters: {'n_splines': 19, 'lam': 204.0397620790446, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 21:58:55,917] Trial 28 finished with value: 22.88772177298265 and parameters: {'n_splines': 34, 'lam': 5.746654769229659, 'spline_order': 3}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 21:59:14,319] Trial 29 finished with value: 241.9694883505993 and parameters: {'n_splines': 75, 'lam': 0.0010732211072686285, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 21:59:15,957] Trial 30 finished with value: 98.79323416620295 and parameters: {'n_splines': 19, 'lam': 0.004016374475092312, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 21:59:17,304] Trial 31 finished with value: 2.7554309818712404 and parameters: {'n_splines': 16, 'lam': 194.29041931018122, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 21:59:20,772] Trial 32 finished with value: 5.41718481959667 and parameters: {'n_splines': 26, 'lam': 264.6376825267326, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 21:59:22,542] Trial 33 finished with value: 8.631356777861907 and parameters: {'n_splines': 18, 'lam': 44.633522815617034, 'spline_order': 3}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 21:59:24,991] Trial 34 finished with value: 9.86093149230297 and parameters: {'n_splines': 25, 'lam': 150.42745547151267, 'spline_order': 1}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 21:59:26,410] Trial 35 finished with value: 21.596470461153743 and parameters: {'n_splines': 16, 'lam': 2.730391843275674, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 21:59:34,471] Trial 36 finished with value: 13.343619715429407 and parameters: {'n_splines': 44, 'lam': 378.91292278765485, 'spline_order': 5}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 21:59:39,139] Trial 37 finished with value: 22.70758444146545 and parameters: {'n_splines': 31, 'lam': 20.455640689481392, 'spline_order': 2}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:00:08,027] Trial 38 finished with value: 30.613280691988237 and parameters: {'n_splines': 85, 'lam': 38.86979109443016, 'spline_order': 3}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:00:10,655] Trial 39 finished with value: 19.958416631457794 and parameters: {'n_splines': 22, 'lam': 1.3253912157006928, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:00:12,065] Trial 40 finished with value: 3.3471933913479273 and parameters: {'n_splines': 14, 'lam': 158.93989391312053, 'spline_order': 5}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:00:13,908] Trial 41 finished with value: 3.3044048923921077 and parameters: {'n_splines': 14, 'lam': 496.76631284317597, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:00:16,302] Trial 42 finished with value: 2.680854760347491 and parameters: {'n_splines': 23, 'lam': 494.36208163861477, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:00:18,467] Trial 43 finished with value: 20.307316160763147 and parameters: {'n_splines': 23, 'lam': 12.767232156187957, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:00:21,547] Trial 44 finished with value: 7.921205480588437 and parameters: {'n_splines': 28, 'lam': 208.20715224957863, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:00:27,033] Trial 45 finished with value: 11.826785655784073 and parameters: {'n_splines': 41, 'lam': 442.24516018719817, 'spline_order': 3}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:00:36,226] Trial 46 finished with value: 21.565562612743875 and parameters: {'n_splines': 49, 'lam': 114.09561976611265, 'spline_order': 5}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:00:40,452] Trial 47 finished with value: 19.127279663033406 and parameters: {'n_splines': 34, 'lam': 66.98694310691782, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:00:59,738] Trial 48 finished with value: 105.71284254970817 and parameters: {'n_splines': 71, 'lam': 0.2118447716601109, 'spline_order': 5}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:01:05,263] Trial 49 finished with value: 22.5736149050278 and parameters: {'n_splines': 39, 'lam': 27.197558806339185, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:01:07,811] Trial 50 finished with value: 2.852316802874097 and parameters: {'n_splines': 21, 'lam': 532.401303957126, 'spline_order': 5}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:01:10,521] Trial 51 finished with value: 2.917690914030828 and parameters: {'n_splines': 21, 'lam': 578.8115927434624, 'spline_order': 5}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:01:11,938] Trial 52 finished with value: 3.3243905647860235 and parameters: {'n_splines': 16, 'lam': 315.45215849685195, 'spline_order': 5}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:01:15,853] Trial 53 finished with value: 4.041766603804909 and parameters: {'n_splines': 30, 'lam': 626.1644772195611, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:01:17,005] Trial 54 finished with value: 3.388083548888421 and parameters: {'n_splines': 12, 'lam': 175.36023109745827, 'spline_order': 5}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:01:19,749] Trial 55 finished with value: 3.392308300255533 and parameters: {'n_splines': 24, 'lam': 357.55195287510065, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:01:59,637] Trial 56 finished with value: 22.892536025530415 and parameters: {'n_splines': 98, 'lam': 667.5806395651078, 'spline_order': 3}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:02:02,440] Trial 57 finished with value: 5.491015565529493 and parameters: {'n_splines': 21, 'lam': 99.72148028774504, 'spline_order': 5}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:02:06,193] Trial 58 finished with value: 6.017646977688325 and parameters: {'n_splines': 27, 'lam': 264.97278021262696, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:02:08,253] Trial 59 finished with value: 10.418462466834457 and parameters: {'n_splines': 18, 'lam': 0.04148162803504288, 'spline_order': 1}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:02:15,365] Trial 60 finished with value: 22.343541044109003 and parameters: {'n_splines': 33, 'lam': 11.547192243433049, 'spline_order': 3}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:02:16,474] Trial 61 finished with value: 3.118667783339149 and parameters: {'n_splines': 14, 'lam': 704.0453120628357, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:02:17,462] Trial 62 finished with value: 2.747750732406775 and parameters: {'n_splines': 12, 'lam': 775.782551504809, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:02:18,258] Trial 63 finished with value: 3.1697370859812444 and parameters: {'n_splines': 11, 'lam': 233.12816081835265, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:02:19,807] Trial 64 finished with value: 3.3206769149975726 and parameters: {'n_splines': 19, 'lam': 940.5241513011307, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:02:21,100] Trial 65 finished with value: 4.116389846303039 and parameters: {'n_splines': 16, 'lam': 60.25785256695432, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:02:31,984] Trial 66 finished with value: 22.464170386663692 and parameters: {'n_splines': 57, 'lam': 134.9779998775086, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:02:32,741] Trial 67 finished with value: 2.9169093496866894 and parameters: {'n_splines': 10, 'lam': 418.87182295453295, 'spline_order': 5}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:02:35,038] Trial 68 finished with value: 5.075684729097031 and parameters: {'n_splines': 24, 'lam': 249.79969867085032, 'spline_order': 3}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:02:36,779] Trial 69 finished with value: 12.119744704590603 and parameters: {'n_splines': 20, 'lam': 30.85200952859142, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:02:37,992] Trial 70 finished with value: 4.778026984512315 and parameters: {'n_splines': 16, 'lam': 80.86622744707897, 'spline_order': 2}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:02:39,018] Trial 71 finished with value: 2.9392296514868925 and parameters: {'n_splines': 14, 'lam': 949.2625982766995, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:02:39,899] Trial 72 finished with value: 3.0354659991747317 and parameters: {'n_splines': 12, 'lam': 419.96441147104053, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:02:43,259] Trial 73 finished with value: 2.826781054583495 and parameters: {'n_splines': 27, 'lam': 739.8091817446639, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:02:46,088] Trial 74 finished with value: 3.104553505778882 and parameters: {'n_splines': 27, 'lam': 619.7925589853786, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:02:48,361] Trial 75 finished with value: 5.101974578903295 and parameters: {'n_splines': 23, 'lam': 183.35216566788907, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:02:51,629] Trial 76 finished with value: 6.439829509291732 and parameters: {'n_splines': 30, 'lam': 351.10879903435375, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:02:53,177] Trial 77 finished with value: 3.3296178881336242 and parameters: {'n_splines': 18, 'lam': 985.988478660611, 'spline_order': 5}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:02:55,610] Trial 78 finished with value: 9.069001852009455 and parameters: {'n_splines': 25, 'lam': 114.03842936192818, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:02:59,348] Trial 79 finished with value: 6.007837733875329 and parameters: {'n_splines': 32, 'lam': 534.1713125886322, 'spline_order': 3}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:03:01,729] Trial 80 finished with value: 98.94102256739798 and parameters: {'n_splines': 22, 'lam': 0.0027390447696285527, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:03:02,762] Trial 81 finished with value: 3.365374333816429 and parameters: {'n_splines': 13, 'lam': 320.03683171718666, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:03:03,992] Trial 82 finished with value: 3.3285897257066086 and parameters: {'n_splines': 16, 'lam': 764.7691117752145, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:03:09,678] Trial 83 finished with value: 7.7286096099713655 and parameters: {'n_splines': 36, 'lam': 513.2382931524289, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:03:12,134] Trial 84 finished with value: 3.1394351444994064 and parameters: {'n_splines': 20, 'lam': 205.317208572574, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:03:16,061] Trial 85 finished with value: 3.018919688420376 and parameters: {'n_splines': 28, 'lam': 737.5716354820175, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:03:19,249] Trial 86 finished with value: 3.2282122843454606 and parameters: {'n_splines': 17, 'lam': 475.3306364554088, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:03:21,320] Trial 87 finished with value: 2.909151804405581 and parameters: {'n_splines': 10, 'lam': 131.51795189859277, 'spline_order': 5}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:03:22,657] Trial 88 finished with value: 2.9019056363350804 and parameters: {'n_splines': 10, 'lam': 133.03974645036752, 'spline_order': 5}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:03:24,245] Trial 89 finished with value: 2.707108761821352 and parameters: {'n_splines': 13, 'lam': 47.830566486076236, 'spline_order': 5}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:03:26,810] Trial 90 finished with value: 13.93480835742965 and parameters: {'n_splines': 26, 'lam': 53.272728685956025, 'spline_order': 5}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:03:27,727] Trial 91 finished with value: 3.2076339741533046 and parameters: {'n_splines': 13, 'lam': 330.67706321495143, 'spline_order': 5}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:03:29,397] Trial 92 finished with value: 17.181579541951006 and parameters: {'n_splines': 20, 'lam': 0.6022743377785694, 'spline_order': 5}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:03:31,059] Trial 93 finished with value: 2.892527467129832 and parameters: {'n_splines': 17, 'lam': 229.34916839640246, 'spline_order': 5}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:03:33,165] Trial 94 finished with value: 3.423969688430414 and parameters: {'n_splines': 22, 'lam': 217.03758639222653, 'spline_order': 5}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:03:34,273] Trial 95 finished with value: 3.3869089557850107 and parameters: {'n_splines': 15, 'lam': 261.04178921422016, 'spline_order': 5}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:03:35,717] Trial 96 finished with value: 3.6499122096419727 and parameters: {'n_splines': 18, 'lam': 89.19698306550384, 'spline_order': 5}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:03:38,333] Trial 97 finished with value: 5.562001231563755 and parameters: {'n_splines': 24, 'lam': 164.54358222136202, 'spline_order': 5}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:04:04,765] Trial 98 finished with value: 22.866900390731878 and parameters: {'n_splines': 88, 'lam': 414.2851300394433, 'spline_order': 5}. Best is trial 17 with value: 2.640915494774266.\n",
      "[I 2024-03-27 22:04:07,156] Trial 99 finished with value: 3.2333519998190066 and parameters: {'n_splines': 19, 'lam': 754.1180645694689, 'spline_order': 4}. Best is trial 17 with value: 2.640915494774266.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE GAM:  33.18405963367804\n"
     ]
    }
   ],
   "source": [
    "#### GAM model\n",
    "N_TRIALS=100\n",
    "def gam_model(trial):\n",
    "\n",
    "    # Define the search space for n_splines, lam, and spline_order\n",
    "    params={'n_splines': trial.suggest_int('n_splines', 10, 100),\n",
    "    'lam':trial.suggest_float('lam', 1e-3, 1e3, log=True),\n",
    "    'spline_order':trial.suggest_int('spline_order', 1, 5)}\n",
    "    \n",
    "    # Create and train the model\n",
    "    gam = LinearGAM(n_splines=params['n_splines'], spline_order=params['spline_order'], lam=params['lam']).fit(X_train_, y_train_)\n",
    "\n",
    "    # Predict on the validation set and calculate the RMSE\n",
    "    y_val_hat_gam = gam.predict(X_val)\n",
    "    RMSE_gam = np.sqrt(np.mean((y_val - y_val_hat_gam) ** 2))\n",
    "\n",
    "    return RMSE_gam\n",
    "\n",
    "# Create the sampler and study\n",
    "sampler_gam = optuna.samplers.TPESampler(seed=seed)\n",
    "study_gam = optuna.create_study(sampler=sampler_gam, direction='minimize')\n",
    "\n",
    "# Optimize the model\n",
    "study_gam.optimize(gam_model, n_trials=N_TRIALS)\n",
    "\n",
    "# Create the final model with the best parameters\n",
    "best_params = study_gam.best_params\n",
    "\n",
    "final_gam_model = LinearGAM(n_splines=best_params['n_splines'], spline_order=best_params['spline_order'], lam=best_params['lam'])\n",
    "\n",
    "# Fit the model\n",
    "final_gam_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_hat_gam = final_gam_model.predict(X_test)\n",
    "# Calculate the RMSE\n",
    "RMSE_gam = np.sqrt(np.mean((y_test - y_test_hat_gam) ** 2))\n",
    "print(\"RMSE GAM: \", RMSE_gam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the final model with the best parameters\n",
    "best_params = study_gam.best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splines = []\n",
    "lam = []\n",
    "spline_order = []\n",
    "\n",
    "# Iterate over each covariate in X_train_\n",
    "for col in X_train.columns:\n",
    "    # Define the search space for n_splines, lam, and spline_order\n",
    "    n_splines.append(best_params[f'n_splines_{col}'])\n",
    "    lam.append(best_params[f'lam_{col}'])\n",
    "    spline_order.append(best_params[f'spline_order_{col}'])\n",
    "\n",
    "final_gam_model = LinearGAM(n_splines=n_splines, spline_order=spline_order, lam=lam)\n",
    "\n",
    "# Fit the model\n",
    "final_gam_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the train set\n",
    "y_train_hat_gam = final_gam_model.predict(X_train)\n",
    "std_dev_error = np.std(y_train - y_train_hat_gam)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_hat_gam = final_gam_model.predict(X_test)\n",
    "\n",
    "# Calculate the CRPS\n",
    "crps_gam = [crps_gaussian(y_test_np[i], mu=y_test_hat_gam[i], sig=std_dev_error) for i in range(len(y_test_hat_gam))]\n",
    "crps_gam = np.mean(crps_gam)\n",
    "print(\"CRPS GAM: \", crps_gam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-03-27 20:45:26,719] A new study created in memory with name: no-name-f0eb0079-46e2-4e12-9380-3ad1264ea862\n",
      "[I 2024-03-27 20:45:54,208] Trial 0 finished with value: 38.545020865511354 and parameters: {'n_splines': 79, 'lam': 0.0013320229150659071, 'spline_order': 4}. Best is trial 0 with value: 38.545020865511354.\n",
      "[I 2024-03-27 20:46:16,055] Trial 1 finished with value: 13.423005516937682 and parameters: {'n_splines': 76, 'lam': 0.9795848815655198, 'spline_order': 2}. Best is trial 1 with value: 13.423005516937682.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRPS GAM:  5.158456944302808\n"
     ]
    }
   ],
   "source": [
    "N_TRIALS=2\n",
    "#### GAM model\n",
    "def gam_model(trial):\n",
    "\n",
    "    # Define the hyperparameters to optimize\n",
    "    params = {'n_splines': trial.suggest_int('n_splines', 10, 100),\n",
    "              'lam': trial.suggest_float('lam', 1e-3, 1e3, log=True),\n",
    "              'spline_order': trial.suggest_int('spline_order', 1, 5)}\n",
    "    \n",
    "    # Create and train the model\n",
    "    gam = LinearGAM(n_splines=params['n_splines'], spline_order=params['spline_order'], lam=params['lam']).fit(X_train_, y_train_)\n",
    "\n",
    "    # Predict on the validation set and calculate the CRPS\n",
    "    y_train_hat_gam = gam.predict(X_train)\n",
    "    std_dev_error = np.std(y_train - y_train_hat_gam)\n",
    "    y_val_hat_gam = gam.predict(X_val)\n",
    "    crps_gam = [crps_gaussian(y_val_np[i], mu=y_val_hat_gam[i], sig=std_dev_error) for i in range(len(y_val_hat_gam))]\n",
    "    crps_gam = np.mean(crps_gam)\n",
    "\n",
    "    return crps_gam\n",
    "\n",
    "# Create the sampler and study\n",
    "sampler_gam = optuna.samplers.TPESampler(seed=seed)\n",
    "study_gam = optuna.create_study(sampler=sampler_gam, direction='minimize')\n",
    "\n",
    "# Optimize the model\n",
    "study_gam.optimize(gam_model, n_trials=N_TRIALS)\n",
    "\n",
    "# Create the final model with the best parameters\n",
    "best_params = study_gam.best_params\n",
    "final_gam_model = LinearGAM(n_splines=best_params['n_splines'], spline_order=best_params['spline_order'], lam=best_params['lam'])\n",
    "\n",
    "# Fit the model\n",
    "final_gam_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the train set\n",
    "y_train_hat_gam = final_gam_model.predict(X_train)\n",
    "std_dev_error = np.std(y_train - y_train_hat_gam)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_hat_gam = final_gam_model.predict(X_test)\n",
    "\n",
    "# Calculate the CRPS\n",
    "crps_gam = [crps_gaussian(y_test_np[i], mu=y_test_hat_gam[i], sig=std_dev_error) for i in range(len(y_test_hat_gam))]\n",
    "crps_gam = np.mean(crps_gam)\n",
    "print(\"CRPS GAM: \", crps_gam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearGAM                                                                                                 \n",
      "=============================================== ==========================================================\n",
      "Distribution:                        NormalDist Effective DoF:                                    483.5689\n",
      "Link Function:                     IdentityLink Log Likelihood:                                -17415.2801\n",
      "Number of Samples:                         6553 AIC:                                             35799.698\n",
      "                                                AICc:                                           35877.2568\n",
      "                                                GCV:                                                5.9962\n",
      "                                                Scale:                                              5.2054\n",
      "                                                Pseudo R-Squared:                                   0.9394\n",
      "==========================================================================================================\n",
      "Feature Function                  Lambda               Rank         EDoF         P > x        Sig. Code   \n",
      "================================= ==================== ============ ============ ============ ============\n",
      "s(0)                              [0.0017]             63           34.5         1.11e-16     ***         \n",
      "s(1)                              [0.068]              17           9.7          4.90e-04     ***         \n",
      "s(2)                              [0.0017]             80           61.9         1.11e-16     ***         \n",
      "s(3)                              [6.5909]             38           9.7          6.83e-08     ***         \n",
      "s(4)                              [190.3229]           13           2.7          1.11e-16     ***         \n",
      "s(5)                              [0.3199]             89           51.6         4.33e-03     **          \n",
      "s(6)                              [3.8638]             56           20.7         1.11e-16     ***         \n",
      "s(7)                              [0.0014]             37           32.0         7.59e-06     ***         \n",
      "s(8)                              [2.2155]             32           14.6         1.11e-16     ***         \n",
      "s(9)                              [0.0571]             53           36.4         4.96e-07     ***         \n",
      "s(10)                             [0.1092]             99           56.3         8.45e-05     ***         \n",
      "s(11)                             [0.4412]             98           32.8         9.02e-09     ***         \n",
      "s(12)                             [2.9432]             57           12.1         1.72e-13     ***         \n",
      "s(13)                             [0.6126]             16           4.5          9.85e-01                 \n",
      "s(14)                             [3.2819]             75           29.1         1.73e-07     ***         \n",
      "s(15)                             [2.3941]             41           11.1         1.11e-16     ***         \n",
      "s(16)                             [10.985]             56           20.4         1.11e-16     ***         \n",
      "s(17)                             [0.1064]             14           8.3          1.11e-16     ***         \n",
      "s(18)                             [206.5202]           26           1.2          2.97e-13     ***         \n",
      "s(19)                             [32.7323]            29           11.1         4.00e-14     ***         \n",
      "s(20)                             [0.6587]             77           22.7         1.11e-16     ***         \n",
      "intercept                                              1            0.0          3.58e-01                 \n",
      "==========================================================================================================\n",
      "Significance codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n",
      "\n",
      "WARNING: Fitting splines and a linear function to a feature introduces a model identifiability problem\n",
      "         which can cause p-values to appear significant when they are not.\n",
      "\n",
      "WARNING: p-values calculated in this manner behave correctly for un-penalized models or models with\n",
      "         known smoothing parameters, but when smoothing parameters have been estimated, the p-values\n",
      "         are typically lower than they should be, meaning that the tests reject the null too readily.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dalma\\AppData\\Local\\Temp\\ipykernel_2548\\3018071388.py:1: UserWarning: KNOWN BUG: p-values computed in this summary are likely much smaller than they should be. \n",
      " \n",
      "Please do not make inferences based on these values! \n",
      "\n",
      "Collaborate on a solution, and stay up to date at: \n",
      "github.com/dswah/pyGAM/issues/163 \n",
      "\n",
      "  final_gam_model.summary()\n"
     ]
    }
   ],
   "source": [
    "final_gam_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "(slice(None, None, None), 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3791\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3790\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3791\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3792\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:158\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '(slice(None, None, None), 0)' is an invalid key",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create and train the model\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m gam \u001b[38;5;241m=\u001b[39m \u001b[43mLinearGAM\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splines\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mspline_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgridsearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlam\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlogspace\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:1936\u001b[0m, in \u001b[0;36mGAM.gridsearch\u001b[1;34m(self, X, y, weights, return_scores, keep_best, objective, progress, **param_grids)\u001b[0m\n\u001b[0;32m   1934\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_fitted:\n\u001b[0;32m   1935\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m-> 1936\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data_dep_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1938\u001b[0m y \u001b[38;5;241m=\u001b[39m check_y(y, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlink, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribution, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m   1939\u001b[0m X \u001b[38;5;241m=\u001b[39m check_X(X, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\pygam.py:328\u001b[0m, in \u001b[0;36mGAM._validate_data_dep_params\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m remove:\n\u001b[0;32m    326\u001b[0m     \u001b[38;5;28mdelattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, k)\n\u001b[1;32m--> 328\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mterms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\terms.py:1722\u001b[0m, in \u001b[0;36mTermList.compile\u001b[1;34m(self, X, verbose)\u001b[0m\n\u001b[0;32m   1707\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"method to validate and prepare data-dependent parameters\u001b[39;00m\n\u001b[0;32m   1708\u001b[0m \n\u001b[0;32m   1709\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1719\u001b[0m \u001b[38;5;124;03mNone\u001b[39;00m\n\u001b[0;32m   1720\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1721\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m term \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_terms:\n\u001b[1;32m-> 1722\u001b[0m     \u001b[43mterm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1724\u001b[0m \u001b[38;5;66;03m# now remove duplicate intercepts\u001b[39;00m\n\u001b[0;32m   1725\u001b[0m n_intercepts \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\terms.py:831\u001b[0m, in \u001b[0;36mSplineTerm.compile\u001b[1;34m(self, X, verbose)\u001b[0m\n\u001b[0;32m    824\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    825\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mby variable requires feature \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    826\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbut X has only \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dimensions\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mby, X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    827\u001b[0m     )\n\u001b[0;32m    829\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124medge_knots_\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m    830\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39medge_knots_ \u001b[38;5;241m=\u001b[39m gen_edge_knots(\n\u001b[1;32m--> 831\u001b[0m         \u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature\u001b[49m\u001b[43m]\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, verbose\u001b[38;5;241m=\u001b[39mverbose\n\u001b[0;32m    832\u001b[0m     )\n\u001b[0;32m    833\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pandas\\core\\frame.py:3893\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3891\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3892\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3893\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3895\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3798\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3799\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m-> 3803\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_indexing_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pandas\\core\\indexes\\base.py:5975\u001b[0m, in \u001b[0;36mIndex._check_indexing_error\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   5971\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_indexing_error\u001b[39m(\u001b[38;5;28mself\u001b[39m, key):\n\u001b[0;32m   5972\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_scalar(key):\n\u001b[0;32m   5973\u001b[0m         \u001b[38;5;66;03m# if key is not a scalar, directly raise an error (the code below\u001b[39;00m\n\u001b[0;32m   5974\u001b[0m         \u001b[38;5;66;03m# would convert to numpy arrays and raise later any way) - GH29926\u001b[39;00m\n\u001b[1;32m-> 5975\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m: (slice(None, None, None), 0)"
     ]
    }
   ],
   "source": [
    "# Create and train the model\n",
    "gam = LinearGAM(s(0, n_splines=20, spline_order=5)).gridsearch(X_train_, y_train_, lam=np.logspace(-3, 3, 11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lread</th>\n",
       "      <th>lwrite</th>\n",
       "      <th>scall</th>\n",
       "      <th>sread</th>\n",
       "      <th>swrite</th>\n",
       "      <th>fork</th>\n",
       "      <th>exec</th>\n",
       "      <th>rchar</th>\n",
       "      <th>wchar</th>\n",
       "      <th>pgout</th>\n",
       "      <th>...</th>\n",
       "      <th>pgfree</th>\n",
       "      <th>pgscan</th>\n",
       "      <th>atch</th>\n",
       "      <th>pgin</th>\n",
       "      <th>ppgin</th>\n",
       "      <th>pflt</th>\n",
       "      <th>vflt</th>\n",
       "      <th>runqsz</th>\n",
       "      <th>freemem</th>\n",
       "      <th>freeswap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1036.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>114.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>172076.0</td>\n",
       "      <td>355965.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>4.00</td>\n",
       "      <td>73.60</td>\n",
       "      <td>89.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6527.0</td>\n",
       "      <td>1851864.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>62.0</td>\n",
       "      <td>77.0</td>\n",
       "      <td>3806.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1.40</td>\n",
       "      <td>1.40</td>\n",
       "      <td>492142.0</td>\n",
       "      <td>268706.0</td>\n",
       "      <td>4.80</td>\n",
       "      <td>...</td>\n",
       "      <td>44.00</td>\n",
       "      <td>79.20</td>\n",
       "      <td>2.20</td>\n",
       "      <td>7.60</td>\n",
       "      <td>12.20</td>\n",
       "      <td>68.00</td>\n",
       "      <td>218.80</td>\n",
       "      <td>5.2</td>\n",
       "      <td>256.0</td>\n",
       "      <td>1314590.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4721.0</td>\n",
       "      <td>256.0</td>\n",
       "      <td>177.0</td>\n",
       "      <td>0.99</td>\n",
       "      <td>2.58</td>\n",
       "      <td>524787.0</td>\n",
       "      <td>174964.0</td>\n",
       "      <td>14.51</td>\n",
       "      <td>...</td>\n",
       "      <td>88.47</td>\n",
       "      <td>189.86</td>\n",
       "      <td>1.99</td>\n",
       "      <td>4.17</td>\n",
       "      <td>24.85</td>\n",
       "      <td>95.63</td>\n",
       "      <td>248.91</td>\n",
       "      <td>1.0</td>\n",
       "      <td>233.0</td>\n",
       "      <td>972606.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1692.0</td>\n",
       "      <td>132.0</td>\n",
       "      <td>87.0</td>\n",
       "      <td>0.40</td>\n",
       "      <td>1.80</td>\n",
       "      <td>220194.0</td>\n",
       "      <td>107031.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>3.81</td>\n",
       "      <td>4.01</td>\n",
       "      <td>23.65</td>\n",
       "      <td>42.08</td>\n",
       "      <td>2.2</td>\n",
       "      <td>2291.0</td>\n",
       "      <td>1010703.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>635.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>47.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>87465.0</td>\n",
       "      <td>40740.0</td>\n",
       "      <td>8.60</td>\n",
       "      <td>...</td>\n",
       "      <td>8.80</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.60</td>\n",
       "      <td>2.20</td>\n",
       "      <td>227.60</td>\n",
       "      <td>226.40</td>\n",
       "      <td>1.0</td>\n",
       "      <td>289.0</td>\n",
       "      <td>1806587.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8186</th>\n",
       "      <td>15.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>2599.0</td>\n",
       "      <td>277.0</td>\n",
       "      <td>234.0</td>\n",
       "      <td>3.61</td>\n",
       "      <td>2.20</td>\n",
       "      <td>133465.0</td>\n",
       "      <td>102035.0</td>\n",
       "      <td>17.03</td>\n",
       "      <td>...</td>\n",
       "      <td>50.10</td>\n",
       "      <td>46.29</td>\n",
       "      <td>10.22</td>\n",
       "      <td>10.02</td>\n",
       "      <td>15.83</td>\n",
       "      <td>187.78</td>\n",
       "      <td>297.80</td>\n",
       "      <td>2.0</td>\n",
       "      <td>139.0</td>\n",
       "      <td>1731732.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8188</th>\n",
       "      <td>29.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1906.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>2.00</td>\n",
       "      <td>8175.0</td>\n",
       "      <td>27313.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>56.20</td>\n",
       "      <td>78.60</td>\n",
       "      <td>3.6</td>\n",
       "      <td>166.0</td>\n",
       "      <td>1107088.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8189</th>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>926.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.60</td>\n",
       "      <td>1.00</td>\n",
       "      <td>5411.0</td>\n",
       "      <td>19322.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.40</td>\n",
       "      <td>53.40</td>\n",
       "      <td>154.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1177.0</td>\n",
       "      <td>1020400.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8190</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>418.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3959.0</td>\n",
       "      <td>10679.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.20</td>\n",
       "      <td>61.00</td>\n",
       "      <td>73.20</td>\n",
       "      <td>2.4</td>\n",
       "      <td>6355.0</td>\n",
       "      <td>1702592.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8191</th>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1888.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>215.0</td>\n",
       "      <td>6.20</td>\n",
       "      <td>1.80</td>\n",
       "      <td>216420.0</td>\n",
       "      <td>39346.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.40</td>\n",
       "      <td>14.80</td>\n",
       "      <td>296.60</td>\n",
       "      <td>420.20</td>\n",
       "      <td>4.6</td>\n",
       "      <td>1628.0</td>\n",
       "      <td>1757696.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6377 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lread  lwrite   scall  sread  swrite  fork  exec     rchar     wchar  \\\n",
       "0       6.0     2.0  1036.0  103.0   114.0  1.00  1.00  172076.0  355965.0   \n",
       "2      62.0    77.0  3806.0  258.0   166.0  1.40  1.40  492142.0  268706.0   \n",
       "3       5.0     0.0  4721.0  256.0   177.0  0.99  2.58  524787.0  174964.0   \n",
       "5       5.0     1.0  1692.0  132.0    87.0  0.40  1.80  220194.0  107031.0   \n",
       "6       3.0     0.0   635.0   65.0    47.0  3.00  3.00   87465.0   40740.0   \n",
       "...     ...     ...     ...    ...     ...   ...   ...       ...       ...   \n",
       "8186   15.0    11.0  2599.0  277.0   234.0  3.61  2.20  133465.0  102035.0   \n",
       "8188   29.0    40.0  1906.0  118.0    90.0  0.80  2.00    8175.0   27313.0   \n",
       "8189    3.0     0.0   926.0   90.0    67.0  0.60  1.00    5411.0   19322.0   \n",
       "8190    4.0     0.0   418.0   30.0    29.0  0.80  1.00    3959.0   10679.0   \n",
       "8191    5.0     0.0  1888.0  248.0   215.0  6.20  1.80  216420.0   39346.0   \n",
       "\n",
       "      pgout  ...  pgfree  pgscan   atch   pgin  ppgin    pflt    vflt  runqsz  \\\n",
       "0      0.00  ...    0.00    0.00   0.00   2.00   4.00   73.60   89.00     2.0   \n",
       "2      4.80  ...   44.00   79.20   2.20   7.60  12.20   68.00  218.80     5.2   \n",
       "3     14.51  ...   88.47  189.86   1.99   4.17  24.85   95.63  248.91     1.0   \n",
       "5      0.00  ...    0.00    0.00   0.00   3.81   4.01   23.65   42.08     2.2   \n",
       "6      8.60  ...    8.80    0.00   0.00   1.60   2.20  227.60  226.40     1.0   \n",
       "...     ...  ...     ...     ...    ...    ...    ...     ...     ...     ...   \n",
       "8186  17.03  ...   50.10   46.29  10.22  10.02  15.83  187.78  297.80     2.0   \n",
       "8188   0.00  ...    0.00    0.00   0.00   0.80   0.80   56.20   78.60     3.6   \n",
       "8189   0.00  ...    0.00    0.00   0.40   0.40   0.40   53.40  154.00     1.0   \n",
       "8190   0.00  ...    0.00    0.00   0.00   0.20   0.20   61.00   73.20     2.4   \n",
       "8191   0.00  ...    0.00    0.00   0.00   7.40  14.80  296.60  420.20     4.6   \n",
       "\n",
       "      freemem   freeswap  \n",
       "0      6527.0  1851864.0  \n",
       "2       256.0  1314590.0  \n",
       "3       233.0   972606.0  \n",
       "5      2291.0  1010703.0  \n",
       "6       289.0  1806587.0  \n",
       "...       ...        ...  \n",
       "8186    139.0  1731732.0  \n",
       "8188    166.0  1107088.0  \n",
       "8189   1177.0  1020400.0  \n",
       "8190   6355.0  1702592.0  \n",
       "8191   1628.0  1757696.0  \n",
       "\n",
       "[6377 rows x 21 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lread</th>\n",
       "      <th>lwrite</th>\n",
       "      <th>scall</th>\n",
       "      <th>sread</th>\n",
       "      <th>swrite</th>\n",
       "      <th>fork</th>\n",
       "      <th>exec</th>\n",
       "      <th>rchar</th>\n",
       "      <th>wchar</th>\n",
       "      <th>pgout</th>\n",
       "      <th>...</th>\n",
       "      <th>pgfree</th>\n",
       "      <th>pgscan</th>\n",
       "      <th>atch</th>\n",
       "      <th>pgin</th>\n",
       "      <th>ppgin</th>\n",
       "      <th>pflt</th>\n",
       "      <th>vflt</th>\n",
       "      <th>runqsz</th>\n",
       "      <th>freemem</th>\n",
       "      <th>freeswap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2165.0</td>\n",
       "      <td>205.0</td>\n",
       "      <td>101.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>1.20</td>\n",
       "      <td>43107.0</td>\n",
       "      <td>44139.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>...</td>\n",
       "      <td>75.8</td>\n",
       "      <td>181.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>85.40</td>\n",
       "      <td>88.20</td>\n",
       "      <td>19.40</td>\n",
       "      <td>161.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1131931.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>42.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>3949.0</td>\n",
       "      <td>249.0</td>\n",
       "      <td>244.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>4.60</td>\n",
       "      <td>197289.0</td>\n",
       "      <td>529200.0</td>\n",
       "      <td>4.2</td>\n",
       "      <td>...</td>\n",
       "      <td>6.6</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.80</td>\n",
       "      <td>2.20</td>\n",
       "      <td>219.60</td>\n",
       "      <td>297.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>331.0</td>\n",
       "      <td>1013805.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1341.0</td>\n",
       "      <td>240.0</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.60</td>\n",
       "      <td>718437.0</td>\n",
       "      <td>672290.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.60</td>\n",
       "      <td>15.00</td>\n",
       "      <td>42.80</td>\n",
       "      <td>52.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>2532.0</td>\n",
       "      <td>1037078.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>172.0</td>\n",
       "      <td>113.0</td>\n",
       "      <td>1427.0</td>\n",
       "      <td>171.0</td>\n",
       "      <td>134.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>3.39</td>\n",
       "      <td>261636.0</td>\n",
       "      <td>103649.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.99</td>\n",
       "      <td>5.39</td>\n",
       "      <td>62.28</td>\n",
       "      <td>152.1</td>\n",
       "      <td>744.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>37.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>4667.0</td>\n",
       "      <td>673.0</td>\n",
       "      <td>457.0</td>\n",
       "      <td>2.6</td>\n",
       "      <td>0.80</td>\n",
       "      <td>262181.0</td>\n",
       "      <td>220110.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>...</td>\n",
       "      <td>6.6</td>\n",
       "      <td>9.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>3.40</td>\n",
       "      <td>3.40</td>\n",
       "      <td>130.80</td>\n",
       "      <td>212.8</td>\n",
       "      <td>1.4</td>\n",
       "      <td>287.0</td>\n",
       "      <td>1073834.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8172</th>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2075.0</td>\n",
       "      <td>254.0</td>\n",
       "      <td>204.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.00</td>\n",
       "      <td>110184.0</td>\n",
       "      <td>69205.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>...</td>\n",
       "      <td>59.6</td>\n",
       "      <td>96.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>82.00</td>\n",
       "      <td>82.40</td>\n",
       "      <td>164.60</td>\n",
       "      <td>374.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1106848.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8176</th>\n",
       "      <td>71.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>1052.0</td>\n",
       "      <td>221.0</td>\n",
       "      <td>157.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.40</td>\n",
       "      <td>145004.0</td>\n",
       "      <td>48770.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>...</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.8</td>\n",
       "      <td>3.6</td>\n",
       "      <td>10.40</td>\n",
       "      <td>15.40</td>\n",
       "      <td>36.60</td>\n",
       "      <td>120.4</td>\n",
       "      <td>896.0</td>\n",
       "      <td>89.0</td>\n",
       "      <td>11.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8177</th>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2330.0</td>\n",
       "      <td>228.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>10.6</td>\n",
       "      <td>21.80</td>\n",
       "      <td>955802.0</td>\n",
       "      <td>67045.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>...</td>\n",
       "      <td>68.6</td>\n",
       "      <td>101.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>51.80</td>\n",
       "      <td>53.20</td>\n",
       "      <td>359.60</td>\n",
       "      <td>761.8</td>\n",
       "      <td>4.4</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1107003.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8181</th>\n",
       "      <td>10.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2981.0</td>\n",
       "      <td>190.0</td>\n",
       "      <td>107.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>670209.0</td>\n",
       "      <td>86240.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>...</td>\n",
       "      <td>233.4</td>\n",
       "      <td>460.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>62.60</td>\n",
       "      <td>108.80</td>\n",
       "      <td>75.00</td>\n",
       "      <td>367.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>137.0</td>\n",
       "      <td>1317451.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8187</th>\n",
       "      <td>74.0</td>\n",
       "      <td>49.0</td>\n",
       "      <td>2688.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>32.20</td>\n",
       "      <td>57714.0</td>\n",
       "      <td>38484.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.80</td>\n",
       "      <td>343.20</td>\n",
       "      <td>649.4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>314.0</td>\n",
       "      <td>1096333.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1815 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      lread  lwrite   scall  sread  swrite  fork   exec     rchar     wchar  \\\n",
       "1       1.0     0.0  2165.0  205.0   101.0   0.4   1.20   43107.0   44139.0   \n",
       "4      42.0    55.0  3949.0  249.0   244.0   2.6   4.60  197289.0  529200.0   \n",
       "7       7.0     5.0  1341.0  240.0   120.0   0.4   0.60  718437.0  672290.0   \n",
       "14    172.0   113.0  1427.0  171.0   134.0   0.6   3.39  261636.0  103649.0   \n",
       "16     37.0    49.0  4667.0  673.0   457.0   2.6   0.80  262181.0  220110.0   \n",
       "...     ...     ...     ...    ...     ...   ...    ...       ...       ...   \n",
       "8172    4.0     0.0  2075.0  254.0   204.0   3.4   1.00  110184.0   69205.0   \n",
       "8176   71.0    80.0  1052.0  221.0   157.0   0.4   0.40  145004.0   48770.0   \n",
       "8177   40.0     1.0  2330.0  228.0    81.0  10.6  21.80  955802.0   67045.0   \n",
       "8181   10.0     2.0  2981.0  190.0   107.0   1.0   2.00  670209.0   86240.0   \n",
       "8187   74.0    49.0  2688.0  176.0   103.0  11.0  32.20   57714.0   38484.0   \n",
       "\n",
       "      pgout  ...  pgfree  pgscan  atch   pgin   ppgin    pflt   vflt  runqsz  \\\n",
       "1       4.8  ...    75.8   181.4   0.2  85.40   88.20   19.40  161.8     3.0   \n",
       "4       4.2  ...     6.6     0.0   1.4   1.80    2.20  219.60  297.2     3.4   \n",
       "7       0.0  ...     0.0     0.0   0.0   7.60   15.00   42.80   52.8     4.8   \n",
       "14      0.0  ...     0.0     0.0   0.8   4.99    5.39   62.28  152.1   744.0   \n",
       "16      0.2  ...     6.6     9.2   0.2   3.40    3.40  130.80  212.8     1.4   \n",
       "...     ...  ...     ...     ...   ...    ...     ...     ...    ...     ...   \n",
       "8172    7.8  ...    59.6    96.2   1.6  82.00   82.40  164.60  374.0     1.4   \n",
       "8176    1.4  ...     3.8     3.8   3.6  10.40   15.40   36.60  120.4   896.0   \n",
       "8177    7.0  ...    68.6   101.2   1.4  51.80   53.20  359.60  761.8     4.4   \n",
       "8181   16.0  ...   233.4   460.0   0.4  62.60  108.80   75.00  367.8     5.6   \n",
       "8187    0.8  ...     1.0     0.0   0.0   0.80    0.80  343.20  649.4     7.0   \n",
       "\n",
       "      freemem   freeswap  \n",
       "1       130.0  1131931.0  \n",
       "4       331.0  1013805.0  \n",
       "7      2532.0  1037078.0  \n",
       "14       92.0        8.0  \n",
       "16      287.0  1073834.0  \n",
       "...       ...        ...  \n",
       "8172    130.0  1106848.0  \n",
       "8176     89.0       11.0  \n",
       "8177    122.0  1107003.0  \n",
       "8181    137.0  1317451.0  \n",
       "8187    314.0  1096333.0  \n",
       "\n",
       "[1815 rows x 21 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lread</th>\n",
       "      <th>lwrite</th>\n",
       "      <th>scall</th>\n",
       "      <th>sread</th>\n",
       "      <th>swrite</th>\n",
       "      <th>fork</th>\n",
       "      <th>exec</th>\n",
       "      <th>rchar</th>\n",
       "      <th>wchar</th>\n",
       "      <th>pgout</th>\n",
       "      <th>ppgout</th>\n",
       "      <th>pgfree</th>\n",
       "      <th>atch</th>\n",
       "      <th>pgin</th>\n",
       "      <th>ppgin</th>\n",
       "      <th>pflt</th>\n",
       "      <th>vflt</th>\n",
       "      <th>runqsz</th>\n",
       "      <th>freemem</th>\n",
       "      <th>freeswap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lread</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.533737</td>\n",
       "      <td>0.191377</td>\n",
       "      <td>0.132881</td>\n",
       "      <td>0.119953</td>\n",
       "      <td>0.140284</td>\n",
       "      <td>0.110965</td>\n",
       "      <td>0.109624</td>\n",
       "      <td>0.081270</td>\n",
       "      <td>0.082463</td>\n",
       "      <td>0.130590</td>\n",
       "      <td>0.114438</td>\n",
       "      <td>0.021563</td>\n",
       "      <td>0.189799</td>\n",
       "      <td>0.161345</td>\n",
       "      <td>0.137463</td>\n",
       "      <td>0.165539</td>\n",
       "      <td>0.023379</td>\n",
       "      <td>-0.083214</td>\n",
       "      <td>-0.081293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lwrite</th>\n",
       "      <td>0.533737</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.143404</td>\n",
       "      <td>0.128403</td>\n",
       "      <td>0.101524</td>\n",
       "      <td>0.052511</td>\n",
       "      <td>0.038237</td>\n",
       "      <td>0.117464</td>\n",
       "      <td>0.091319</td>\n",
       "      <td>0.067013</td>\n",
       "      <td>0.079485</td>\n",
       "      <td>0.065692</td>\n",
       "      <td>0.028310</td>\n",
       "      <td>0.091068</td>\n",
       "      <td>0.089011</td>\n",
       "      <td>0.067024</td>\n",
       "      <td>0.094965</td>\n",
       "      <td>0.043562</td>\n",
       "      <td>-0.091133</td>\n",
       "      <td>-0.116478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>scall</th>\n",
       "      <td>0.191377</td>\n",
       "      <td>0.143404</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.696887</td>\n",
       "      <td>0.619984</td>\n",
       "      <td>0.446766</td>\n",
       "      <td>0.308999</td>\n",
       "      <td>0.352609</td>\n",
       "      <td>0.274199</td>\n",
       "      <td>0.194529</td>\n",
       "      <td>0.208400</td>\n",
       "      <td>0.199778</td>\n",
       "      <td>0.077969</td>\n",
       "      <td>0.241628</td>\n",
       "      <td>0.219070</td>\n",
       "      <td>0.481781</td>\n",
       "      <td>0.531760</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>-0.387520</td>\n",
       "      <td>-0.350629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sread</th>\n",
       "      <td>0.132881</td>\n",
       "      <td>0.128403</td>\n",
       "      <td>0.696887</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.881069</td>\n",
       "      <td>0.416721</td>\n",
       "      <td>0.164084</td>\n",
       "      <td>0.504412</td>\n",
       "      <td>0.401923</td>\n",
       "      <td>0.193679</td>\n",
       "      <td>0.225906</td>\n",
       "      <td>0.212911</td>\n",
       "      <td>0.085468</td>\n",
       "      <td>0.207012</td>\n",
       "      <td>0.210225</td>\n",
       "      <td>0.452020</td>\n",
       "      <td>0.491045</td>\n",
       "      <td>0.041096</td>\n",
       "      <td>-0.286437</td>\n",
       "      <td>-0.302036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>swrite</th>\n",
       "      <td>0.119953</td>\n",
       "      <td>0.101524</td>\n",
       "      <td>0.619984</td>\n",
       "      <td>0.881069</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.376876</td>\n",
       "      <td>0.103643</td>\n",
       "      <td>0.333322</td>\n",
       "      <td>0.394356</td>\n",
       "      <td>0.151371</td>\n",
       "      <td>0.159442</td>\n",
       "      <td>0.145458</td>\n",
       "      <td>0.061373</td>\n",
       "      <td>0.147000</td>\n",
       "      <td>0.144278</td>\n",
       "      <td>0.396580</td>\n",
       "      <td>0.416571</td>\n",
       "      <td>0.014101</td>\n",
       "      <td>-0.248574</td>\n",
       "      <td>-0.237062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fork</th>\n",
       "      <td>0.140284</td>\n",
       "      <td>0.052511</td>\n",
       "      <td>0.446766</td>\n",
       "      <td>0.416721</td>\n",
       "      <td>0.376876</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.763974</td>\n",
       "      <td>0.280955</td>\n",
       "      <td>0.061040</td>\n",
       "      <td>0.130192</td>\n",
       "      <td>0.166872</td>\n",
       "      <td>0.168082</td>\n",
       "      <td>0.047194</td>\n",
       "      <td>0.163468</td>\n",
       "      <td>0.132181</td>\n",
       "      <td>0.931040</td>\n",
       "      <td>0.939348</td>\n",
       "      <td>-0.017393</td>\n",
       "      <td>-0.123357</td>\n",
       "      <td>-0.130442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exec</th>\n",
       "      <td>0.110965</td>\n",
       "      <td>0.038237</td>\n",
       "      <td>0.308999</td>\n",
       "      <td>0.164084</td>\n",
       "      <td>0.103643</td>\n",
       "      <td>0.763974</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.168085</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>0.111465</td>\n",
       "      <td>0.149334</td>\n",
       "      <td>0.146163</td>\n",
       "      <td>0.052307</td>\n",
       "      <td>0.186099</td>\n",
       "      <td>0.149911</td>\n",
       "      <td>0.645239</td>\n",
       "      <td>0.691754</td>\n",
       "      <td>-0.006048</td>\n",
       "      <td>-0.158565</td>\n",
       "      <td>-0.153347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rchar</th>\n",
       "      <td>0.109624</td>\n",
       "      <td>0.117464</td>\n",
       "      <td>0.352609</td>\n",
       "      <td>0.504412</td>\n",
       "      <td>0.333322</td>\n",
       "      <td>0.280955</td>\n",
       "      <td>0.168085</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.505186</td>\n",
       "      <td>0.210885</td>\n",
       "      <td>0.268056</td>\n",
       "      <td>0.276503</td>\n",
       "      <td>0.171322</td>\n",
       "      <td>0.299051</td>\n",
       "      <td>0.346427</td>\n",
       "      <td>0.313150</td>\n",
       "      <td>0.363618</td>\n",
       "      <td>0.131645</td>\n",
       "      <td>-0.149771</td>\n",
       "      <td>-0.220934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wchar</th>\n",
       "      <td>0.081270</td>\n",
       "      <td>0.091319</td>\n",
       "      <td>0.274199</td>\n",
       "      <td>0.401923</td>\n",
       "      <td>0.394356</td>\n",
       "      <td>0.061040</td>\n",
       "      <td>0.000640</td>\n",
       "      <td>0.505186</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.192523</td>\n",
       "      <td>0.188655</td>\n",
       "      <td>0.159009</td>\n",
       "      <td>0.181452</td>\n",
       "      <td>0.177709</td>\n",
       "      <td>0.200614</td>\n",
       "      <td>0.086396</td>\n",
       "      <td>0.111358</td>\n",
       "      <td>0.191111</td>\n",
       "      <td>-0.149335</td>\n",
       "      <td>-0.225983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pgout</th>\n",
       "      <td>0.082463</td>\n",
       "      <td>0.067013</td>\n",
       "      <td>0.194529</td>\n",
       "      <td>0.193679</td>\n",
       "      <td>0.151371</td>\n",
       "      <td>0.130192</td>\n",
       "      <td>0.111465</td>\n",
       "      <td>0.210885</td>\n",
       "      <td>0.192523</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.872445</td>\n",
       "      <td>0.730381</td>\n",
       "      <td>0.147759</td>\n",
       "      <td>0.385648</td>\n",
       "      <td>0.414865</td>\n",
       "      <td>0.151285</td>\n",
       "      <td>0.229129</td>\n",
       "      <td>-0.009814</td>\n",
       "      <td>-0.269687</td>\n",
       "      <td>-0.245378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppgout</th>\n",
       "      <td>0.130590</td>\n",
       "      <td>0.079485</td>\n",
       "      <td>0.208400</td>\n",
       "      <td>0.225906</td>\n",
       "      <td>0.159442</td>\n",
       "      <td>0.166872</td>\n",
       "      <td>0.149334</td>\n",
       "      <td>0.268056</td>\n",
       "      <td>0.188655</td>\n",
       "      <td>0.872445</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.917790</td>\n",
       "      <td>0.093336</td>\n",
       "      <td>0.488261</td>\n",
       "      <td>0.542392</td>\n",
       "      <td>0.185941</td>\n",
       "      <td>0.285708</td>\n",
       "      <td>-0.018781</td>\n",
       "      <td>-0.247554</td>\n",
       "      <td>-0.213791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pgfree</th>\n",
       "      <td>0.114438</td>\n",
       "      <td>0.065692</td>\n",
       "      <td>0.199778</td>\n",
       "      <td>0.212911</td>\n",
       "      <td>0.145458</td>\n",
       "      <td>0.168082</td>\n",
       "      <td>0.146163</td>\n",
       "      <td>0.276503</td>\n",
       "      <td>0.159009</td>\n",
       "      <td>0.730381</td>\n",
       "      <td>0.917790</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.069290</td>\n",
       "      <td>0.532834</td>\n",
       "      <td>0.593396</td>\n",
       "      <td>0.190468</td>\n",
       "      <td>0.301851</td>\n",
       "      <td>-0.019083</td>\n",
       "      <td>-0.234195</td>\n",
       "      <td>-0.210184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>atch</th>\n",
       "      <td>0.021563</td>\n",
       "      <td>0.028310</td>\n",
       "      <td>0.077969</td>\n",
       "      <td>0.085468</td>\n",
       "      <td>0.061373</td>\n",
       "      <td>0.047194</td>\n",
       "      <td>0.052307</td>\n",
       "      <td>0.171322</td>\n",
       "      <td>0.181452</td>\n",
       "      <td>0.147759</td>\n",
       "      <td>0.093336</td>\n",
       "      <td>0.069290</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.057639</td>\n",
       "      <td>0.057373</td>\n",
       "      <td>0.050594</td>\n",
       "      <td>0.095504</td>\n",
       "      <td>0.182370</td>\n",
       "      <td>-0.086029</td>\n",
       "      <td>-0.121668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pgin</th>\n",
       "      <td>0.189799</td>\n",
       "      <td>0.091068</td>\n",
       "      <td>0.241628</td>\n",
       "      <td>0.207012</td>\n",
       "      <td>0.147000</td>\n",
       "      <td>0.163468</td>\n",
       "      <td>0.186099</td>\n",
       "      <td>0.299051</td>\n",
       "      <td>0.177709</td>\n",
       "      <td>0.385648</td>\n",
       "      <td>0.488261</td>\n",
       "      <td>0.532834</td>\n",
       "      <td>0.057639</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.923621</td>\n",
       "      <td>0.175644</td>\n",
       "      <td>0.303013</td>\n",
       "      <td>0.070547</td>\n",
       "      <td>-0.230779</td>\n",
       "      <td>-0.278927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ppgin</th>\n",
       "      <td>0.161345</td>\n",
       "      <td>0.089011</td>\n",
       "      <td>0.219070</td>\n",
       "      <td>0.210225</td>\n",
       "      <td>0.144278</td>\n",
       "      <td>0.132181</td>\n",
       "      <td>0.149911</td>\n",
       "      <td>0.346427</td>\n",
       "      <td>0.200614</td>\n",
       "      <td>0.414865</td>\n",
       "      <td>0.542392</td>\n",
       "      <td>0.593396</td>\n",
       "      <td>0.057373</td>\n",
       "      <td>0.923621</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.149962</td>\n",
       "      <td>0.263352</td>\n",
       "      <td>0.071542</td>\n",
       "      <td>-0.215318</td>\n",
       "      <td>-0.253941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pflt</th>\n",
       "      <td>0.137463</td>\n",
       "      <td>0.067024</td>\n",
       "      <td>0.481781</td>\n",
       "      <td>0.452020</td>\n",
       "      <td>0.396580</td>\n",
       "      <td>0.931040</td>\n",
       "      <td>0.645239</td>\n",
       "      <td>0.313150</td>\n",
       "      <td>0.086396</td>\n",
       "      <td>0.151285</td>\n",
       "      <td>0.185941</td>\n",
       "      <td>0.190468</td>\n",
       "      <td>0.050594</td>\n",
       "      <td>0.175644</td>\n",
       "      <td>0.149962</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.935370</td>\n",
       "      <td>-0.005786</td>\n",
       "      <td>-0.112774</td>\n",
       "      <td>-0.130609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vflt</th>\n",
       "      <td>0.165539</td>\n",
       "      <td>0.094965</td>\n",
       "      <td>0.531760</td>\n",
       "      <td>0.491045</td>\n",
       "      <td>0.416571</td>\n",
       "      <td>0.939348</td>\n",
       "      <td>0.691754</td>\n",
       "      <td>0.363618</td>\n",
       "      <td>0.111358</td>\n",
       "      <td>0.229129</td>\n",
       "      <td>0.285708</td>\n",
       "      <td>0.301851</td>\n",
       "      <td>0.095504</td>\n",
       "      <td>0.303013</td>\n",
       "      <td>0.263352</td>\n",
       "      <td>0.935370</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.016180</td>\n",
       "      <td>-0.201790</td>\n",
       "      <td>-0.245384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>runqsz</th>\n",
       "      <td>0.023379</td>\n",
       "      <td>0.043562</td>\n",
       "      <td>0.000162</td>\n",
       "      <td>0.041096</td>\n",
       "      <td>0.014101</td>\n",
       "      <td>-0.017393</td>\n",
       "      <td>-0.006048</td>\n",
       "      <td>0.131645</td>\n",
       "      <td>0.191111</td>\n",
       "      <td>-0.009814</td>\n",
       "      <td>-0.018781</td>\n",
       "      <td>-0.019083</td>\n",
       "      <td>0.182370</td>\n",
       "      <td>0.070547</td>\n",
       "      <td>0.071542</td>\n",
       "      <td>-0.005786</td>\n",
       "      <td>0.016180</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.094763</td>\n",
       "      <td>-0.433571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freemem</th>\n",
       "      <td>-0.083214</td>\n",
       "      <td>-0.091133</td>\n",
       "      <td>-0.387520</td>\n",
       "      <td>-0.286437</td>\n",
       "      <td>-0.248574</td>\n",
       "      <td>-0.123357</td>\n",
       "      <td>-0.158565</td>\n",
       "      <td>-0.149771</td>\n",
       "      <td>-0.149335</td>\n",
       "      <td>-0.269687</td>\n",
       "      <td>-0.247554</td>\n",
       "      <td>-0.234195</td>\n",
       "      <td>-0.086029</td>\n",
       "      <td>-0.230779</td>\n",
       "      <td>-0.215318</td>\n",
       "      <td>-0.112774</td>\n",
       "      <td>-0.201790</td>\n",
       "      <td>-0.094763</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.572632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freeswap</th>\n",
       "      <td>-0.081293</td>\n",
       "      <td>-0.116478</td>\n",
       "      <td>-0.350629</td>\n",
       "      <td>-0.302036</td>\n",
       "      <td>-0.237062</td>\n",
       "      <td>-0.130442</td>\n",
       "      <td>-0.153347</td>\n",
       "      <td>-0.220934</td>\n",
       "      <td>-0.225983</td>\n",
       "      <td>-0.245378</td>\n",
       "      <td>-0.213791</td>\n",
       "      <td>-0.210184</td>\n",
       "      <td>-0.121668</td>\n",
       "      <td>-0.278927</td>\n",
       "      <td>-0.253941</td>\n",
       "      <td>-0.130609</td>\n",
       "      <td>-0.245384</td>\n",
       "      <td>-0.433571</td>\n",
       "      <td>0.572632</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             lread    lwrite     scall     sread    swrite      fork  \\\n",
       "lread     1.000000  0.533737  0.191377  0.132881  0.119953  0.140284   \n",
       "lwrite    0.533737  1.000000  0.143404  0.128403  0.101524  0.052511   \n",
       "scall     0.191377  0.143404  1.000000  0.696887  0.619984  0.446766   \n",
       "sread     0.132881  0.128403  0.696887  1.000000  0.881069  0.416721   \n",
       "swrite    0.119953  0.101524  0.619984  0.881069  1.000000  0.376876   \n",
       "fork      0.140284  0.052511  0.446766  0.416721  0.376876  1.000000   \n",
       "exec      0.110965  0.038237  0.308999  0.164084  0.103643  0.763974   \n",
       "rchar     0.109624  0.117464  0.352609  0.504412  0.333322  0.280955   \n",
       "wchar     0.081270  0.091319  0.274199  0.401923  0.394356  0.061040   \n",
       "pgout     0.082463  0.067013  0.194529  0.193679  0.151371  0.130192   \n",
       "ppgout    0.130590  0.079485  0.208400  0.225906  0.159442  0.166872   \n",
       "pgfree    0.114438  0.065692  0.199778  0.212911  0.145458  0.168082   \n",
       "atch      0.021563  0.028310  0.077969  0.085468  0.061373  0.047194   \n",
       "pgin      0.189799  0.091068  0.241628  0.207012  0.147000  0.163468   \n",
       "ppgin     0.161345  0.089011  0.219070  0.210225  0.144278  0.132181   \n",
       "pflt      0.137463  0.067024  0.481781  0.452020  0.396580  0.931040   \n",
       "vflt      0.165539  0.094965  0.531760  0.491045  0.416571  0.939348   \n",
       "runqsz    0.023379  0.043562  0.000162  0.041096  0.014101 -0.017393   \n",
       "freemem  -0.083214 -0.091133 -0.387520 -0.286437 -0.248574 -0.123357   \n",
       "freeswap -0.081293 -0.116478 -0.350629 -0.302036 -0.237062 -0.130442   \n",
       "\n",
       "              exec     rchar     wchar     pgout    ppgout    pgfree  \\\n",
       "lread     0.110965  0.109624  0.081270  0.082463  0.130590  0.114438   \n",
       "lwrite    0.038237  0.117464  0.091319  0.067013  0.079485  0.065692   \n",
       "scall     0.308999  0.352609  0.274199  0.194529  0.208400  0.199778   \n",
       "sread     0.164084  0.504412  0.401923  0.193679  0.225906  0.212911   \n",
       "swrite    0.103643  0.333322  0.394356  0.151371  0.159442  0.145458   \n",
       "fork      0.763974  0.280955  0.061040  0.130192  0.166872  0.168082   \n",
       "exec      1.000000  0.168085  0.000640  0.111465  0.149334  0.146163   \n",
       "rchar     0.168085  1.000000  0.505186  0.210885  0.268056  0.276503   \n",
       "wchar     0.000640  0.505186  1.000000  0.192523  0.188655  0.159009   \n",
       "pgout     0.111465  0.210885  0.192523  1.000000  0.872445  0.730381   \n",
       "ppgout    0.149334  0.268056  0.188655  0.872445  1.000000  0.917790   \n",
       "pgfree    0.146163  0.276503  0.159009  0.730381  0.917790  1.000000   \n",
       "atch      0.052307  0.171322  0.181452  0.147759  0.093336  0.069290   \n",
       "pgin      0.186099  0.299051  0.177709  0.385648  0.488261  0.532834   \n",
       "ppgin     0.149911  0.346427  0.200614  0.414865  0.542392  0.593396   \n",
       "pflt      0.645239  0.313150  0.086396  0.151285  0.185941  0.190468   \n",
       "vflt      0.691754  0.363618  0.111358  0.229129  0.285708  0.301851   \n",
       "runqsz   -0.006048  0.131645  0.191111 -0.009814 -0.018781 -0.019083   \n",
       "freemem  -0.158565 -0.149771 -0.149335 -0.269687 -0.247554 -0.234195   \n",
       "freeswap -0.153347 -0.220934 -0.225983 -0.245378 -0.213791 -0.210184   \n",
       "\n",
       "              atch      pgin     ppgin      pflt      vflt    runqsz  \\\n",
       "lread     0.021563  0.189799  0.161345  0.137463  0.165539  0.023379   \n",
       "lwrite    0.028310  0.091068  0.089011  0.067024  0.094965  0.043562   \n",
       "scall     0.077969  0.241628  0.219070  0.481781  0.531760  0.000162   \n",
       "sread     0.085468  0.207012  0.210225  0.452020  0.491045  0.041096   \n",
       "swrite    0.061373  0.147000  0.144278  0.396580  0.416571  0.014101   \n",
       "fork      0.047194  0.163468  0.132181  0.931040  0.939348 -0.017393   \n",
       "exec      0.052307  0.186099  0.149911  0.645239  0.691754 -0.006048   \n",
       "rchar     0.171322  0.299051  0.346427  0.313150  0.363618  0.131645   \n",
       "wchar     0.181452  0.177709  0.200614  0.086396  0.111358  0.191111   \n",
       "pgout     0.147759  0.385648  0.414865  0.151285  0.229129 -0.009814   \n",
       "ppgout    0.093336  0.488261  0.542392  0.185941  0.285708 -0.018781   \n",
       "pgfree    0.069290  0.532834  0.593396  0.190468  0.301851 -0.019083   \n",
       "atch      1.000000  0.057639  0.057373  0.050594  0.095504  0.182370   \n",
       "pgin      0.057639  1.000000  0.923621  0.175644  0.303013  0.070547   \n",
       "ppgin     0.057373  0.923621  1.000000  0.149962  0.263352  0.071542   \n",
       "pflt      0.050594  0.175644  0.149962  1.000000  0.935370 -0.005786   \n",
       "vflt      0.095504  0.303013  0.263352  0.935370  1.000000  0.016180   \n",
       "runqsz    0.182370  0.070547  0.071542 -0.005786  0.016180  1.000000   \n",
       "freemem  -0.086029 -0.230779 -0.215318 -0.112774 -0.201790 -0.094763   \n",
       "freeswap -0.121668 -0.278927 -0.253941 -0.130609 -0.245384 -0.433571   \n",
       "\n",
       "           freemem  freeswap  \n",
       "lread    -0.083214 -0.081293  \n",
       "lwrite   -0.091133 -0.116478  \n",
       "scall    -0.387520 -0.350629  \n",
       "sread    -0.286437 -0.302036  \n",
       "swrite   -0.248574 -0.237062  \n",
       "fork     -0.123357 -0.130442  \n",
       "exec     -0.158565 -0.153347  \n",
       "rchar    -0.149771 -0.220934  \n",
       "wchar    -0.149335 -0.225983  \n",
       "pgout    -0.269687 -0.245378  \n",
       "ppgout   -0.247554 -0.213791  \n",
       "pgfree   -0.234195 -0.210184  \n",
       "atch     -0.086029 -0.121668  \n",
       "pgin     -0.230779 -0.278927  \n",
       "ppgin    -0.215318 -0.253941  \n",
       "pflt     -0.112774 -0.130609  \n",
       "vflt     -0.201790 -0.245384  \n",
       "runqsz   -0.094763 -0.433571  \n",
       "freemem   1.000000  0.572632  \n",
       "freeswap  0.572632  1.000000  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
