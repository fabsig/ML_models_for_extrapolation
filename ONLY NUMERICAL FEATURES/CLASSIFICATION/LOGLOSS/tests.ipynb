{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import setuptools\n",
    "import openml\n",
    "from sklearn.linear_model import LinearRegression \n",
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from engression import engression, engression_bagged\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rtdl_revisiting_models import MLP, ResNet, FTTransformer\n",
    "import random\n",
    "import gpytorch\n",
    "import tqdm.auto as tqdm\n",
    "import os\n",
    "from pygam import LinearGAM, s, f\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#SUITE_ID = 336 # Regression on numerical features\n",
    "SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(benchmark_suite.tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'for task_id in benchmark_suite.tasks:\\n    task = openml.tasks.get_task(task_id)\\n    dataset = task.get_dataset()\\n    X, y, categorical_indicator, attribute_names = dataset.get_data(\\n        dataset_format=\"dataframe\", target=dataset.default_target_attribute)\\n    print(y.value_counts())'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''for task_id in benchmark_suite.tasks:\n",
    "    task = openml.tasks.get_task(task_id)\n",
    "    dataset = task.get_dataset()\n",
    "    X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "    print(y.value_counts())'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('uint8')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "task_id=361055\n",
    "task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "dataset = task.get_dataset()\n",
    "\n",
    "X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "\n",
    "# Transform y to int type, to then be able to apply BCEWithLogitsLoss\n",
    "y=y.astype('int')\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "N_TRIALS=5\n",
    "N_SAMPLES=100\n",
    "seed=10\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "# calculate the mean and covariance matrix of the dataset\n",
    "mean = np.mean(X, axis=0)\n",
    "cov = np.cov(X.T)\n",
    "\n",
    "# calculate the Mahalanobis distance for each data point\n",
    "mahalanobis_dist = [mahalanobis(x, mean, np.linalg.inv(cov)) for x in X.values]\n",
    "\n",
    "mahalanobis_dist=pd.Series(mahalanobis_dist,index=X.index)\n",
    "far_index=mahalanobis_dist.index[np.where(mahalanobis_dist>=np.quantile(mahalanobis_dist,0.8))[0]]\n",
    "close_index=mahalanobis_dist.index[np.where(mahalanobis_dist<np.quantile(mahalanobis_dist,0.8))[0]]\n",
    "\n",
    "X_train = X.loc[close_index,:]\n",
    "X_test = X.loc[far_index,:]\n",
    "y_train = y.loc[close_index]\n",
    "y_test = y.loc[far_index]\n",
    "\n",
    "mean = np.mean(X_train, axis=0)\n",
    "cov = np.cov(X_train.T)\n",
    "\n",
    "# calculate the Mahalanobis distance for each data point\n",
    "mahalanobis_dist_ = [mahalanobis(x, mean, np.linalg.inv(cov)) for x in X_train.values]\n",
    "\n",
    "mahalanobis_dist_=pd.Series(mahalanobis_dist_,index=X_train.index)\n",
    "far_index_=mahalanobis_dist_.index[np.where(mahalanobis_dist_>=np.quantile(mahalanobis_dist_,0.8))[0]]\n",
    "close_index_=mahalanobis_dist_.index[np.where(mahalanobis_dist_<np.quantile(mahalanobis_dist_,0.8))[0]]\n",
    "\n",
    "X_train_ = X_train.loc[close_index_,:]\n",
    "X_val = X_train.loc[far_index_,:]\n",
    "y_train_ = y_train.loc[close_index_]\n",
    "y_val = y_train.loc[far_index_]\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train__tensor = torch.tensor(X_train_.values, dtype=torch.float32)\n",
    "y_train__tensor = torch.tensor(y_train_.values, dtype=torch.float32)\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Convert to use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    X_train__tensor = X_train__tensor.cuda()\n",
    "    y_train__tensor = y_train__tensor.cuda()\n",
    "    X_train_tensor = X_train_tensor.cuda()\n",
    "    y_train_tensor = y_train_tensor.cuda()\n",
    "    X_val_tensor = X_val_tensor.cuda()\n",
    "    y_val_tensor = y_val_tensor.cuda()\n",
    "    X_test_tensor = X_test_tensor.cuda()\n",
    "    y_test_tensor = y_test_tensor.cuda()\n",
    "\n",
    "# Create flattened versions of the data\n",
    "y_val_np = y_val.values.flatten()\n",
    "y_test_np = y_test.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-30 18:40:26,318] A new study created in memory with name: no-name-b3ff4b09-1560-464e-827d-d5c557220d80\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8193ac4a44044de8482bfac4ca9c424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-30 18:40:27,866] Trial 0 finished with value: 0.5237383177570093 and parameters: {'n_blocks': 4, 'd_block': 20, 'dropout': 0.6336482349262754, 'n_epochs': 75, 'learning_rate': 0.002215416944953109, 'weight_decay': 1.33040303714882e-07}. Best is trial 0 with value: 0.5237383177570093.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef7431b026fe42a6996e62e839d816a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-30 18:40:28,378] Trial 1 finished with value: 0.3024299065420561 and parameters: {'n_blocks': 1, 'd_block': 383, 'dropout': 0.16911083656253545, 'n_epochs': 9, 'learning_rate': 0.007075637776590665, 'weight_decay': 0.0005847452881552242}. Best is trial 0 with value: 0.5237383177570093.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb1b2bc042c3453fb0403bd07843ec5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/62 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-30 18:40:30,433] Trial 2 finished with value: 0.2927102803738318 and parameters: {'n_blocks': 1, 'd_block': 261, 'dropout': 0.8126209616521135, 'n_epochs': 62, 'learning_rate': 0.008871477434617912, 'weight_decay': 2.879919449586155e-07}. Best is trial 0 with value: 0.5237383177570093.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be77d6148bef4241a00fe17bc2c15e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-30 18:40:38,895] Trial 3 finished with value: 0.4033644859813084 and parameters: {'n_blocks': 5, 'd_block': 360, 'dropout': 0.5425443680112613, 'n_epochs': 15, 'learning_rate': 0.0010177368807699995, 'weight_decay': 2.3478377182859888e-05}. Best is trial 0 with value: 0.5237383177570093.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8330ac49fbe74cf1adb3691f7c22089c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-30 18:40:47,405] Trial 4 finished with value: 0.29233644859813085 and parameters: {'n_blocks': 3, 'd_block': 223, 'dropout': 0.6177669784693172, 'n_epochs': 52, 'learning_rate': 0.005693803629695728, 'weight_decay': 1.0120332166548561e-05}. Best is trial 0 with value: 0.5237383177570093.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb2e5ae1d25f4911987664ab27d9c023",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/75 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy MLP:  0.5770266227938977\n"
     ]
    }
   ],
   "source": [
    "# #### Define train function\n",
    "def train(model,criterion,loss_Adam,optimizer,training_iterations,X_train_tensor,y_train_tensor):\n",
    "    iterator = tqdm.tqdm(range(training_iterations), desc=\"Train\")\n",
    "\n",
    "    for _ in iterator:\n",
    "        # making a pridiction in forward pass\n",
    "        y_train_hat = model(X_train_tensor).reshape(-1,)\n",
    "        # calculating the loss between original and predicted data points\n",
    "        loss = criterion(y_train_hat, torch.Tensor(y_train_tensor))\n",
    "        # store loss into list\n",
    "        loss_Adam.append(loss.item())\n",
    "        # zeroing gradients after each iteration\n",
    "        optimizer.zero_grad()\n",
    "        # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n",
    "        loss.backward()\n",
    "        # updating the parameters after each iteration\n",
    "        optimizer.step()\n",
    "        iterator.set_postfix(loss=loss.item())\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "# #### MLP\n",
    "d_out = 1  \n",
    "d_in=X_train_.shape[1]\n",
    "\n",
    "def MLP_opt(trial):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    n_blocks = trial.suggest_int(\"n_blocks\", 1, 5)\n",
    "    d_block = trial.suggest_int(\"d_block\", 10, 500)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0, 1)\n",
    "\n",
    "    MLP_model = MLP(\n",
    "    d_in=d_in,\n",
    "    d_out=1,  # For binary classification, output dimension should be 1\n",
    "    n_blocks=n_blocks,\n",
    "    d_block=d_block,\n",
    "    dropout=dropout,\n",
    "    )\n",
    "    n_epochs=trial.suggest_int('n_epochs', 1, 100)\n",
    "    learning_rate=trial.suggest_float('learning_rate', 0.0001, 0.05, log=True)\n",
    "    weight_decay=trial.suggest_float('weight_decay', 1e-8, 1e-3, log=True)\n",
    "    optimizer=torch.optim.Adam(MLP_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()  # Use Binary Cross Entropy loss for binary classification\n",
    "    loss_Adam=[]\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        MLP_model = MLP_model.cuda()\n",
    "    \n",
    "    train(MLP_model,criterion,loss_Adam,optimizer,n_epochs,X_train__tensor,y_train__tensor)\n",
    "\n",
    "    # Point prediction\n",
    "    y_val_hat_MLP = torch.sigmoid(MLP_model(X_val_tensor).reshape(-1,))  # Apply sigmoid to get probabilities\n",
    "    accuracy_MLP = accuracy_score(y_val_tensor.cpu().numpy(), y_val_hat_MLP.ge(0.5).float().cpu().numpy())  # Calculate accuracy\n",
    "\n",
    "    return accuracy_MLP\n",
    "\n",
    "sampler_MLP = optuna.samplers.TPESampler(seed=seed)\n",
    "study_MLP = optuna.create_study(sampler=sampler_MLP, direction='maximize')  # We want to maximize accuracy\n",
    "study_MLP.optimize(MLP_opt, n_trials=N_TRIALS)\n",
    "\n",
    "MLP_model = MLP(\n",
    "    d_in=d_in,\n",
    "    d_out=1,  # For binary classification, output dimension should be 1\n",
    "    n_blocks=study_MLP.best_params['n_blocks'],\n",
    "    d_block=study_MLP.best_params['d_block'],\n",
    "    dropout=study_MLP.best_params['dropout'],\n",
    "    )\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    MLP_model = MLP_model.cuda()\n",
    "    \n",
    "n_epochs=study_MLP.best_params['n_epochs']\n",
    "learning_rate=study_MLP.best_params['learning_rate']\n",
    "weight_decay=study_MLP.best_params['weight_decay']\n",
    "optimizer=torch.optim.Adam(MLP_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()  # Use Binary Cross Entropy loss for binary classification\n",
    "loss_Adam=[]\n",
    "\n",
    "train(MLP_model,criterion,loss_Adam,optimizer,n_epochs,X_train_tensor,y_train_tensor)\n",
    "\n",
    "# Point prediction\n",
    "y_test_hat_MLP = torch.sigmoid(MLP_model(X_test_tensor).reshape(-1,))  # Apply sigmoid to get probabilities\n",
    "accuracy_MLP = accuracy_score(y_test_tensor.cpu().numpy(), y_test_hat_MLP.ge(0.5).float().cpu().numpy())  # Calculate accuracy\n",
    "print(\"Accuracy MLP: \", accuracy_MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-30 18:41:35,770] A new study created in memory with name: no-name-33f8520b-de15-43c3-98c6-49a88fa307ad\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746c34aec4f94cf499722432e811dc02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/23 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-30 18:41:37,102] Trial 0 finished with value: 0.4672897196261682 and parameters: {'n_blocks': 4, 'd_block': 20, 'dropout1': 0.6336482349262754, 'dropout2': 0.7488038825386119, 'd_hidden_multiplier': 1.7462675307564761, 'n_epochs': 23, 'learning_rate': 0.000342425210145967, 'weight_decay': 6.348243270946383e-05}. Best is trial 0 with value: 0.4672897196261682.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2837c76661fe43d1a600e6c37b9c6ad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-30 18:41:38,381] Trial 1 finished with value: 0.5364485981308411 and parameters: {'n_blocks': 1, 'd_block': 53, 'dropout1': 0.6853598183677972, 'dropout2': 0.9533933461949365, 'd_hidden_multiplier': 0.5098706658197861, 'n_epochs': 52, 'learning_rate': 0.015604131457893529, 'weight_decay': 1.155128593079557e-05}. Best is trial 1 with value: 0.5364485981308411.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2ebdc20c3524c98b59940852cf8b35d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/15 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-30 18:41:45,953] Trial 2 finished with value: 0.2930841121495327 and parameters: {'n_blocks': 4, 'd_block': 153, 'dropout1': 0.9177741225129434, 'dropout2': 0.7145757833976906, 'd_hidden_multiplier': 1.8563609200281532, 'n_epochs': 15, 'learning_rate': 0.0010177368807699995, 'weight_decay': 2.3478377182859888e-05}. Best is trial 1 with value: 0.5364485981308411.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "017185c33f894e179a9614faac6cb50e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/61 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-30 18:42:31,859] Trial 3 finished with value: 0.4291588785046729 and parameters: {'n_blocks': 3, 'd_block': 223, 'dropout1': 0.6177669784693172, 'dropout2': 0.5131382425543909, 'd_hidden_multiplier': 2.125992954828668, 'n_epochs': 61, 'learning_rate': 0.014902984681415245, 'weight_decay': 4.057287303826027e-06}. Best is trial 1 with value: 0.5364485981308411.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "532c7f064b374558bad657578e2cfb84",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/83 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-30 18:43:03,974] Trial 4 finished with value: 0.2930841121495327 and parameters: {'n_blocks': 5, 'd_block': 166, 'dropout1': 0.09045934927090737, 'dropout2': 0.30070005663620336, 'd_hidden_multiplier': 0.7849609046588744, 'n_epochs': 83, 'learning_rate': 0.00013383563361780206, 'weight_decay': 1.3534298216580227e-05}. Best is trial 1 with value: 0.5364485981308411.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3350dea230a4933a1798daffc06aab7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/52 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy ResNet:  0.3104995513012264\n"
     ]
    }
   ],
   "source": [
    "d_out = 1  \n",
    "d_in=X_train_.shape[1]\n",
    "\n",
    "def ResNet_opt(trial):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    n_blocks = trial.suggest_int(\"n_blocks\", 1, 5)\n",
    "    d_block = trial.suggest_int(\"d_block\", 10, 500)\n",
    "    dropout1 = trial.suggest_float(\"dropout1\", 0, 1)\n",
    "    dropout2 = trial.suggest_float(\"dropout2\", 0, 1)\n",
    "    d_hidden_multiplier=trial.suggest_float(\"d_hidden_multiplier\", 0.5, 3)\n",
    "\n",
    "    ResNet_model = ResNet(\n",
    "    d_in=d_in,\n",
    "    d_out=1,  # For binary classification, output dimension should be 1\n",
    "    n_blocks=n_blocks,\n",
    "    d_block=d_block,\n",
    "    d_hidden=None,\n",
    "    d_hidden_multiplier=d_hidden_multiplier,\n",
    "    dropout1=dropout1,\n",
    "    dropout2=dropout2,\n",
    "    )\n",
    "    if torch.cuda.is_available():\n",
    "        ResNet_model = ResNet_model.cuda()\n",
    "    n_epochs=trial.suggest_int('n_epochs', 1, 100)\n",
    "    learning_rate=trial.suggest_float('learning_rate', 0.0001, 0.05, log=True)\n",
    "    weight_decay=trial.suggest_float('weight_decay', 1e-8, 1e-3, log=True)\n",
    "    optimizer=torch.optim.Adam(ResNet_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()  # Use Binary Cross Entropy loss for binary classification\n",
    "    loss_Adam=[]\n",
    "\n",
    "    train(ResNet_model,criterion,loss_Adam,optimizer,n_epochs,X_train__tensor,y_train__tensor)\n",
    "\n",
    "    # Point prediction\n",
    "    y_val_hat_ResNet = torch.sigmoid(ResNet_model(X_val_tensor).reshape(-1,))  # Apply sigmoid to get probabilities\n",
    "    accuracy_ResNet = accuracy_score(y_val_tensor.cpu().numpy(), y_val_hat_ResNet.ge(0.5).float().cpu().numpy())  # Calculate accuracy\n",
    "\n",
    "    return accuracy_ResNet\n",
    "\n",
    "sampler_ResNet = optuna.samplers.TPESampler(seed=seed)\n",
    "study_ResNet = optuna.create_study(sampler=sampler_ResNet, direction='maximize')  # We want to maximize accuracy\n",
    "study_ResNet.optimize(ResNet_opt, n_trials=N_TRIALS)\n",
    "\n",
    "ResNet_model = ResNet(\n",
    "    d_in=d_in,\n",
    "    d_out=1,  # For binary classification, output dimension should be 1\n",
    "    n_blocks=study_ResNet.best_params['n_blocks'],\n",
    "    d_block=study_ResNet.best_params['d_block'],\n",
    "    d_hidden=None,\n",
    "    d_hidden_multiplier=study_ResNet.best_params['d_hidden_multiplier'],\n",
    "    dropout1=study_ResNet.best_params['dropout1'],\n",
    "    dropout2=study_ResNet.best_params['dropout2'],\n",
    "    )\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    ResNet_model = ResNet_model.cuda()\n",
    "\n",
    "n_epochs=study_ResNet.best_params['n_epochs']\n",
    "learning_rate=study_ResNet.best_params['learning_rate']\n",
    "weight_decay=study_ResNet.best_params['weight_decay']\n",
    "optimizer=torch.optim.Adam(ResNet_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()  # Use Binary Cross Entropy loss for binary classification\n",
    "loss_Adam=[]\n",
    "\n",
    "train(ResNet_model,criterion,loss_Adam,optimizer,n_epochs,X_train_tensor,y_train_tensor)\n",
    "\n",
    "# Point prediction\n",
    "y_test_hat_ResNet = torch.sigmoid(ResNet_model(X_test_tensor).reshape(-1,))  # Apply sigmoid to get probabilities\n",
    "accuracy_ResNet = accuracy_score(y_test_tensor.cpu().numpy(), y_test_hat_ResNet.ge(0.5).float().cpu().numpy())  # Calculate accuracy\n",
    "print(\"Accuracy ResNet: \", accuracy_ResNet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-30 18:43:50,479] A new study created in memory with name: no-name-26b1d987-23fe-4e9e-870e-6f9839d4e3b3\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea3b44daa22b42f3b96c23f87d5872a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-30 18:44:05,395] Trial 0 finished with value: 0.4654205607476635 and parameters: {'n_blocks': 4, 'd_block_multiplier': 1, 'attention_n_heads': 13, 'attention_dropout': 0.7488038825386119, 'ffn_d_hidden_multiplier': 1.7462675307564761, 'ffn_dropout': 0.22479664553084766, 'residual_dropout': 0.19806286475962398, 'n_epochs': 3, 'learning_rate': 0.0002860388842288948, 'weight_decay': 2.765025054332623e-08}. Best is trial 0 with value: 0.4654205607476635.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b07e2bbb2c443a28a93016d11d62285",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-30 18:44:07,731] Trial 1 finished with value: 0.29121495327102803 and parameters: {'n_blocks': 4, 'd_block_multiplier': 24, 'attention_n_heads': 1, 'attention_dropout': 0.5121922633857766, 'ffn_d_hidden_multiplier': 2.531552404130284, 'ffn_dropout': 0.6125260668293881, 'residual_dropout': 0.7217553174317995, 'n_epochs': 1, 'learning_rate': 0.029994721053560828, 'weight_decay': 3.7400629930578146e-05}. Best is trial 0 with value: 0.4654205607476635.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4f9e21ead634493e9ac12478efe3868b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-30 18:44:14,214] Trial 2 finished with value: 0.32560747663551404 and parameters: {'n_blocks': 3, 'd_block_multiplier': 4, 'attention_n_heads': 8, 'attention_dropout': 0.6741336150663453, 'ffn_d_hidden_multiplier': 1.6045829360574901, 'ffn_dropout': 0.4340139933332937, 'residual_dropout': 0.6177669784693172, 'n_epochs': 2, 'learning_rate': 0.005693803629695728, 'weight_decay': 1.0120332166548561e-05}. Best is trial 0 with value: 0.4654205607476635.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81079b51dd13410c898f1cff88143f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# #### FFTransformer\n",
    "\n",
    "def train_trans(model,criterion,loss_Adam,optimizer,training_iterations,X_train_tensor,y_train_tensor):\n",
    "    iterator = tqdm.tqdm(range(training_iterations), desc=\"Train\")\n",
    "\n",
    "    for _ in iterator:\n",
    "        # making a pridiction in forward pass\n",
    "        y_train_hat = model(X_train_tensor, None).reshape(-1,)\n",
    "        # calculating the loss between original and predicted data points\n",
    "        loss = criterion(y_train_hat, torch.Tensor(y_train_tensor))\n",
    "        # store loss into list\n",
    "        loss_Adam.append(loss.item())\n",
    "        # zeroing gradients after each iteration\n",
    "        optimizer.zero_grad()\n",
    "        # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n",
    "        loss.backward()\n",
    "        # updating the parameters after each iteration\n",
    "        optimizer.step()\n",
    "        iterator.set_postfix(loss=loss.item())\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "d_out = 1  \n",
    "d_in=X_train_.shape[1]\n",
    "\n",
    "def FTTrans_opt(trial):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    n_blocks = trial.suggest_int(\"n_blocks\", 1, 5)\n",
    "    d_block_multiplier = trial.suggest_int(\"d_block_multiplier\", 1, 25)\n",
    "    attention_n_heads = trial.suggest_int(\"attention_n_heads\", 1, 20)\n",
    "    attention_dropout = trial.suggest_float(\"attention_dropout\", 0, 1)\n",
    "    ffn_d_hidden_multiplier=trial.suggest_float(\"ffn_d_hidden_multiplier\", 0.5, 3)\n",
    "    ffn_dropout = trial.suggest_float(\"ffn_dropout\", 0, 1)\n",
    "    residual_dropout = trial.suggest_float(\"residual_dropout\", 0, 1)\n",
    "\n",
    "    FTTrans_model = FTTransformer(\n",
    "    n_cont_features=d_in,\n",
    "    cat_cardinalities=[],\n",
    "    d_out=1,  # For binary classification, output dimension should be 1\n",
    "    n_blocks=n_blocks,\n",
    "    d_block=d_block_multiplier*attention_n_heads,\n",
    "    attention_n_heads=attention_n_heads,\n",
    "    attention_dropout=attention_dropout,\n",
    "    ffn_d_hidden=None,\n",
    "    ffn_d_hidden_multiplier=ffn_d_hidden_multiplier,\n",
    "    ffn_dropout=ffn_dropout,\n",
    "    residual_dropout=residual_dropout,\n",
    "    )\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        FTTrans_model = FTTrans_model.cuda()\n",
    "\n",
    "    n_epochs=trial.suggest_int('n_epochs', 1, 3)\n",
    "    learning_rate=trial.suggest_float('learning_rate', 0.0001, 0.05, log=True)\n",
    "    weight_decay=trial.suggest_float('weight_decay', 1e-8, 1e-3, log=True)\n",
    "    optimizer=torch.optim.Adam(FTTrans_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()  # Use Binary Cross Entropy loss for binary classification\n",
    "    loss_Adam=[]\n",
    "\n",
    "    train_trans(FTTrans_model,criterion,loss_Adam,optimizer,n_epochs,X_train__tensor,y_train__tensor)\n",
    "\n",
    "    # Point prediction\n",
    "    y_val_hat_FTTrans = torch.sigmoid(FTTrans_model(X_val_tensor, None).reshape(-1,))  # Apply sigmoid to get probabilities\n",
    "    accuracy_FTTrans = accuracy_score(y_val_tensor.cpu().numpy(), y_val_hat_FTTrans.ge(0.5).float().cpu().numpy())  # Calculate accuracy\n",
    "\n",
    "    return accuracy_FTTrans\n",
    "\n",
    "sampler_FTTrans = optuna.samplers.TPESampler(seed=seed)\n",
    "study_FTTrans = optuna.create_study(sampler=sampler_FTTrans, direction='maximize')  # We want to maximize accuracy\n",
    "study_FTTrans.optimize(FTTrans_opt, n_trials=N_TRIALS)\n",
    "\n",
    "\n",
    "FTTrans_model = FTTransformer(\n",
    "    n_cont_features=d_in,\n",
    "    cat_cardinalities=[],\n",
    "    d_out=1,  # For binary classification, output dimension should be 1\n",
    "    n_blocks=study_FTTrans.best_params['n_blocks'],\n",
    "    d_block=study_FTTrans.best_params['d_block_multiplier']*study_FTTrans.best_params['attention_n_heads'],\n",
    "    attention_n_heads=study_FTTrans.best_params['attention_n_heads'],\n",
    "    attention_dropout=study_FTTrans.best_params['attention_dropout'],\n",
    "    ffn_d_hidden=None,\n",
    "    ffn_d_hidden_multiplier=study_FTTrans.best_params['ffn_d_hidden_multiplier'],\n",
    "    ffn_dropout=study_FTTrans.best_params['ffn_dropout'],\n",
    "    residual_dropout=study_FTTrans.best_params['residual_dropout'],\n",
    "    )\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    FTTrans_model = FTTrans_model.cuda()\n",
    "\n",
    "n_epochs=study_FTTrans.best_params['n_epochs']\n",
    "learning_rate=study_FTTrans.best_params['learning_rate']\n",
    "weight_decay=study_FTTrans.best_params['weight_decay']\n",
    "optimizer=torch.optim.Adam(FTTrans_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()  # Use Binary Cross Entropy loss for binary classification\n",
    "loss_Adam=[]\n",
    "\n",
    "train_trans(FTTrans_model,criterion,loss_Adam,optimizer,n_epochs,X_train_tensor,y_train_tensor)\n",
    "\n",
    "# Point prediction\n",
    "y_test_hat_FTTrans = torch.sigmoid(FTTrans_model(X_test_tensor, None).reshape(-1,))  # Apply sigmoid to get probabilities\n",
    "accuracy_FTTrans = accuracy_score(y_test_tensor.cpu().numpy(), y_test_hat_FTTrans.ge(0.5).float().cpu().numpy())  # Calculate accuracy\n",
    "print(\"Accuracy FTTrans: \", accuracy_FTTrans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import setuptools\n",
    "import openml\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from engression import engression, engression_bagged\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rtdl_revisiting_models import MLP, ResNet, FTTransformer\n",
    "import random\n",
    "import gpytorch\n",
    "import tqdm.auto as tqdm\n",
    "import os\n",
    "from pygam import LogisticGAM, s\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#SUITE_ID = 336 # Regression on numerical features\n",
    "SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "\n",
    "task_id=361055\n",
    "task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "dataset = task.get_dataset()\n",
    "\n",
    "X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "\n",
    "# Transform y to int type, to then be able to apply BCEWithLogitsLoss\n",
    "y=y.astype('int')\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "N_TRIALS=2\n",
    "N_SAMPLES=100\n",
    "seed=10\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "# calculate the mean and covariance matrix of the dataset\n",
    "mean = np.mean(X, axis=0)\n",
    "cov = np.cov(X.T)\n",
    "\n",
    "# calculate the Mahalanobis distance for each data point\n",
    "mahalanobis_dist = [mahalanobis(x, mean, np.linalg.inv(cov)) for x in X.values]\n",
    "\n",
    "mahalanobis_dist=pd.Series(mahalanobis_dist,index=X.index)\n",
    "far_index=mahalanobis_dist.index[np.where(mahalanobis_dist>=np.quantile(mahalanobis_dist,0.8))[0]]\n",
    "close_index=mahalanobis_dist.index[np.where(mahalanobis_dist<np.quantile(mahalanobis_dist,0.8))[0]]\n",
    "\n",
    "X_train = X.loc[close_index,:]\n",
    "X_test = X.loc[far_index,:]\n",
    "y_train = y.loc[close_index]\n",
    "y_test = y.loc[far_index]\n",
    "\n",
    "mean = np.mean(X_train, axis=0)\n",
    "cov = np.cov(X_train.T)\n",
    "\n",
    "# calculate the Mahalanobis distance for each data point\n",
    "mahalanobis_dist_ = [mahalanobis(x, mean, np.linalg.inv(cov)) for x in X_train.values]\n",
    "\n",
    "mahalanobis_dist_=pd.Series(mahalanobis_dist_,index=X_train.index)\n",
    "far_index_=mahalanobis_dist_.index[np.where(mahalanobis_dist_>=np.quantile(mahalanobis_dist_,0.8))[0]]\n",
    "close_index_=mahalanobis_dist_.index[np.where(mahalanobis_dist_<np.quantile(mahalanobis_dist_,0.8))[0]]\n",
    "\n",
    "X_train_ = X_train.loc[close_index_,:]\n",
    "X_val = X_train.loc[far_index_,:]\n",
    "y_train_ = y_train.loc[close_index_]\n",
    "y_val = y_train.loc[far_index_]\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train__tensor = torch.tensor(X_train_.values, dtype=torch.float32)\n",
    "y_train__tensor = torch.tensor(y_train_.values, dtype=torch.float32)\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Convert to use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    X_train__tensor = X_train__tensor.cuda()\n",
    "    y_train__tensor = y_train__tensor.cuda()\n",
    "    X_train_tensor = X_train_tensor.cuda()\n",
    "    y_train_tensor = y_train_tensor.cuda()\n",
    "    X_val_tensor = X_val_tensor.cuda()\n",
    "    y_val_tensor = y_val_tensor.cuda()\n",
    "    X_test_tensor = X_test_tensor.cuda()\n",
    "    y_test_tensor = y_test_tensor.cuda()\n",
    "\n",
    "# Create flattened versions of the data\n",
    "y_val_np = y_val.values.flatten()\n",
    "y_test_np = y_test.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-30 17:22:58,030] A new study created in memory with name: no-name-a9479560-c8b2-457d-9e60-17d267dba0d7\n",
      "[I 2024-01-30 17:22:58,162] Trial 0 finished with value: 0.8160747663551402 and parameters: {'learning_rate': 0.12071779104534666, 'n_estimators': 108, 'reg_lambda': 0.005044685709888605, 'max_depth': 23, 'min_child_samples': 55}. Best is trial 0 with value: 0.8160747663551402.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 4080, number of negative: 6616\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000355 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 871\n",
      "[LightGBM] [Info] Number of data points in the train set: 10696, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.381451 -> initscore=-0.483394\n",
      "[LightGBM] [Info] Start training from score -0.483394\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 4080, number of negative: 6616\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000296 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 871\n",
      "[LightGBM] [Info] Number of data points in the train set: 10696, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.381451 -> initscore=-0.483394\n",
      "[LightGBM] [Info] Start training from score -0.483394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-30 17:22:58,310] Trial 1 finished with value: 0.7547663551401869 and parameters: {'learning_rate': 0.004043145805966843, 'n_estimators': 179, 'reg_lambda': 0.0699481785242808, 'max_depth': 6, 'min_child_samples': 18}. Best is trial 0 with value: 0.8160747663551402.\n",
      "[I 2024-01-30 17:22:58,312] A new study created in memory with name: no-name-eb52592e-93e1-4bea-9990-bab4c9266d7b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-30 17:23:00,957] Trial 0 finished with value: 0.7293457943925233 and parameters: {'n_estimators': 409, 'max_depth': 1, 'max_features': 20, 'min_samples_leaf': 78}. Best is trial 0 with value: 0.7293457943925233.\n",
      "[I 2024-01-30 17:23:05,578] Trial 1 finished with value: 0.7742056074766355 and parameters: {'n_estimators': 299, 'max_depth': 7, 'max_features': 6, 'min_samples_leaf': 79}. Best is trial 1 with value: 0.7742056074766355.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Number of positive: 5971, number of negative: 7400\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000514 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 885\n",
      "[LightGBM] [Info] Number of data points in the train set: 13371, number of used features: 10\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.446563 -> initscore=-0.214566\n",
      "[LightGBM] [Info] Start training from score -0.214566\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "[I 2024-01-30 17:23:11,553] A new study created in memory with name: no-name-2fbe94fa-4558-4448-aa8c-e475d193bafe\n",
      "C:\\Users\\dalma\\AppData\\Local\\Temp\\ipykernel_54032\\3665157655.py:66: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lam': trial.suggest_loguniform('lam', 1e-3, 1)}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy logistic regression:  0.7870176488184266\n",
      "Accuracy boosted trees:  0.8232126832186659\n",
      "Accuracy random forest:  0.8196230930302124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-01-30 17:23:11,873] Trial 0 finished with value: 0.6919626168224299 and parameters: {'n_splines': 17, 'lam': 0.001154132971137168}. Best is trial 0 with value: 0.6919626168224299.\n",
      "C:\\Users\\dalma\\AppData\\Local\\Temp\\ipykernel_54032\\3665157655.py:66: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  'lam': trial.suggest_loguniform('lam', 1e-3, 1)}\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:135: RuntimeWarning: overflow encountered in exp\n",
      "  elp = np.exp(lp)\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\pygam\\links.py:136: RuntimeWarning: invalid value encountered in divide\n",
      "  return dist.levels * elp / (elp + 1)\n",
      "[I 2024-01-30 17:23:12,109] Trial 1 finished with value: 0.6923364485981308 and parameters: {'n_splines': 15, 'lam': 0.17636469336159113}. Best is trial 1 with value: 0.6923364485981308.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy GAM:  0.7218067603948549\n"
     ]
    }
   ],
   "source": [
    "def boosted(trial):\n",
    "\n",
    "    params = {'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.5, log=True),\n",
    "              'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "              'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "              'max_depth': trial.suggest_int('max_depth', 1, 30),\n",
    "              'min_child_samples': trial.suggest_int('min_child_samples', 10, 100)}\n",
    "    \n",
    "    boosted_tree_model=lgbm.LGBMClassifier(**params)\n",
    "    boosted_tree_model.fit(X_train_, y_train_)\n",
    "    y_val_hat_boost=boosted_tree_model.predict(X_val)\n",
    "    accuracy_boost=accuracy_score(y_val, y_val_hat_boost)\n",
    "\n",
    "    return accuracy_boost\n",
    "\n",
    "sampler_boost = optuna.samplers.TPESampler(seed=seed)\n",
    "study_boost = optuna.create_study(sampler=sampler_boost, direction='maximize')\n",
    "study_boost.optimize(boosted, n_trials=N_TRIALS)\n",
    "boosted_model=lgbm.LGBMClassifier(**study_boost.best_params)\n",
    "\n",
    "def rf(trial):\n",
    "\n",
    "    params = {'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "              'max_depth': trial.suggest_int('max_depth', 1, 30),\n",
    "              'max_features': trial.suggest_int('max_features', 1, 30),\n",
    "              'min_samples_leaf': trial.suggest_int('min_samples_leaf', 10, 100)}\n",
    "    \n",
    "    rf_model=RandomForestClassifier(**params)\n",
    "    rf_model.fit(X_train_, y_train_)\n",
    "    y_val_hat_rf=rf_model.predict(X_val)\n",
    "    accuracy_rf=accuracy_score(y_val, y_val_hat_rf)\n",
    "\n",
    "    return accuracy_rf\n",
    "\n",
    "sampler_rf = optuna.samplers.TPESampler(seed=seed)\n",
    "study_rf = optuna.create_study(sampler=sampler_rf, direction='maximize')\n",
    "study_rf.optimize(rf, n_trials=N_TRIALS)\n",
    "rf_model=RandomForestClassifier(**study_rf.best_params)\n",
    "\n",
    "\n",
    "# Fit the boosted model and make predictions\n",
    "boosted_model.fit(X_train, y_train)\n",
    "y_test_hat_boosted = boosted_model.predict(X_test)\n",
    "accuracy_boosted = accuracy_score(y_test, y_test_hat_boosted)\n",
    "\n",
    "# Fit the random forest model and make predictions\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_test_hat_rf = rf_model.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test, y_test_hat_rf)\n",
    "\n",
    "# Fit the logistic regression model and make predictions\n",
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_test_hat_logreg = log_reg.predict(X_test)\n",
    "accuracy_logreg = accuracy_score(y_test, y_test_hat_logreg)\n",
    "\n",
    "print(\"Accuracy logistic regression: \", accuracy_logreg)\n",
    "print(\"Accuracy boosted trees: \", accuracy_boosted)\n",
    "print(\"Accuracy random forest: \", accuracy_rf)\n",
    "\n",
    "# GAM model\n",
    "def gam_model(trial):\n",
    "\n",
    "    # Define the hyperparameters to optimize\n",
    "    params = {'n_splines': trial.suggest_int('n_splines', 5, 20),\n",
    "              'lam': trial.suggest_loguniform('lam', 1e-3, 1)}\n",
    "\n",
    "    # Create and train the model\n",
    "    gam = LogisticGAM(s(0, n_splines=params['n_splines'], lam=params['lam'])).fit(X_train_, y_train_)\n",
    "\n",
    "    # Predict on the validation set and calculate the accuracy\n",
    "    y_val_hat_gam = gam.predict(X_val)\n",
    "    accuracy_gam = accuracy_score(y_val, y_val_hat_gam)\n",
    "\n",
    "    return accuracy_gam\n",
    "\n",
    "# Create the sampler and study\n",
    "sampler_gam = optuna.samplers.TPESampler(seed=seed)\n",
    "study_gam = optuna.create_study(sampler=sampler_gam, direction='maximize')\n",
    "\n",
    "# Optimize the model\n",
    "study_gam.optimize(gam_model, n_trials=N_TRIALS)\n",
    "\n",
    "# Create the final model with the best parameters\n",
    "best_params = study_gam.best_params\n",
    "final_gam_model = LogisticGAM(s(0, n_splines=best_params['n_splines'], lam=best_params['lam']))\n",
    "\n",
    "# Fit the model\n",
    "final_gam_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_hat_gam = final_gam_model.predict(X_test)\n",
    "# Calculate the accuracy\n",
    "accuracy_gam = accuracy_score(y_test, y_test_hat_gam)\n",
    "print(\"Accuracy GAM: \", accuracy_gam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d5eb522d8094cfba8e56ea16354c38a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\linear_operator\\utils\\linear_cg.py:338: NumericalWarning: CG terminated in 1000 iterations with average residual norm 12.572010040283203 which is larger than the tolerance of 0.01 specified by linear_operator.settings.cg_tolerance. If performance is affected, consider raising the maximum number of CG iterations by running code in a linear_operator.settings.max_cg_iterations(value) context.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\gpytorch\\likelihoods\\gaussian_likelihood.py:300: GPInputWarning: You have passed data through a FixedNoiseGaussianLikelihood that did not match the size of the fixed noise, *and* you did not specify noise. This is treated as a no-op.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [2675, 2]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 69\u001b[0m\n\u001b[0;32m     66\u001b[0m     preds \u001b[38;5;241m=\u001b[39m likelihood(output)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[1;32m---> 69\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_val_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreds\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mge\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Update the best kernel if the current kernel has a higher accuracy\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m accuracy \u001b[38;5;241m>\u001b[39m best_accuracy:\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:213\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:85\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     60\u001b[0m \n\u001b[0;32m     61\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     86\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_true\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     87\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my_pred\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:430\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    428\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 430\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    431\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    432\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    433\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [2675, 2]"
     ]
    }
   ],
   "source": [
    "class DirichletGPModel(gpytorch.models.ExactGP):\n",
    "    \n",
    "    def __init__(self, train_x, train_y, likelihood, num_classes, kernel):\n",
    "        super(DirichletGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size((num_classes,)))\n",
    "        self.covar_module = kernel\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "# Define the learning params\n",
    "training_iterations = 1\n",
    "\n",
    "# Define the kernels\n",
    "kernels = [\n",
    "    gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=0.5, ard_num_dims=X_train_.shape[1])),\n",
    "    gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=X_train_.shape[1])),\n",
    "    gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=2.5, ard_num_dims=X_train_.shape[1])),\n",
    "    gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=X_train_.shape[1])),\n",
    "]\n",
    "\n",
    "best_accuracy = 0\n",
    "best_kernel = None\n",
    "\n",
    "def train(model,X_train_tensor,y_train_tensor):\n",
    "    iterator = tqdm.tqdm(range(training_iterations), desc=\"Train\")\n",
    "\n",
    "    for _ in iterator:\n",
    "        # Zero backprop gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        output = model(X_train_tensor)\n",
    "        # Calc loss and backprop derivatives\n",
    "        loss = -mll(output, y_train_tensor).sum()\n",
    "        loss.backward()\n",
    "        iterator.set_postfix(loss=loss.item())\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "for kernel in kernels:\n",
    "    # Initialize the Gaussian Process model and likelihood\n",
    "    likelihood = gpytorch.likelihoods.DirichletClassificationLikelihood(y_train__tensor.long(), learn_additional__noise=True)\n",
    "    model = DirichletGPModel(X_train__tensor, likelihood.transformed_targets, likelihood, num_classes=likelihood.num_classes, kernel=kernel)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    # Train the model\n",
    "    train(model,X_train__tensor,y_train__tensor.long())\n",
    "    \n",
    "    # Set the model in evaluation mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        output = model(X_val_tensor)\n",
    "        preds = likelihood(output)\n",
    "\n",
    "    # Calculate accuracy\n",
    "    accuracy = accuracy_score(y_val_tensor, preds.mean.ge(0.5).float())\n",
    "\n",
    "    # Update the best kernel if the current kernel has a higher accuracy\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_kernel = kernel\n",
    "\n",
    "        \n",
    "\n",
    "class DirichletGPModel(gpytorch.models.ExactGP):\n",
    "    \n",
    "    def __init__(self, train_x, train_y, likelihood, num_classes):\n",
    "        super(DirichletGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size((num_classes,)))\n",
    "        self.covar_module = best_kernel\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Initialize the Gaussian Process model and likelihood\n",
    "likelihood = gpytorch.likelihoods.DirichletClassificationLikelihood(y_train_tensor.long(), learn_additional_noise=True)\n",
    "model = DirichletGPModel(X_train_tensor, likelihood.transformed_targets, likelihood, num_classes=likelihood.num_classes)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "# Train the model\n",
    "train(model,X_train_tensor,y_train_tensor.long())\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Make predictions on the validation set\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    output = model(X_test_tensor)\n",
    "    preds = likelihood(output)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy_GP = accuracy_score(y_test_tensor, preds.mean.argmax(dim=1))\n",
    "print(\"Accuracy GP: \", accuracy_GP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([13371, 10])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-30 18:17:16,304] A new study created in memory with name: no-name-e3e67ada-b8b6-49b3-9383-6a9426722612\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Residual blocks (skip-connections) are typically recommended for more than 2 layers; turn it on by setting resblock=True.\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 11] energy-loss: 0.1685,  E(|Y-Yhat|): 0.3708,  E(|Yhat-Yhat'|): 0.4045\n",
      "[Epoch 100 (84%), batch 11] energy-loss: 0.1775,  E(|Y-Yhat|): 0.3623,  E(|Yhat-Yhat'|): 0.3696\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1698,  E(|Y-Yhat|): 0.3465,  E(|Yhat-Yhat'|): 0.3535\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-30 18:21:43,643] Trial 0 finished with value: 0.8 and parameters: {'learning_rate': 0.0034885205571560775, 'num_epoches': 118, 'num_layer': 4, 'hidden_dim': 400}. Best is trial 0 with value: 0.8.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 11] energy-loss: 0.3275,  E(|Y-Yhat|): 0.4255,  E(|Yhat-Yhat'|): 0.1961\n",
      "[Epoch 100 (33%), batch 11] energy-loss: 0.1882,  E(|Y-Yhat|): 0.3657,  E(|Yhat-Yhat'|): 0.3549\n",
      "[Epoch 200 (66%), batch 11] energy-loss: 0.1783,  E(|Y-Yhat|): 0.3528,  E(|Yhat-Yhat'|): 0.3490\n",
      "[Epoch 300 (99%), batch 11] energy-loss: 0.1864,  E(|Y-Yhat|): 0.3561,  E(|Yhat-Yhat'|): 0.3394\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1735,  E(|Y-Yhat|): 0.3456,  E(|Yhat-Yhat'|): 0.3441\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-30 18:25:17,712] Trial 1 finished with value: 0.8213084112149532 and parameters: {'learning_rate': 0.000993148119483195, 'num_epoches': 302, 'num_layer': 2, 'hidden_dim': 404}. Best is trial 1 with value: 0.8213084112149532.\n"
     ]
    }
   ],
   "source": [
    "def engressor_NN(trial):\n",
    "\n",
    "    params = {'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.01, log=True),\n",
    "              'num_epoches': trial.suggest_int('num_epoches', 100, 1000),\n",
    "              'num_layer': trial.suggest_int('num_layer', 2, 5),\n",
    "              'hidden_dim': trial.suggest_int('hidden_dim', 100, 500),}\n",
    "    params['noise_dim']=params['hidden_dim']\n",
    "\n",
    "    # Check if CUDA is available and if so, move the tensors and the model to the GPU\n",
    "    if torch.cuda.is_available():\n",
    "        engressor_model=engression(X_train__tensor, y_train__tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=1000, sigmoid=True, device=\"cuda\")\n",
    "    else: \n",
    "        engressor_model=engression(X_train__tensor, y_train__tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=1000, sigmoid=True)\n",
    "    \n",
    "    # Generate a sample from the engression model for each data point\n",
    "    y_val_hat_engression=engressor_model.predict(X_val_tensor, target=\"mean\")\n",
    "    y_val_hat_engression = y_val_hat_engression.ge(0.5).float()  # Apply threshold to get binary predictions\n",
    "\n",
    "    accuracy_engression = accuracy_score(y_val_tensor.cpu().numpy(), y_val_hat_engression.cpu().numpy())  # Calculate accuracy\n",
    "\n",
    "    return accuracy_engression\n",
    "\n",
    "sampler_engression = optuna.samplers.TPESampler(seed=seed)\n",
    "study_engression = optuna.create_study(sampler=sampler_engression, direction='maximize')  # We want to maximize accuracy\n",
    "study_engression.optimize(engressor_NN, n_trials=N_TRIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 14] energy-loss: 0.3112,  E(|Y-Yhat|): 0.4466,  E(|Yhat-Yhat'|): 0.2709\n",
      "[Epoch 100 (33%), batch 14] energy-loss: 0.1418,  E(|Y-Yhat|): 0.3083,  E(|Yhat-Yhat'|): 0.3330\n",
      "[Epoch 200 (66%), batch 14] energy-loss: 0.1566,  E(|Y-Yhat|): 0.3433,  E(|Yhat-Yhat'|): 0.3733\n",
      "[Epoch 300 (99%), batch 14] energy-loss: 0.1661,  E(|Y-Yhat|): 0.3170,  E(|Yhat-Yhat'|): 0.3019\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1656,  E(|Y-Yhat|): 0.3354,  E(|Yhat-Yhat'|): 0.3396\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Classification metrics can't handle a mix of binary and continuous targets",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 13\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Convert the probabilities to class labels\u001b[39;00m\n\u001b[0;32m     12\u001b[0m y_val_hat_engression \u001b[38;5;241m=\u001b[39m y_test_hat_engression\u001b[38;5;241m.\u001b[39mge(\u001b[38;5;241m0.5\u001b[39m)\u001b[38;5;241m.\u001b[39mfloat()  \u001b[38;5;66;03m# Apply threshold to get binary predictions\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m accuracy_engression \u001b[38;5;241m=\u001b[39m \u001b[43maccuracy_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_hat_engression\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy logistic regression: \u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_logreg)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy boosted trees: \u001b[39m\u001b[38;5;124m\"\u001b[39m, accuracy_boosted)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:213\u001b[0m, in \u001b[0;36maccuracy_score\u001b[1;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Accuracy classification score.\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \n\u001b[0;32m    149\u001b[0m \u001b[38;5;124;03mIn multilabel classification, this function computes subset accuracy:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[38;5;124;03m0.5\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# Compute accuracy for each possible representation\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_type\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultilabel\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:94\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     91\u001b[0m     y_type \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y_type) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 94\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClassification metrics can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt handle a mix of \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m and \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m targets\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m     96\u001b[0m             type_true, type_pred\n\u001b[0;32m     97\u001b[0m         )\n\u001b[0;32m     98\u001b[0m     )\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# We can't have more than one value on y_type => The set is no more needed\u001b[39;00m\n\u001b[0;32m    101\u001b[0m y_type \u001b[38;5;241m=\u001b[39m y_type\u001b[38;5;241m.\u001b[39mpop()\n",
      "\u001b[1;31mValueError\u001b[0m: Classification metrics can't handle a mix of binary and continuous targets"
     ]
    }
   ],
   "source": [
    "# Engression model\n",
    "params=study_engression.best_params\n",
    "params['noise_dim']=params['hidden_dim']\n",
    "# Check if CUDA is available and if so, move the tensors and the model to the GPU\n",
    "if torch.cuda.is_available():\n",
    "    engressor_model=engression(X_train_tensor, y_train_tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=1000, sigmoid=True, device=\"cuda\")\n",
    "else: \n",
    "    engressor_model=engression(X_train_tensor, y_train_tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=1000, sigmoid=True)\n",
    "# Assuming the model outputs probabilities for the two classes\n",
    "y_test_hat_engression=engressor_model.predict(X_test_tensor, target=\"mean\")\n",
    "# Convert the probabilities to class labels\n",
    "y_test_hat_engression = y_test_hat_engression.ge(0.5).float()  # Apply threshold to get binary predictions\n",
    "accuracy_engression = accuracy_score(y_test_tensor.cpu().numpy(), y_test_hat_engression.cpu().numpy())  # Calculate accuracy\n",
    "\n",
    "print(\"Accuracy logistic regression: \", accuracy_logreg)\n",
    "print(\"Accuracy boosted trees: \", accuracy_boosted)\n",
    "print(\"Accuracy random forest: \", accuracy_rf)\n",
    "print(\"Accuracy engression: \", accuracy_engression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy logistic regression:  0.7870176488184266\n",
      "Accuracy boosted trees:  0.8232126832186659\n",
      "Accuracy random forest:  0.8196230930302124\n",
      "Accuracy engression:  0.8175291654202812\n"
     ]
    }
   ],
   "source": [
    "y_test_hat_engression = y_test_hat_engression.ge(0.5).float()  # Apply threshold to get binary predictions\n",
    "accuracy_engression = accuracy_score(y_test_tensor.cpu().numpy(), y_test_hat_engression.cpu().numpy())  # Calculate accuracy\n",
    "\n",
    "print(\"Accuracy logistic regression: \", accuracy_logreg)\n",
    "print(\"Accuracy boosted trees: \", accuracy_boosted)\n",
    "print(\"Accuracy random forest: \", accuracy_rf)\n",
    "print(\"Accuracy engression: \", accuracy_engression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567bb04cf28a481ca28ed218ac9aa828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14eaaa1742f14b359677bbad88b59104",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f40be4ced848fa8350d9774f03e520",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3c9785d68cd4a018835cc47e0c865a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "575afa40351f46bfa018f9a4db34093c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE GP:  tensor(71.9368)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import setuptools\n",
    "import openml\n",
    "from sklearn.linear_model import LinearRegression \n",
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from engression import engression, engression_bagged\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rtdl_revisiting_models import MLP, ResNet, FTTransformer\n",
    "from properscoring import crps_gaussian, crps_ensemble\n",
    "import random\n",
    "import gpytorch\n",
    "import tqdm.auto as tqdm\n",
    "import os\n",
    "from pygam import LinearGAM, s, f\n",
    "\n",
    "SUITE_ID = 336 # Regression on numerical features\n",
    "#SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "\n",
    "task_id=361072\n",
    "task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "dataset = task.get_dataset()\n",
    "\n",
    "X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "N_TRIALS=100\n",
    "N_SAMPLES=100\n",
    "seed=10\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "# calculate the mean and covariance matrix of the dataset\n",
    "mean = np.mean(X, axis=0)\n",
    "cov = np.cov(X.T)\n",
    "\n",
    "# calculate the Mahalanobis distance for each data point\n",
    "mahalanobis_dist = [mahalanobis(x, mean, np.linalg.inv(cov)) for x in X.values]\n",
    "\n",
    "mahalanobis_dist=pd.Series(mahalanobis_dist,index=X.index)\n",
    "far_index=mahalanobis_dist.index[np.where(mahalanobis_dist>=np.quantile(mahalanobis_dist,0.8))[0]]\n",
    "close_index=mahalanobis_dist.index[np.where(mahalanobis_dist<np.quantile(mahalanobis_dist,0.8))[0]]\n",
    "\n",
    "X_train = X.loc[close_index,:]\n",
    "X_test = X.loc[far_index,:]\n",
    "y_train = y.loc[close_index]\n",
    "y_test = y.loc[far_index]\n",
    "\n",
    "mean = np.mean(X_train, axis=0)\n",
    "cov = np.cov(X_train.T)\n",
    "\n",
    "# calculate the Mahalanobis distance for each data point\n",
    "mahalanobis_dist_ = [mahalanobis(x, mean, np.linalg.inv(cov)) for x in X_train.values]\n",
    "\n",
    "mahalanobis_dist_=pd.Series(mahalanobis_dist_,index=X_train.index)\n",
    "far_index_=mahalanobis_dist_.index[np.where(mahalanobis_dist_>=np.quantile(mahalanobis_dist_,0.8))[0]]\n",
    "close_index_=mahalanobis_dist_.index[np.where(mahalanobis_dist_<np.quantile(mahalanobis_dist_,0.8))[0]]\n",
    "\n",
    "X_train_ = X_train.loc[close_index_,:]\n",
    "X_val = X_train.loc[far_index_,:]\n",
    "y_train_ = y_train.loc[close_index_]\n",
    "y_val = y_train.loc[far_index_]\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train__tensor = torch.tensor(X_train_.values, dtype=torch.float32)\n",
    "y_train__tensor = torch.tensor(y_train_.values, dtype=torch.float32)\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Convert to use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    X_train__tensor = X_train__tensor.cuda()\n",
    "    y_train__tensor = y_train__tensor.cuda()\n",
    "    X_train_tensor = X_train_tensor.cuda()\n",
    "    y_train_tensor = y_train_tensor.cuda()\n",
    "    X_val_tensor = X_val_tensor.cuda()\n",
    "    y_val_tensor = y_val_tensor.cuda()\n",
    "    X_test_tensor = X_test_tensor.cuda()\n",
    "    y_test_tensor = y_test_tensor.cuda()\n",
    "\n",
    "# Create flattened versions of the data\n",
    "y_val_np = y_val.values.flatten()\n",
    "y_test_np = y_test.values.flatten()\n",
    "\n",
    "#### Gaussian process\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, kernel):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = kernel\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Define the learning params\n",
    "training_iterations = 3 #1000\n",
    "\n",
    "# Define the kernels\n",
    "kernels = [\n",
    "    gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=0.5, ard_num_dims=X_train_.shape[1])),\n",
    "    gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=X_train_.shape[1])),\n",
    "    gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=2.5, ard_num_dims=X_train_.shape[1])),\n",
    "    gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=X_train_.shape[1])),\n",
    "]\n",
    "\n",
    "best_RMSE = float('inf')\n",
    "best_kernel = None\n",
    "\n",
    "def train(model,X_train_tensor,y_train_tensor):\n",
    "    iterator = tqdm.tqdm(range(training_iterations), desc=\"Train\")\n",
    "\n",
    "    for _ in iterator:\n",
    "        # Zero backprop gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        output = model(X_train_tensor)\n",
    "        # Calc loss and backprop derivatives\n",
    "        loss = -mll(output, y_train_tensor)\n",
    "        loss.backward()\n",
    "        iterator.set_postfix(loss=loss.item())\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "for kernel in kernels:\n",
    "    # Initialize the Gaussian Process model and likelihood\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = ExactGPModel(X_train__tensor, y_train__tensor, likelihood, kernel)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    # Train the model\n",
    "    train(model,X_train__tensor,y_train__tensor)\n",
    "    \n",
    "    # Set the model in evaluation mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        y_pred = model(X_val_tensor)\n",
    "\n",
    "    # Calculate RMSE\n",
    "    RMSE = torch.sqrt(torch.mean(torch.square(y_val_tensor - y_pred.mean)))\n",
    "\n",
    "    # Update the best kernel if the current kernel has a lower RMSE\n",
    "    if RMSE < best_RMSE:\n",
    "        best_RMSE = RMSE\n",
    "        best_kernel = kernel\n",
    "\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = best_kernel\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Define the learning params\n",
    "training_iterations = 3 #1000\n",
    "\n",
    "# Initialize the Gaussian Process model and likelihood\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(X_train_tensor, y_train_tensor, likelihood)\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# Train the model\n",
    "train(model,X_train_tensor,y_train_tensor)\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Make predictions on the validation set\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    y_pred = model(X_test_tensor)\n",
    "\n",
    "# Calculate RMSE\n",
    "RMSE_GP = torch.sqrt(torch.mean(torch.square(y_test_tensor - y_pred.mean)))\n",
    "print(\"RMSE GP: \", RMSE_GP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bccb4854b8742c2a35ca06b15af612d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64c525a2172b4fa7b6a8bce70c154e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8505123548940b2b6a100dbb1f71853",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6453b872069340bb8ec487a0e26f4894",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c150ce58a7ff4e3fa57bccbb2efccd97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE GP:  tensor(71.9368)\n"
     ]
    }
   ],
   "source": [
    "#### Gaussian process\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, kernel):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = kernel\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Define the learning params\n",
    "training_iterations = 3 #1000\n",
    "\n",
    "# Define the kernels\n",
    "kernels = [\n",
    "    gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=0.5, ard_num_dims=X_train_.shape[1])),\n",
    "    gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=X_train_.shape[1])),\n",
    "    gpytorch.kernels.ScaleKernel(gpytorch.kernels.MaternKernel(nu=2.5, ard_num_dims=X_train_.shape[1])),\n",
    "    gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel(ard_num_dims=X_train_.shape[1])),\n",
    "]\n",
    "\n",
    "best_RMSE = float('inf')\n",
    "best_kernel = None\n",
    "\n",
    "def train(model,X_train_tensor,y_train_tensor):\n",
    "    iterator = tqdm.tqdm(range(training_iterations), desc=\"Train\")\n",
    "\n",
    "    for _ in iterator:\n",
    "        # Zero backprop gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Get output from model\n",
    "        output = model(X_train_tensor)\n",
    "        # Calc loss and backprop derivatives\n",
    "        loss = -mll(output, y_train_tensor)\n",
    "        loss.backward()\n",
    "        iterator.set_postfix(loss=loss.item())\n",
    "        optimizer.step()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "for kernel in kernels:\n",
    "    # Initialize the Gaussian Process model and likelihood\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = ExactGPModel(X_train__tensor, y_train__tensor, likelihood, kernel)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        model = model.cuda()\n",
    "\n",
    "    # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    # Train the model\n",
    "    train(model,X_train__tensor,y_train__tensor)\n",
    "    \n",
    "    # Set the model in evaluation mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        y_pred = model(X_val_tensor)\n",
    "\n",
    "    # Calculate RMSE\n",
    "    RMSE = torch.sqrt(torch.mean(torch.square(y_val_tensor - y_pred.mean)))\n",
    "\n",
    "    # Update the best kernel if the current kernel has a lower RMSE\n",
    "    if RMSE < best_RMSE:\n",
    "        best_RMSE = RMSE\n",
    "        best_kernel = kernel\n",
    "\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = best_kernel\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Define the learning params\n",
    "training_iterations = 3 #1000\n",
    "\n",
    "# Initialize the Gaussian Process model and likelihood\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = ExactGPModel(X_train_tensor, y_train_tensor, likelihood)\n",
    "\n",
    "# Find optimal model hyperparameters\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the adam optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# \"Loss\" for GPs - the marginal log likelihood\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# Train the model\n",
    "train(model,X_train_tensor,y_train_tensor)\n",
    "\n",
    "# Set the model in evaluation mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Make predictions on the validation set\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    y_pred = model(X_test_tensor)\n",
    "\n",
    "# Calculate RMSE\n",
    "RMSE_GP = torch.sqrt(torch.mean(torch.square(y_test_tensor - y_pred.mean)))\n",
    "print(\"RMSE GP: \", RMSE_GP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n"
     ]
    }
   ],
   "source": [
    "from umap import UMAP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import setuptools\n",
    "import openml\n",
    "from sklearn.linear_model import LinearRegression \n",
    "import lightgbm as lgbm\n",
    "import lightgbmlss\n",
    "import optuna\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from engression import engression, engression_bagged\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rtdl_revisiting_models import MLP, ResNet, FTTransformer\n",
    "from properscoring import crps_gaussian, crps_ensemble\n",
    "import random\n",
    "import gpytorch\n",
    "import tqdm.auto as tqdm\n",
    "from lightgbmlss.model import *\n",
    "from lightgbmlss.distributions.Gaussian import *\n",
    "from drf import drf\n",
    "import os\n",
    "from pygam import LinearGAM, s, f\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "#openml.config.apikey = 'FILL_IN_OPENML_API_KEY'  # set the OpenML Api Key\n",
    "#SUITE_ID = 336 # Regression on numerical features\n",
    "#SUITE_ID = 337 # Classification on numerical features\n",
    "SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "\n",
    "task_id=361093\n",
    "task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "dataset = task.get_dataset()\n",
    "\n",
    "X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        dataset_format=\"dataframe\", target=dataset.default_target_attribute)\n",
    "\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "N_TRIALS=100\n",
    "N_SAMPLES=100\n",
    "seed=10\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "# Apply UMAP decomposition\n",
    "umap = UMAP(n_components=2, random_state=42)\n",
    "X_umap = umap.fit_transform(X)\n",
    "\n",
    "# calculate the Euclidean distance matrix\n",
    "euclidean_dist_matrix = euclidean_distances(X_umap)\n",
    "\n",
    "# calculate the Euclidean distance for each data point\n",
    "euclidean_dist = np.mean(euclidean_dist_matrix, axis=1)\n",
    "\n",
    "euclidean_dist = pd.Series(euclidean_dist, index=X.index)\n",
    "far_index = euclidean_dist.index[np.where(euclidean_dist >= np.quantile(euclidean_dist, 0.8))[0]]\n",
    "close_index = euclidean_dist.index[np.where(euclidean_dist < np.quantile(euclidean_dist, 0.8))[0]]\n",
    "\n",
    "X_train = X.loc[close_index,:]\n",
    "\n",
    "# Apply UMAP decomposition on the training set\n",
    "X_umap_train = umap.fit_transform(X_train)\n",
    "\n",
    "# calculate the Euclidean distance matrix for the training set\n",
    "euclidean_dist_matrix_train = euclidean_distances(X_umap_train)\n",
    "\n",
    "# calculate the Euclidean distance for each data point in the training set\n",
    "euclidean_dist_train = np.mean(euclidean_dist_matrix_train, axis=1)\n",
    "\n",
    "euclidean_dist_train = pd.Series(euclidean_dist_train, index=X_train.index)\n",
    "far_index_train = euclidean_dist_train.index[np.where(euclidean_dist_train >= np.quantile(euclidean_dist_train, 0.8))[0]]\n",
    "close_index_train = euclidean_dist_train.index[np.where(euclidean_dist_train < np.quantile(euclidean_dist_train, 0.8))[0]]\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "# Modify X_train_, X_val, X_train, and X_test to have dummy variables\n",
    "X = pd.get_dummies(X.astype(str), drop_first=True)\n",
    "\n",
    "X_train = X.loc[close_index,:]\n",
    "X_test = X.loc[far_index,:]\n",
    "y_train = y.loc[close_index]\n",
    "y_test = y.loc[far_index]\n",
    "\n",
    "X_train_ = X_train.loc[close_index_train,:]\n",
    "X_val = X_train.loc[far_index_train,:]\n",
    "y_train_ = y_train.loc[close_index_train]\n",
    "y_val = y_train.loc[far_index_train]\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train__tensor = torch.tensor(X_train_.values, dtype=torch.float32)\n",
    "y_train__tensor = torch.tensor(y_train_.values, dtype=torch.float32)\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Convert to use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    X_train__tensor = X_train__tensor.cuda()\n",
    "    y_train__tensor = y_train__tensor.cuda()\n",
    "    X_train_tensor = X_train_tensor.cuda()\n",
    "    y_train_tensor = y_train_tensor.cuda()\n",
    "    X_val_tensor = X_val_tensor.cuda()\n",
    "    y_val_tensor = y_val_tensor.cuda()\n",
    "    X_test_tensor = X_test_tensor.cuda()\n",
    "    y_test_tensor = y_test_tensor.cuda()\n",
    "\n",
    "# Create flattened versions of the data\n",
    "y_val_np = y_val.values.flatten()\n",
    "y_test_np = y_test.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_TRIALS=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-02 18:12:10,537] A new study created in memory with name: no-name-a791fa4b-af88-403d-afc4-ceadc6387b1d\n",
      "[I 2024-02-02 18:12:11,182] Trial 0 finished with value: 0.28205499224360375 and parameters: {'learning_rate': 0.12071779104534666, 'n_estimators': 108, 'reg_lambda': 0.005044685709888605, 'max_depth': 23, 'min_child_samples': 55}. Best is trial 0 with value: 0.28205499224360375.\n",
      "[I 2024-02-02 18:12:11,229] Trial 1 finished with value: 0.29362264402953486 and parameters: {'learning_rate': 0.004043145805966843, 'n_estimators': 179, 'reg_lambda': 0.0699481785242808, 'max_depth': 6, 'min_child_samples': 18}. Best is trial 0 with value: 0.28205499224360375.\n",
      "[I 2024-02-02 18:12:11,360] Trial 2 finished with value: 0.2875465361619137 and parameters: {'learning_rate': 0.07075637776590661, 'n_estimators': 482, 'reg_lambda': 1.08526150100961e-08, 'max_depth': 16, 'min_child_samples': 83}. Best is trial 0 with value: 0.28205499224360375.\n",
      "[I 2024-02-02 18:12:11,473] Trial 3 finished with value: 0.28218167204809275 and parameters: {'learning_rate': 0.044997613517186334, 'n_estimators': 389, 'reg_lambda': 4.235304245072407e-06, 'max_depth': 28, 'min_child_samples': 75}. Best is trial 0 with value: 0.28205499224360375.\n",
      "[I 2024-02-02 18:12:11,566] Trial 4 finished with value: 0.28794836859907463 and parameters: {'learning_rate': 0.029128020748551788, 'n_estimators': 157, 'reg_lambda': 2.2912202578440842e-05, 'max_depth': 21, 'min_child_samples': 50}. Best is trial 0 with value: 0.28205499224360375.\n",
      "[I 2024-02-02 18:12:11,569] A new study created in memory with name: no-name-64afe246-b648-4551-9546-7c66be39b71f\n",
      "[I 2024-02-02 18:12:12,116] Trial 0 finished with value: 0.3067020941042034 and parameters: {'n_estimators': 409, 'max_depth': 1, 'max_features': 20, 'min_samples_leaf': 78}. Best is trial 0 with value: 0.3067020941042034.\n",
      "[I 2024-02-02 18:12:12,547] Trial 1 finished with value: 0.3127936987304492 and parameters: {'n_estimators': 299, 'max_depth': 7, 'max_features': 6, 'min_samples_leaf': 79}. Best is trial 0 with value: 0.3067020941042034.\n",
      "[I 2024-02-02 18:12:12,809] Trial 2 finished with value: 0.31370842026251894 and parameters: {'n_estimators': 167, 'max_depth': 3, 'max_features': 21, 'min_samples_leaf': 96}. Best is trial 0 with value: 0.3067020941042034.\n",
      "[I 2024-02-02 18:12:13,038] Trial 3 finished with value: 0.29625370373448096 and parameters: {'n_estimators': 101, 'max_depth': 16, 'max_features': 25, 'min_samples_leaf': 65}. Best is trial 3 with value: 0.29625370373448096.\n",
      "[I 2024-02-02 18:12:14,144] Trial 4 finished with value: 0.30295498826718303 and parameters: {'n_estimators': 389, 'max_depth': 9, 'max_features': 28, 'min_samples_leaf': 75}. Best is trial 3 with value: 0.29625370373448096.\n",
      "[I 2024-02-02 18:12:14,146] A new study created in memory with name: no-name-642526f5-1d7d-4caf-801e-3a7181d5584c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Residual blocks (skip-connections) are typically recommended for more than 2 layers; turn it on by setting resblock=True.\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 3] energy-loss: 0.4471,  E(|Y-Yhat|): 0.9615,  E(|Yhat-Yhat'|): 1.0288\n",
      "[Epoch 100 (84%), batch 3] energy-loss: 0.0536,  E(|Y-Yhat|): 0.1128,  E(|Yhat-Yhat'|): 0.1184\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1241,  E(|Y-Yhat|): 0.1767,  E(|Yhat-Yhat'|): 0.1053\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-02 18:13:10,130] Trial 0 finished with value: 873.9304809570312 and parameters: {'learning_rate': 0.0034885205571560775, 'num_epoches': 118, 'num_layer': 4, 'hidden_dim': 400}. Best is trial 0 with value: 873.9304809570312.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 3] energy-loss: 0.3969,  E(|Y-Yhat|): 0.6212,  E(|Yhat-Yhat'|): 0.4485\n",
      "[Epoch 100 (33%), batch 3] energy-loss: 0.0832,  E(|Y-Yhat|): 0.1829,  E(|Yhat-Yhat'|): 0.1994\n",
      "[Epoch 200 (66%), batch 3] energy-loss: 0.0406,  E(|Y-Yhat|): 0.1058,  E(|Yhat-Yhat'|): 0.1303\n",
      "[Epoch 300 (99%), batch 3] energy-loss: 0.0330,  E(|Y-Yhat|): 0.0883,  E(|Yhat-Yhat'|): 0.1105\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0214,  E(|Y-Yhat|): 0.0527,  E(|Yhat-Yhat'|): 0.0625\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-02 18:14:05,414] Trial 1 finished with value: 421.4085388183594 and parameters: {'learning_rate': 0.000993148119483195, 'num_epoches': 302, 'num_layer': 2, 'hidden_dim': 404}. Best is trial 1 with value: 421.4085388183594.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Residual blocks (skip-connections) are typically recommended for more than 2 layers; turn it on by setting resblock=True.\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 3] energy-loss: 0.4837,  E(|Y-Yhat|): 0.6800,  E(|Yhat-Yhat'|): 0.3925\n",
      "[Epoch 100 (55%), batch 3] energy-loss: 0.0713,  E(|Y-Yhat|): 0.1832,  E(|Yhat-Yhat'|): 0.2239\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1023,  E(|Y-Yhat|): 0.1655,  E(|Yhat-Yhat'|): 0.1264\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-02 18:15:44,552] Trial 2 finished with value: 2361.20751953125 and parameters: {'learning_rate': 0.00021788216053884017, 'num_epoches': 179, 'num_layer': 4, 'hidden_dim': 482}. Best is trial 1 with value: 421.4085388183594.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Residual blocks (skip-connections) are typically recommended for more than 2 layers; turn it on by setting resblock=True.\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 3] energy-loss: 0.4192,  E(|Y-Yhat|): 0.6044,  E(|Yhat-Yhat'|): 0.3705\n",
      "[Epoch 100 (18%), batch 3] energy-loss: 0.0742,  E(|Y-Yhat|): 0.2349,  E(|Yhat-Yhat'|): 0.3214\n",
      "[Epoch 200 (35%), batch 3] energy-loss: 0.0583,  E(|Y-Yhat|): 0.1741,  E(|Yhat-Yhat'|): 0.2316\n",
      "[Epoch 300 (53%), batch 3] energy-loss: 0.0432,  E(|Y-Yhat|): 0.1381,  E(|Yhat-Yhat'|): 0.1899\n",
      "[Epoch 400 (71%), batch 3] energy-loss: 0.0400,  E(|Y-Yhat|): 0.1147,  E(|Yhat-Yhat'|): 0.1493\n",
      "[Epoch 500 (89%), batch 3] energy-loss: 0.0421,  E(|Y-Yhat|): 0.1125,  E(|Yhat-Yhat'|): 0.1409\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1909,  E(|Y-Yhat|): 0.2455,  E(|Yhat-Yhat'|): 0.1091\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-02 18:20:46,277] Trial 3 finished with value: 806.5403442382812 and parameters: {'learning_rate': 0.00010183487453386067, 'num_epoches': 561, 'num_layer': 5, 'hidden_dim': 345}. Best is trial 1 with value: 421.4085388183594.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Residual blocks (skip-connections) are typically recommended for more than 2 layers; turn it on by setting resblock=True.\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 3] energy-loss: 0.4663,  E(|Y-Yhat|): 0.8335,  E(|Yhat-Yhat'|): 0.7344\n",
      "[Epoch 100 (27%), batch 3] energy-loss: 0.0273,  E(|Y-Yhat|): 0.0838,  E(|Yhat-Yhat'|): 0.1129\n",
      "[Epoch 200 (55%), batch 3] energy-loss: 0.0690,  E(|Y-Yhat|): 0.1447,  E(|Yhat-Yhat'|): 0.1516\n",
      "[Epoch 300 (83%), batch 3] energy-loss: 0.0280,  E(|Y-Yhat|): 0.0770,  E(|Yhat-Yhat'|): 0.0980\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.1706,  E(|Y-Yhat|): 0.2243,  E(|Yhat-Yhat'|): 0.1074\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-02 18:24:39,617] Trial 4 finished with value: 471.7718200683594 and parameters: {'learning_rate': 0.0027765828373405767, 'num_epoches': 362, 'num_layer': 5, 'hidden_dim': 386}. Best is trial 1 with value: 421.4085388183594.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 4] energy-loss: 0.3629,  E(|Y-Yhat|): 0.6052,  E(|Yhat-Yhat'|): 0.4847\n",
      "[Epoch 100 (33%), batch 4] energy-loss: 0.0492,  E(|Y-Yhat|): 0.1462,  E(|Yhat-Yhat'|): 0.1940\n",
      "[Epoch 200 (66%), batch 4] energy-loss: 0.0794,  E(|Y-Yhat|): 0.1462,  E(|Yhat-Yhat'|): 0.1336\n",
      "[Epoch 300 (99%), batch 4] energy-loss: 0.0403,  E(|Y-Yhat|): 0.0967,  E(|Yhat-Yhat'|): 0.1130\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.0213,  E(|Y-Yhat|): 0.0510,  E(|Yhat-Yhat'|): 0.0595\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "RMSE linear regression:  0.1064209928338373\n",
      "RMSE boosted trees 0.5306943387726853\n",
      "RMSE random forest 0.5718517712286753\n",
      "RMSE engression tensor(104.6443)\n"
     ]
    }
   ],
   "source": [
    "def boosted(trial):\n",
    "\n",
    "    params = {'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.5, log=True),\n",
    "              'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "              'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "              'max_depth': trial.suggest_int('max_depth', 1, 30),\n",
    "              'min_child_samples': trial.suggest_int('min_child_samples', 10, 100)}\n",
    "    \n",
    "    boosted_tree_model=lgbm.LGBMRegressor(**params)\n",
    "    boosted_tree_model.fit(X_train_, y_train_)\n",
    "    y_val_hat_boost=boosted_tree_model.predict(X_val)\n",
    "    RMSE_boost=np.sqrt(np.mean((y_val-y_val_hat_boost)**2))\n",
    "\n",
    "    return RMSE_boost\n",
    "\n",
    "sampler_boost = optuna.samplers.TPESampler(seed=seed)\n",
    "study_boost = optuna.create_study(sampler=sampler_boost, direction='minimize')\n",
    "study_boost.optimize(boosted, n_trials=N_TRIALS)\n",
    "boosted_model=lgbm.LGBMRegressor(**study_boost.best_params)\n",
    "\n",
    "def rf(trial):\n",
    "\n",
    "    params = {'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "              'max_depth': trial.suggest_int('max_depth', 1, 30),\n",
    "              'max_features': trial.suggest_int('max_features', 1, 30),\n",
    "              'min_samples_leaf': trial.suggest_int('min_samples_leaf', 10, 100)}\n",
    "    \n",
    "    rf_model=RandomForestRegressor(**params)\n",
    "    rf_model.fit(X_train_, y_train_)\n",
    "    y_val_hat_rf=rf_model.predict(X_val)\n",
    "    RMSE_rf=np.sqrt(np.mean((y_val-y_val_hat_rf)**2))\n",
    "\n",
    "    return RMSE_rf\n",
    "\n",
    "sampler_rf = optuna.samplers.TPESampler(seed=seed)\n",
    "study_rf = optuna.create_study(sampler=sampler_rf, direction='minimize')\n",
    "study_rf.optimize(rf, n_trials=N_TRIALS)\n",
    "rf_model=RandomForestRegressor(**study_rf.best_params)\n",
    "\n",
    "\n",
    "def engressor_NN(trial):\n",
    "\n",
    "    params = {'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.01, log=True),\n",
    "              'num_epoches': trial.suggest_int('num_epoches', 100, 1000),\n",
    "              'num_layer': trial.suggest_int('num_layer', 2, 5),\n",
    "              'hidden_dim': trial.suggest_int('hidden_dim', 100, 500),}\n",
    "    params['noise_dim']=params['hidden_dim']\n",
    "\n",
    "    # Check if CUDA is available and if so, move the tensors and the model to the GPU\n",
    "    if torch.cuda.is_available():\n",
    "        engressor_model=engression(X_train__tensor, y_train__tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=1000, device=\"cuda\")\n",
    "    else: \n",
    "        engressor_model=engression(X_train__tensor, y_train__tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=1000)\n",
    "    \n",
    "    # Generate a sample from the engression model for each data point\n",
    "    y_val_hat_engression=engressor_model.predict(X_val_tensor, target=\"mean\")\n",
    "    RMSE_engression=torch.sqrt(torch.mean(torch.square(y_val_tensor.reshape(-1,1) - y_val_hat_engression)))\n",
    "\n",
    "    return RMSE_engression\n",
    "\n",
    "sampler_engression = optuna.samplers.TPESampler(seed=seed)\n",
    "study_engression = optuna.create_study(sampler=sampler_engression, direction='minimize')\n",
    "study_engression.optimize(engressor_NN, n_trials=N_TRIALS)\n",
    "\n",
    "\n",
    "boosted_model.fit(X_train, y_train)\n",
    "y_test_hat_boosted=boosted_model.predict(X_test)\n",
    "RMSE_boosted=np.sqrt(np.mean((y_test-y_test_hat_boosted)**2))\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_test_hat_rf=rf_model.predict(X_test)\n",
    "RMSE_rf=np.sqrt(np.mean((y_test-y_test_hat_rf)**2))\n",
    "\n",
    "lin_reg=LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "y_test_hat_linreg=lin_reg.predict(X_test)\n",
    "RMSE_linreg=np.sqrt(np.mean((y_test-y_test_hat_linreg)**2))\n",
    "\n",
    "params=study_engression.best_params\n",
    "params['noise_dim']=params['hidden_dim']\n",
    "# Check if CUDA is available and if so, move the tensors and the model to the GPU\n",
    "if torch.cuda.is_available():\n",
    "    engressor_model=engression(X_train_tensor, y_train_tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=1000, device=\"cuda\")\n",
    "else: \n",
    "    engressor_model=engression(X_train_tensor, y_train_tensor.reshape(-1,1), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=1000)\n",
    "y_test_hat_engression=engressor_model.predict(X_test_tensor, target=\"mean\")\n",
    "RMSE_engression=torch.sqrt(torch.mean(torch.square(y_test_tensor.reshape(-1,1) - y_test_hat_engression)))\n",
    "\n",
    "\n",
    "print(\"RMSE linear regression: \",RMSE_linreg)\n",
    "print(\"RMSE boosted trees\", RMSE_boosted)\n",
    "print(\"RMSE random forest\", RMSE_rf)\n",
    "print(\"RMSE engression\", RMSE_engression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### GAM model\n",
    "def gam_model(trial):\n",
    "\n",
    "    # Define the hyperparameters to optimize\n",
    "    params = {'n_splines': trial.suggest_int('n_splines', 5, 20),\n",
    "              'lam': trial.suggest_loguniform('lam', 1e-3, 1)}\n",
    "\n",
    "    # Create and train the model\n",
    "    gam = LinearGAM(s(0, n_splines=params['n_splines'], lam=params['lam'])).fit(X_train_, y_train_)\n",
    "\n",
    "    # Predict on the validation set and calculate the RMSE\n",
    "    y_val_hat_gam = gam.predict(X_val)\n",
    "    RMSE_gam = np.sqrt(np.mean((y_val - y_val_hat_gam) ** 2))\n",
    "\n",
    "    return RMSE_gam\n",
    "\n",
    "# Create the sampler and study\n",
    "sampler_gam = optuna.samplers.TPESampler(seed=seed)\n",
    "study_gam = optuna.create_study(sampler=sampler_gam, direction='minimize')\n",
    "\n",
    "# Optimize the model\n",
    "study_gam.optimize(gam_model, n_trials=N_TRIALS)\n",
    "\n",
    "# Create the final model with the best parameters\n",
    "best_params = study_gam.best_params\n",
    "final_gam_model = LinearGAM(s(0, n_splines=best_params['n_splines'], lam=best_params['lam']))\n",
    "\n",
    "# Fit the model\n",
    "final_gam_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_hat_gam = final_gam_model.predict(X_test)\n",
    "# Calculate the RMSE\n",
    "RMSE_gam = np.sqrt(np.mean((y_test - y_test_hat_gam) ** 2))\n",
    "print(\"RMSE GAM: \", RMSE_gam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3241, 49)\n",
      "(2592, 49)\n",
      "(649, 49)\n",
      "(811, 49)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(X_train_.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4052, 49)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-02 12:44:44,634] A new study created in memory with name: no-name-42393d17-3449-4f52-aac7-534dc03deab4\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:44,674] Trial 0 finished with value: 0.17618370891210408 and parameters: {'n_splines': 17, 'lam': 0.001154132971137168}. Best is trial 0 with value: 0.17618370891210408.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:44,707] Trial 1 finished with value: 0.1761837089120805 and parameters: {'n_splines': 15, 'lam': 0.17636469336159113}. Best is trial 1 with value: 0.1761837089120805.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:44,744] Trial 2 finished with value: 0.17618370891202992 and parameters: {'n_splines': 12, 'lam': 0.00472487079152679}. Best is trial 2 with value: 0.17618370891202992.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:44,771] Trial 3 finished with value: 0.17618370891193946 and parameters: {'n_splines': 8, 'lam': 0.19124590142517375}. Best is trial 3 with value: 0.17618370891193946.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:44,799] Trial 4 finished with value: 0.17618370891189827 and parameters: {'n_splines': 7, 'lam': 0.0018408544111075849}. Best is trial 4 with value: 0.17618370891189827.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:44,828] Trial 5 finished with value: 0.17618370891208146 and parameters: {'n_splines': 15, 'lam': 0.7247363402746422}. Best is trial 4 with value: 0.17618370891189827.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:44,852] Trial 6 finished with value: 0.17618370891182436 and parameters: {'n_splines': 5, 'lam': 0.03440145332328687}. Best is trial 6 with value: 0.17618370891182436.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:44,885] Trial 7 finished with value: 0.17618370891212357 and parameters: {'n_splines': 18, 'lam': 0.06879837817714736}. Best is trial 6 with value: 0.17618370891182436.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:44,922] Trial 8 finished with value: 0.17618370891209356 and parameters: {'n_splines': 16, 'lam': 0.007509797119626296}. Best is trial 6 with value: 0.17618370891182436.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:44,956] Trial 9 finished with value: 0.17618370891213073 and parameters: {'n_splines': 19, 'lam': 0.13922824544531598}. Best is trial 6 with value: 0.17618370891182436.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:44,998] Trial 10 finished with value: 0.176183708911821 and parameters: {'n_splines': 5, 'lam': 0.018290037788472244}. Best is trial 10 with value: 0.176183708911821.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:45,040] Trial 11 finished with value: 0.17618370891181845 and parameters: {'n_splines': 5, 'lam': 0.018520574836963306}. Best is trial 11 with value: 0.17618370891181845.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:45,084] Trial 12 finished with value: 0.17618370891198956 and parameters: {'n_splines': 10, 'lam': 0.012903344768235856}. Best is trial 11 with value: 0.17618370891181845.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:45,122] Trial 13 finished with value: 0.1761837089118253 and parameters: {'n_splines': 5, 'lam': 0.018164194488257835}. Best is trial 11 with value: 0.17618370891181845.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:45,171] Trial 14 finished with value: 0.1761837089119326 and parameters: {'n_splines': 8, 'lam': 0.003906962915591378}. Best is trial 11 with value: 0.17618370891181845.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:45,224] Trial 15 finished with value: 0.17618370891201113 and parameters: {'n_splines': 11, 'lam': 0.03690588735010913}. Best is trial 11 with value: 0.17618370891181845.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:45,269] Trial 16 finished with value: 0.17618370891185925 and parameters: {'n_splines': 6, 'lam': 0.01476740296725994}. Best is trial 11 with value: 0.17618370891181845.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:45,319] Trial 17 finished with value: 0.17618370891199486 and parameters: {'n_splines': 10, 'lam': 0.06765530414674206}. Best is trial 11 with value: 0.17618370891181845.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:45,372] Trial 18 finished with value: 0.17618370891196156 and parameters: {'n_splines': 9, 'lam': 0.006814738971528428}. Best is trial 11 with value: 0.17618370891181845.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:45,428] Trial 19 finished with value: 0.17618370891204768 and parameters: {'n_splines': 13, 'lam': 0.6674532614549996}. Best is trial 11 with value: 0.17618370891181845.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:45,471] Trial 20 finished with value: 0.17618370891189827 and parameters: {'n_splines': 7, 'lam': 0.0025844453955776565}. Best is trial 11 with value: 0.17618370891181845.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:45,518] Trial 21 finished with value: 0.17618370891182414 and parameters: {'n_splines': 5, 'lam': 0.026871126228970196}. Best is trial 11 with value: 0.17618370891181845.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:45,559] Trial 22 finished with value: 0.17618370891182514 and parameters: {'n_splines': 5, 'lam': 0.01947702359544753}. Best is trial 11 with value: 0.17618370891181845.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:45,606] Trial 23 finished with value: 0.1761837089118977 and parameters: {'n_splines': 7, 'lam': 0.05487099531351018}. Best is trial 11 with value: 0.17618370891181845.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:45,659] Trial 24 finished with value: 0.17618370891185856 and parameters: {'n_splines': 6, 'lam': 0.010919018963440216}. Best is trial 11 with value: 0.17618370891181845.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:45,703] Trial 25 finished with value: 0.17618370891181617 and parameters: {'n_splines': 5, 'lam': 0.026979120242025565}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:45,751] Trial 26 finished with value: 0.17618370891193594 and parameters: {'n_splines': 8, 'lam': 0.11906632370440907}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:45,803] Trial 27 finished with value: 0.17618370891186538 and parameters: {'n_splines': 6, 'lam': 0.009031897167566262}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:45,850] Trial 28 finished with value: 0.17618370891197255 and parameters: {'n_splines': 9, 'lam': 0.41970839116629804}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:45,900] Trial 29 finished with value: 0.17618370891205917 and parameters: {'n_splines': 13, 'lam': 0.02392003175338789}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:45,943] Trial 30 finished with value: 0.17618370891185836 and parameters: {'n_splines': 6, 'lam': 0.04842511876395219}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:45,986] Trial 31 finished with value: 0.17618370891182514 and parameters: {'n_splines': 5, 'lam': 0.030979156652662904}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:46,021] Trial 32 finished with value: 0.1761837089118275 and parameters: {'n_splines': 5, 'lam': 0.02548233762111743}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:46,066] Trial 33 finished with value: 0.1761837089118979 and parameters: {'n_splines': 7, 'lam': 0.005268159618510412}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:46,106] Trial 34 finished with value: 0.17618370891186289 and parameters: {'n_splines': 6, 'lam': 0.11040293744521466}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:46,147] Trial 35 finished with value: 0.1761837089119328 and parameters: {'n_splines': 8, 'lam': 0.014951222653935456}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:46,189] Trial 36 finished with value: 0.17618370891181873 and parameters: {'n_splines': 5, 'lam': 0.08589510603311425}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:46,228] Trial 37 finished with value: 0.1761837089119003 and parameters: {'n_splines': 7, 'lam': 0.30286211921792194}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:46,272] Trial 38 finished with value: 0.17618370891196228 and parameters: {'n_splines': 9, 'lam': 0.21288094414159986}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:46,317] Trial 39 finished with value: 0.17618370891185925 and parameters: {'n_splines': 6, 'lam': 0.09073090194907871}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:46,364] Trial 40 finished with value: 0.17618370891206905 and parameters: {'n_splines': 14, 'lam': 0.0013127933954859418}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:46,403] Trial 41 finished with value: 0.17618370891182433 and parameters: {'n_splines': 5, 'lam': 0.037593398125390705}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:46,481] Trial 42 finished with value: 0.17618370891182464 and parameters: {'n_splines': 5, 'lam': 0.0442077753838799}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:46,552] Trial 43 finished with value: 0.17618370891213758 and parameters: {'n_splines': 20, 'lam': 0.0236514506922463}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:46,603] Trial 44 finished with value: 0.17618370891189963 and parameters: {'n_splines': 7, 'lam': 0.0781258932240431}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:46,648] Trial 45 finished with value: 0.1761837089118275 and parameters: {'n_splines': 5, 'lam': 0.01901692469779726}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:46,693] Trial 46 finished with value: 0.1761837089121125 and parameters: {'n_splines': 17, 'lam': 0.011415938504740888}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:46,734] Trial 47 finished with value: 0.17618370891185864 and parameters: {'n_splines': 6, 'lam': 0.031551926740467834}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:46,771] Trial 48 finished with value: 0.1761837089118968 and parameters: {'n_splines': 7, 'lam': 0.0077850180129158805}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:46,819] Trial 49 finished with value: 0.1761837089119335 and parameters: {'n_splines': 8, 'lam': 0.15378894735242796}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:46,868] Trial 50 finished with value: 0.176183708912016 and parameters: {'n_splines': 11, 'lam': 0.05762067214981892}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:46,906] Trial 51 finished with value: 0.176183708911824 and parameters: {'n_splines': 5, 'lam': 0.03976961037647992}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:46,946] Trial 52 finished with value: 0.17618370891182844 and parameters: {'n_splines': 5, 'lam': 0.026271941838060982}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:46,991] Trial 53 finished with value: 0.17618370891186066 and parameters: {'n_splines': 6, 'lam': 0.040176572361634216}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:47,039] Trial 54 finished with value: 0.17618370891185728 and parameters: {'n_splines': 6, 'lam': 0.01589541067045212}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:47,093] Trial 55 finished with value: 0.1761837089118176 and parameters: {'n_splines': 5, 'lam': 0.06495547143301672}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:47,138] Trial 56 finished with value: 0.1761837089118207 and parameters: {'n_splines': 5, 'lam': 0.08624606042022619}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:47,182] Trial 57 finished with value: 0.17618370891190088 and parameters: {'n_splines': 7, 'lam': 0.21805211860385149}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:47,230] Trial 58 finished with value: 0.17618370891186136 and parameters: {'n_splines': 6, 'lam': 0.07893171542046538}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:47,279] Trial 59 finished with value: 0.17618370891181773 and parameters: {'n_splines': 5, 'lam': 0.12402431564518586}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:47,335] Trial 60 finished with value: 0.17618370891193852 and parameters: {'n_splines': 8, 'lam': 0.1033204719707852}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:47,386] Trial 61 finished with value: 0.17618370891182378 and parameters: {'n_splines': 5, 'lam': 0.0625164165549158}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:47,432] Trial 62 finished with value: 0.1761837089118576 and parameters: {'n_splines': 6, 'lam': 0.12978251573096433}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:47,476] Trial 63 finished with value: 0.17618370891182333 and parameters: {'n_splines': 5, 'lam': 0.17278847323897628}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:47,519] Trial 64 finished with value: 0.17618370891185997 and parameters: {'n_splines': 6, 'lam': 0.053512289062962695}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:47,565] Trial 65 finished with value: 0.1761837089118215 and parameters: {'n_splines': 5, 'lam': 0.07820510959326288}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:47,615] Trial 66 finished with value: 0.1761837089118994 and parameters: {'n_splines': 7, 'lam': 0.27402362774211614}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:47,655] Trial 67 finished with value: 0.17618370891182802 and parameters: {'n_splines': 5, 'lam': 0.09849127590503544}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:47,702] Trial 68 finished with value: 0.17618370891186277 and parameters: {'n_splines': 6, 'lam': 0.020007962014266457}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:47,761] Trial 69 finished with value: 0.17618370891190063 and parameters: {'n_splines': 7, 'lam': 0.46527004236499786}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:47,810] Trial 70 finished with value: 0.17618370891182186 and parameters: {'n_splines': 5, 'lam': 0.06684183097936529}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:47,850] Trial 71 finished with value: 0.1761837089118235 and parameters: {'n_splines': 5, 'lam': 0.07864922533934544}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:47,896] Trial 72 finished with value: 0.1761837089118629 and parameters: {'n_splines': 6, 'lam': 0.04833701022982638}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:47,936] Trial 73 finished with value: 0.17618370891182317 and parameters: {'n_splines': 5, 'lam': 0.14420501373814576}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:47,989] Trial 74 finished with value: 0.1761837089120865 and parameters: {'n_splines': 15, 'lam': 0.11147291770756926}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:48,033] Trial 75 finished with value: 0.1761837089118612 and parameters: {'n_splines': 6, 'lam': 0.0314218931408018}. Best is trial 25 with value: 0.17618370891181617.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:48,077] Trial 76 finished with value: 0.17618370891181553 and parameters: {'n_splines': 5, 'lam': 0.012367302793512804}. Best is trial 76 with value: 0.17618370891181553.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:48,120] Trial 77 finished with value: 0.17618370891190013 and parameters: {'n_splines': 7, 'lam': 0.01025588181295111}. Best is trial 76 with value: 0.17618370891181553.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:48,156] Trial 78 finished with value: 0.1761837089118221 and parameters: {'n_splines': 5, 'lam': 0.006053640006278874}. Best is trial 76 with value: 0.17618370891181553.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:48,201] Trial 79 finished with value: 0.1761837089118614 and parameters: {'n_splines': 6, 'lam': 0.003925965238070822}. Best is trial 76 with value: 0.17618370891181553.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:48,249] Trial 80 finished with value: 0.17618370891182555 and parameters: {'n_splines': 5, 'lam': 0.013192536217145842}. Best is trial 76 with value: 0.17618370891181553.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:48,323] Trial 81 finished with value: 0.17618370891182 and parameters: {'n_splines': 5, 'lam': 0.01683025868871902}. Best is trial 76 with value: 0.17618370891181553.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:48,368] Trial 82 finished with value: 0.17618370891182045 and parameters: {'n_splines': 5, 'lam': 0.016557394806031772}. Best is trial 76 with value: 0.17618370891181553.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:48,405] Trial 83 finished with value: 0.17618370891182086 and parameters: {'n_splines': 5, 'lam': 0.022146025044301534}. Best is trial 76 with value: 0.17618370891181553.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:48,450] Trial 84 finished with value: 0.17618370891186277 and parameters: {'n_splines': 6, 'lam': 0.016576606429030343}. Best is trial 76 with value: 0.17618370891181553.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:48,492] Trial 85 finished with value: 0.17618370891186372 and parameters: {'n_splines': 6, 'lam': 0.012808050243514404}. Best is trial 76 with value: 0.17618370891181553.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:48,533] Trial 86 finished with value: 0.17618370891182716 and parameters: {'n_splines': 5, 'lam': 0.009253065521126647}. Best is trial 76 with value: 0.17618370891181553.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:48,570] Trial 87 finished with value: 0.1761837089118997 and parameters: {'n_splines': 7, 'lam': 0.007958295540839353}. Best is trial 76 with value: 0.17618370891181553.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:48,615] Trial 88 finished with value: 0.1761837089120347 and parameters: {'n_splines': 12, 'lam': 0.02822358921583461}. Best is trial 76 with value: 0.17618370891181553.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:48,652] Trial 89 finished with value: 0.17618370891182275 and parameters: {'n_splines': 5, 'lam': 0.013620040413340178}. Best is trial 76 with value: 0.17618370891181553.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:48,697] Trial 90 finished with value: 0.17618370891199103 and parameters: {'n_splines': 10, 'lam': 0.02045024072107329}. Best is trial 76 with value: 0.17618370891181553.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:48,735] Trial 91 finished with value: 0.1761837089118179 and parameters: {'n_splines': 5, 'lam': 0.017011115624767596}. Best is trial 76 with value: 0.17618370891181553.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:48,782] Trial 92 finished with value: 0.17618370891186036 and parameters: {'n_splines': 6, 'lam': 0.016141982878132174}. Best is trial 76 with value: 0.17618370891181553.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:48,823] Trial 93 finished with value: 0.176183708911821 and parameters: {'n_splines': 5, 'lam': 0.011909651326433133}. Best is trial 76 with value: 0.17618370891181553.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:48,868] Trial 94 finished with value: 0.17618370891186064 and parameters: {'n_splines': 6, 'lam': 0.03494950991410192}. Best is trial 76 with value: 0.17618370891181553.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:48,905] Trial 95 finished with value: 0.17618370891182145 and parameters: {'n_splines': 5, 'lam': 0.017432775542549506}. Best is trial 76 with value: 0.17618370891181553.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:48,949] Trial 96 finished with value: 0.1761837089118234 and parameters: {'n_splines': 5, 'lam': 0.00981908366876641}. Best is trial 76 with value: 0.17618370891181553.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:48,994] Trial 97 finished with value: 0.17618370891185983 and parameters: {'n_splines': 6, 'lam': 0.02290973295490535}. Best is trial 76 with value: 0.17618370891181553.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:49,037] Trial 98 finished with value: 0.17618370891181898 and parameters: {'n_splines': 5, 'lam': 0.04576825225424436}. Best is trial 76 with value: 0.17618370891181553.\n",
      "suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "[I 2024-02-02 12:44:49,077] Trial 99 finished with value: 0.17618370891182675 and parameters: {'n_splines': 5, 'lam': 0.04816290234906834}. Best is trial 76 with value: 0.17618370891181553.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CRPS GAM:  0.33310733322794117\n"
     ]
    }
   ],
   "source": [
    "#### GAM model\n",
    "def gam_model(trial):\n",
    "\n",
    "    # Define the hyperparameters to optimize\n",
    "    params = {'n_splines': trial.suggest_int('n_splines', 5, 20),\n",
    "              'lam': trial.suggest_loguniform('lam', 1e-3, 1)}\n",
    "\n",
    "    # Create and train the model\n",
    "    gam = LinearGAM(s(0, n_splines=params['n_splines'], lam=params['lam'])).fit(X_train_, y_train_)\n",
    "\n",
    "    # Predict on the validation set and calculate the CRPS\n",
    "    y_val_hat_gam = gam.predict(X_val)\n",
    "    std_dev_error = np.std(y_val - y_val_hat_gam)\n",
    "    crps_gam = [crps_gaussian(y_val_np[i], mu=y_val_hat_gam[i], sig=std_dev_error) for i in range(len(y_val_hat_gam))]\n",
    "    crps_gam = np.mean(crps_gam)\n",
    "\n",
    "    return crps_gam\n",
    "\n",
    "# Create the sampler and study\n",
    "sampler_gam = optuna.samplers.TPESampler(seed=seed)\n",
    "study_gam = optuna.create_study(sampler=sampler_gam, direction='minimize')\n",
    "\n",
    "# Optimize the model\n",
    "study_gam.optimize(gam_model, n_trials=N_TRIALS)\n",
    "\n",
    "# Create the final model with the best parameters\n",
    "best_params = study_gam.best_params\n",
    "final_gam_model = LinearGAM(s(0, n_splines=best_params['n_splines'], lam=best_params['lam']))\n",
    "\n",
    "# Fit the model\n",
    "final_gam_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_test_hat_gam = final_gam_model.predict(X_test)\n",
    "\n",
    "# Calculate the CRPS\n",
    "std_dev_error = np.std(y_test - y_test_hat_gam)\n",
    "crps_gam = [crps_gaussian(y_test_np[i], mu=y_test_hat_gam[i], sig=std_dev_error) for i in range(len(y_test_hat_gam))]\n",
    "crps_gam = np.mean(crps_gam)\n",
    "print(\"CRPS GAM: \", crps_gam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import setuptools\n",
    "import openml\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from engression import engression, engression_bagged\n",
    "import torch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from rtdl_revisiting_models import MLP, ResNet, FTTransformer\n",
    "import random\n",
    "import gpytorch\n",
    "import tqdm.auto as tqdm\n",
    "import os\n",
    "from pygam import LogisticGAM, s\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from sklearn.metrics import accuracy_score\n",
    "import gower\n",
    "\n",
    "\n",
    "#SUITE_ID = 336 # Regression on numerical features\n",
    "#SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "\n",
    "task_id=361110\n",
    "task = openml.tasks.get_task(task_id)  # download the OpenML task\n",
    "dataset = task.get_dataset()\n",
    "\n",
    "X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        dataset_format=\"dataframe\", target=dataset.default_target_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at alloc_cpu.cpp:80] data. DefaultCPUAllocator: not enough memory: you tried to allocate 1835201436 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 60\u001b[0m\n\u001b[0;32m     57\u001b[0m y_val \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39mloc[far_index_train]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Convert data to PyTorch tensors\u001b[39;00m\n\u001b[1;32m---> 60\u001b[0m X_train__tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m y_train__tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(y_train_\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m     62\u001b[0m X_train_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(X_train\u001b[38;5;241m.\u001b[39mvalues, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at alloc_cpu.cpp:80] data. DefaultCPUAllocator: not enough memory: you tried to allocate 1835201436 bytes."
     ]
    }
   ],
   "source": [
    "# Set the random seed for reproducibility\n",
    "N_TRIALS=100\n",
    "N_SAMPLES=100\n",
    "seed=10\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "\n",
    "# Compute Gower distance and define train and test set\n",
    "# calculate the Gower distance matrix\n",
    "X_gower = X.copy()\n",
    "\n",
    "for col in X_gower.select_dtypes(['category']).columns:\n",
    "    X_gower[col] = X_gower[col].astype('object')\n",
    "\n",
    "gower_dist_matrix = gower.gower_matrix(X_gower)\n",
    "\n",
    "# calculate the Gower distance for each data point\n",
    "gower_dist = np.mean(gower_dist_matrix, axis=1)\n",
    "\n",
    "gower_dist=pd.Series(gower_dist,index=X.index)\n",
    "far_index=gower_dist.index[np.where(gower_dist>=np.quantile(gower_dist,0.8))[0]]\n",
    "close_index=gower_dist.index[np.where(gower_dist<np.quantile(gower_dist,0.8))[0]]\n",
    "\n",
    "X_train = X.loc[close_index,:]\n",
    "X_gower_ = X_train.copy()\n",
    "\n",
    "for col in X_gower_.select_dtypes(['category']).columns:\n",
    "    X_gower_[col] = X_gower_[col].astype('object')\n",
    "\n",
    "# calculate the Gower distance matrix for the training set\n",
    "gower_dist_matrix_train = gower.gower_matrix(X_gower_)\n",
    "\n",
    "# calculate the Gower distance for each data point in the training set\n",
    "gower_dist_train = np.mean(gower_dist_matrix_train, axis=1)\n",
    "\n",
    "gower_dist_train=pd.Series(gower_dist_train,index=X_train.index)\n",
    "far_index_train=gower_dist_train.index[np.where(gower_dist_train>=np.quantile(gower_dist_train,0.8))[0]]\n",
    "close_index_train=gower_dist_train.index[np.where(gower_dist_train<np.quantile(gower_dist_train,0.8))[0]]\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "# Modify X_train_, X_val, X_train, and X_test to have dummy variables\n",
    "X = pd.get_dummies(X.astype(str), drop_first=True)\n",
    "\n",
    "X_train = X.loc[close_index,:]\n",
    "X_test = X.loc[far_index,:]\n",
    "y_train = y.loc[close_index]\n",
    "y_test = y.loc[far_index]\n",
    "\n",
    "X_train_ = X_train.loc[close_index_train,:]\n",
    "X_val = X_train.loc[far_index_train,:]\n",
    "y_train_ = y_train.loc[close_index_train]\n",
    "y_val = y_train.loc[far_index_train]\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train__tensor = torch.tensor(X_train_.values, dtype=torch.float32)\n",
    "y_train__tensor = torch.tensor(y_train_.values, dtype=torch.float32)\n",
    "X_train_tensor = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val.values, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val.values, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32)\n",
    "\n",
    "# Convert to use GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    X_train__tensor = X_train__tensor.cuda()\n",
    "    y_train__tensor = y_train__tensor.cuda()\n",
    "    X_train_tensor = X_train_tensor.cuda()\n",
    "    y_train_tensor = y_train_tensor.cuda()\n",
    "    X_val_tensor = X_val_tensor.cuda()\n",
    "    y_val_tensor = y_val_tensor.cuda()\n",
    "    X_test_tensor = X_test_tensor.cuda()\n",
    "    y_test_tensor = y_test_tensor.cuda()\n",
    "\n",
    "# Create flattened versions of the data\n",
    "y_val_np = y_val.values.flatten()\n",
    "y_test_np = y_test.values.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(38474, 8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[361110, 361111, 361113, 361282, 361283, 361285, 361286]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "benchmark_suite.tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
