{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from umap import UMAP\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openml\n",
    "from sklearn.linear_model import LinearRegression \n",
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "import gower\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "import gpytorch\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from engression import engression, engression_bagged\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "from rtdl_revisiting_models import MLP, ResNet, FTTransformer\n",
    "from sklearn.metrics.pairwise import euclidean_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#openml.config.apikey = 'FILL_IN_OPENML_API_KEY'  # set the OpenML Api Key\n",
    "SUITE_ID = 336 # Regression on numerical features\n",
    "#SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "\n",
    "task = openml.tasks.get_task(361072)  # download the OpenML task\n",
    "dataset = task.get_dataset()\n",
    "\n",
    "X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        dataset_format=\"dataframe\", target=dataset.default_target_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\Desktop\\THESIS_ETH_NEW\\CODE\\.venv\\lib\\site-packages\\umap\\umap_.py:1943: UserWarning: n_jobs value -1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(f\"n_jobs value {self.n_jobs} overridden to 1 by setting random_state. Use no seed for parallelism.\")\n"
     ]
    }
   ],
   "source": [
    "# Apply UMAP decomposition\n",
    "umap = UMAP(n_components=2, random_state=42)\n",
    "X_umap = umap.fit_transform(X)\n",
    "\n",
    "# calculate the Euclidean distance matrix\n",
    "euclidean_dist_matrix = euclidean_distances(X_umap)\n",
    "\n",
    "# calculate the Euclidean distance for each data point\n",
    "euclidean_dist = np.mean(euclidean_dist_matrix, axis=1)\n",
    "\n",
    "euclidean_dist = pd.Series(euclidean_dist, index=X.index)\n",
    "far_index = euclidean_dist.index[np.where(euclidean_dist >= np.quantile(euclidean_dist, 0.8))[0]]\n",
    "close_index = euclidean_dist.index[np.where(euclidean_dist < np.quantile(euclidean_dist, 0.8))[0]]\n",
    "\n",
    "X_train = X.loc[close_index,:]\n",
    "X_test = X.loc[far_index,:]\n",
    "y_train = y.loc[close_index]\n",
    "y_test = y.loc[far_index]\n",
    "\n",
    "# Apply UMAP decomposition on the training set\n",
    "X_umap_train = umap.fit_transform(X_train)\n",
    "\n",
    "# calculate the Euclidean distance matrix for the training set\n",
    "euclidean_dist_matrix_train = euclidean_distances(X_umap_train)\n",
    "\n",
    "# calculate the Euclidean distance for each data point in the training set\n",
    "euclidean_dist_train = np.mean(euclidean_dist_matrix_train, axis=1)\n",
    "\n",
    "euclidean_dist_train = pd.Series(euclidean_dist_train, index=X_train.index)\n",
    "far_index_train = euclidean_dist_train.index[np.where(euclidean_dist_train >= np.quantile(euclidean_dist_train, 0.8))[0]]\n",
    "close_index_train = euclidean_dist_train.index[np.where(euclidean_dist_train < np.quantile(euclidean_dist_train, 0.8))[0]]\n",
    "\n",
    "X_train_ = X_train.loc[close_index_train,:]\n",
    "X_val = X_train.loc[far_index_train,:]\n",
    "y_train_ = y_train.loc[close_index_train]\n",
    "y_val = y_train.loc[far_index_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(X_train_.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_.values, dtype=torch.float32)yy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5242])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor.shape\n",
    "y_train_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultivariateNormal(loc: torch.Size([5242]))\n",
      "tensor(6009.2500, grad_fn=<NegBackward0>)\n",
      "tensor([[0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931,\n",
      "         0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931,\n",
      "         0.6931, 0.6931, 0.6931]], grad_fn=<SoftplusBackward0>)\n",
      "MultivariateNormal(loc: torch.Size([5242]))\n",
      "tensor(6009.0293, grad_fn=<NegBackward0>)\n",
      "tensor([[0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931,\n",
      "         0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931,\n",
      "         0.6931, 0.6931, 0.6931]], grad_fn=<SoftplusBackward0>)\n",
      "MultivariateNormal(loc: torch.Size([5242]))\n",
      "tensor(6008.8096, grad_fn=<NegBackward0>)\n",
      "tensor([[0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931,\n",
      "         0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931,\n",
      "         0.6931, 0.6931, 0.6931]], grad_fn=<SoftplusBackward0>)\n",
      "MultivariateNormal(loc: torch.Size([5242]))\n",
      "tensor(6008.5894, grad_fn=<NegBackward0>)\n",
      "tensor([[0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931,\n",
      "         0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931,\n",
      "         0.6931, 0.6931, 0.6931]], grad_fn=<SoftplusBackward0>)\n",
      "MultivariateNormal(loc: torch.Size([5242]))\n",
      "tensor(6008.3701, grad_fn=<NegBackward0>)\n",
      "tensor([[0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931,\n",
      "         0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931,\n",
      "         0.6931, 0.6931, 0.6931]], grad_fn=<SoftplusBackward0>)\n"
     ]
    }
   ],
   "source": [
    "seed=10\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "import gpytorch\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "\n",
    "lengthscale=1\n",
    "\n",
    "class SVGPMODEL(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True)\n",
    "        super(SVGPMODEL, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=inducing_points.shape[1], lengthscale=lengthscale),\n",
    "            ard_num_dims=inducing_points.shape[1]\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_.values, dtype=torch.float32)\n",
    "\n",
    "    # Initialize the Gaussian Process model and likelihood\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = SVGPMODEL(X_train_tensor)\n",
    "\n",
    "    # Set the model in training mode\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "    # Define the learning params\n",
    "n_epochs=5 #trial.suggest_int('n_epochs', 100, 5000)\n",
    "learning_rate=0.001\n",
    "\n",
    "    # Use the negative log likelihood as the loss\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=y_train_tensor.numel())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train_tensor)\n",
    "    print(output)\n",
    "    loss = -mll(output, y_train_tensor)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    print(model.covar_module.base_kernel.lengthscale)\n",
    "    optimizer.step()\n",
    "\n",
    "    # Set the model in evaluation mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    y_pred = model(torch.tensor(X_val.values, dtype=torch.float32))\n",
    "\n",
    "    # Calculate RMSE\n",
    "RMSE_SVGP = torch.sqrt(torch.mean(torch.square(torch.tensor(y_val.values) - y_pred.mean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultivariateNormal(loc: torch.Size([5242]))\n",
      "tensor(3550.2903, grad_fn=<NegBackward0>)\n",
      "tensor([[0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931,\n",
      "         0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931,\n",
      "         0.6931, 0.6931, 0.6931]], grad_fn=<SoftplusBackward0>)\n",
      "MultivariateNormal(loc: torch.Size([5242]))\n",
      "tensor(3547.6541, grad_fn=<NegBackward0>)\n",
      "tensor([[0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931,\n",
      "         0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931,\n",
      "         0.6931, 0.6931, 0.6931]], grad_fn=<SoftplusBackward0>)\n",
      "MultivariateNormal(loc: torch.Size([5242]))\n",
      "tensor(3545.0198, grad_fn=<NegBackward0>)\n",
      "tensor([[0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931,\n",
      "         0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931,\n",
      "         0.6931, 0.6931, 0.6931]], grad_fn=<SoftplusBackward0>)\n",
      "MultivariateNormal(loc: torch.Size([5242]))\n",
      "tensor(3542.3904, grad_fn=<NegBackward0>)\n",
      "tensor([[0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931,\n",
      "         0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931,\n",
      "         0.6931, 0.6931, 0.6931]], grad_fn=<SoftplusBackward0>)\n",
      "MultivariateNormal(loc: torch.Size([5242]))\n",
      "tensor(3539.7622, grad_fn=<NegBackward0>)\n",
      "tensor([[0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931,\n",
      "         0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931, 0.6931,\n",
      "         0.6931, 0.6931, 0.6931]], grad_fn=<SoftplusBackward0>)\n"
     ]
    }
   ],
   "source": [
    "seed=10\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "import gpytorch\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "\n",
    "lengthscale=1\n",
    "\n",
    "'''class SVGPMODEL(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True)\n",
    "        super(SVGPMODEL, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=inducing_points.shape[1], lengthscale=lengthscale),\n",
    "            ard_num_dims=inducing_points.shape[1]\n",
    "        )\n",
    "        \n",
    "        def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)'''\n",
    "class GPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(GPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        # Define a prior for the lengthscale\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=train_x.shape[1], lengthscale=lengthscale),\n",
    "            ard_num_dims=train_x.shape[1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_.values, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train_.values, dtype=torch.float32)\n",
    "\n",
    "    # Initialize the Gaussian Process model and likelihood\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "model = GPModel(X_train_tensor, y_train_tensor, likelihood) #(X_train_tensor)\n",
    "\n",
    "    # Set the model in training mode\n",
    "model.train()\n",
    "likelihood.train()\n",
    "\n",
    "    # Define the learning params\n",
    "n_epochs=5 #trial.suggest_int('n_epochs', 100, 5000)\n",
    "learning_rate=0.001\n",
    "\n",
    "    # Use the negative log likelihood as the loss\n",
    "#mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=y_train_tensor.numel())\n",
    "#optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(X_train_tensor)\n",
    "    print(output)\n",
    "    loss = -mll(output, y_train_tensor)\n",
    "    print(loss)\n",
    "    loss.backward()\n",
    "    print(model.covar_module.base_kernel.lengthscale)\n",
    "    optimizer.step()\n",
    "\n",
    "    # Set the model in evaluation mode\n",
    "model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    y_pred = model(torch.tensor(X_val.values, dtype=torch.float32))\n",
    "\n",
    "    # Calculate RMSE\n",
    "RMSE_SVGP = torch.sqrt(torch.mean(torch.square(torch.tensor(y_val.values) - y_pred.mean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPModel(\n",
       "  (likelihood): GaussianLikelihood(\n",
       "    (noise_covar): HomoskedasticNoise(\n",
       "      (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "    )\n",
       "  )\n",
       "  (mean_module): ConstantMean()\n",
       "  (covar_module): ScaleKernel(\n",
       "    (base_kernel): MaternKernel(\n",
       "      (raw_lengthscale_constraint): Positive()\n",
       "    )\n",
       "    (raw_outputscale_constraint): Positive()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.covar_module.base_kernel.lengthscale\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian process with stochastic variational inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-12 10:45:07,387] A new study created in memory with name: no-name-2b329cdc-aab8-4e91-9a9e-c2d140ff1f4e\n",
      "[I 2024-01-12 10:49:35,844] Trial 0 finished with value: 79.97399139404297 and parameters: {'lengthscale': 0.08747537025773001, 'learning_rate': 0.00011376505702653915}. Best is trial 0 with value: 79.97399139404297.\n",
      "[W 2024-01-12 10:50:41,288] Trial 1 failed with parameters: {'lengthscale': 0.005044685709888605, 'learning_rate': 0.010495405390719734} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\dalma\\AppData\\Local\\Temp\\ipykernel_24668\\2789280030.py\", line 55, in SVGP_opt\n",
      "    output = model(X_train_tensor)\n",
      "  File \"c:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gpytorch\\models\\approximate_gp.py\", line 108, in __call__\n",
      "    return self.variational_strategy(inputs, prior=prior, **kwargs)\n",
      "  File \"c:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gpytorch\\variational\\variational_strategy.py\", line 272, in __call__\n",
      "    return super().__call__(x, prior=prior, **kwargs)\n",
      "  File \"c:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gpytorch\\variational\\_variational_strategy.py\", line 341, in __call__\n",
      "    return super().__call__(\n",
      "  File \"c:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gpytorch\\module.py\", line 31, in __call__\n",
      "    outputs = self.forward(*inputs, **kwargs)\n",
      "  File \"c:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gpytorch\\variational\\variational_strategy.py\", line 232, in forward\n",
      "    MatmulLinearOperator(interp_term.transpose(-1, -2), middle_term @ interp_term),\n",
      "  File \"c:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\linear_operator\\operators\\_linear_operator.py\", line 2897, in __matmul__\n",
      "    return self.matmul(other)\n",
      "  File \"c:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\linear_operator\\operators\\_linear_operator.py\", line 1831, in matmul\n",
      "    return Matmul.apply(self.representation_tree(), other, *self.representation())\n",
      "  File \"c:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\function.py\", line 539, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"c:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\linear_operator\\functions\\_matmul.py\", line 21, in forward\n",
      "    res = linear_op._matmul(rhs)\n",
      "  File \"c:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\linear_operator\\operators\\sum_linear_operator.py\", line 49, in _matmul\n",
      "    return sum(linear_op._matmul(rhs) for linear_op in self.linear_ops)\n",
      "  File \"c:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\linear_operator\\operators\\sum_linear_operator.py\", line 49, in <genexpr>\n",
      "    return sum(linear_op._matmul(rhs) for linear_op in self.linear_ops)\n",
      "  File \"c:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\linear_operator\\operators\\root_linear_operator.py\", line 70, in _matmul\n",
      "    return self.root._matmul(self.root._t_matmul(rhs))\n",
      "  File \"c:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\linear_operator\\operators\\triangular_linear_operator.py\", line 107, in _matmul\n",
      "    return self._tensor.matmul(rhs)\n",
      "  File \"c:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\linear_operator\\operators\\_linear_operator.py\", line 1831, in matmul\n",
      "    return Matmul.apply(self.representation_tree(), other, *self.representation())\n",
      "  File \"c:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\function.py\", line 539, in apply\n",
      "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
      "  File \"c:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\linear_operator\\functions\\_matmul.py\", line 21, in forward\n",
      "    res = linear_op._matmul(rhs)\n",
      "  File \"c:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\linear_operator\\operators\\dense_linear_operator.py\", line 65, in _matmul\n",
      "    return torch.matmul(self.tensor, rhs)\n",
      "KeyboardInterrupt\n",
      "[W 2024-01-12 10:50:41,672] Trial 1 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 80\u001b[0m\n\u001b[0;32m     78\u001b[0m sampler_SVGP \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     79\u001b[0m study_SVGP \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(sampler\u001b[38;5;241m=\u001b[39msampler_SVGP, direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 80\u001b[0m \u001b[43mstudy_SVGP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSVGP_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_TRIALS\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[12], line 55\u001b[0m, in \u001b[0;36mSVGP_opt\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_epochs):\n\u001b[0;32m     54\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 55\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmll(output, y_train_tensor)\n\u001b[0;32m     57\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gpytorch\\models\\approximate_gp.py:108\u001b[0m, in \u001b[0;36mApproximateGP.__call__\u001b[1;34m(self, inputs, prior, **kwargs)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    107\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariational_strategy(inputs, prior\u001b[38;5;241m=\u001b[39mprior, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gpytorch\\variational\\variational_strategy.py:272\u001b[0m, in \u001b[0;36mVariationalStrategy.__call__\u001b[1;34m(self, x, prior, **kwargs)\u001b[0m\n\u001b[0;32m    269\u001b[0m         \u001b[38;5;66;03m# Mark that we have updated the variational strategy\u001b[39;00m\n\u001b[0;32m    270\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mupdated_strategy\u001b[38;5;241m.\u001b[39mfill_(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m--> 272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(x, prior\u001b[38;5;241m=\u001b[39mprior, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gpytorch\\variational\\_variational_strategy.py:341\u001b[0m, in \u001b[0;36m_VariationalStrategy.__call__\u001b[1;34m(self, x, prior, **kwargs)\u001b[0m\n\u001b[0;32m    339\u001b[0m \u001b[38;5;66;03m# Get q(f)\u001b[39;00m\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(variational_dist_u, MultivariateNormal):\n\u001b[1;32m--> 341\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    342\u001b[0m         x,\n\u001b[0;32m    343\u001b[0m         inducing_points,\n\u001b[0;32m    344\u001b[0m         inducing_values\u001b[38;5;241m=\u001b[39mvariational_dist_u\u001b[38;5;241m.\u001b[39mmean,\n\u001b[0;32m    345\u001b[0m         variational_inducing_covar\u001b[38;5;241m=\u001b[39mvariational_dist_u\u001b[38;5;241m.\u001b[39mlazy_covariance_matrix,\n\u001b[0;32m    346\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    347\u001b[0m     )\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(variational_dist_u, Delta):\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    350\u001b[0m         x, inducing_points, inducing_values\u001b[38;5;241m=\u001b[39mvariational_dist_u\u001b[38;5;241m.\u001b[39mmean, variational_inducing_covar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    351\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gpytorch\\module.py:31\u001b[0m, in \u001b[0;36mModule.__call__\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Tensor, Distribution, LinearOperator]:\n\u001b[1;32m---> 31\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(\u001b[38;5;241m*\u001b[39minputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m     33\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [_validate_module_outputs(output) \u001b[38;5;28;01mfor\u001b[39;00m output \u001b[38;5;129;01min\u001b[39;00m outputs]\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\gpytorch\\variational\\variational_strategy.py:232\u001b[0m, in \u001b[0;36mVariationalStrategy.forward\u001b[1;34m(self, x, inducing_points, inducing_values, variational_inducing_covar, **kwargs)\u001b[0m\n\u001b[0;32m    225\u001b[0m     predictive_covar \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    226\u001b[0m         data_data_covar\u001b[38;5;241m.\u001b[39madd_jitter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjitter_val)\u001b[38;5;241m.\u001b[39mto_dense()\n\u001b[0;32m    227\u001b[0m         \u001b[38;5;241m+\u001b[39m interp_term\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m) \u001b[38;5;241m@\u001b[39m middle_term\u001b[38;5;241m.\u001b[39mto_dense() \u001b[38;5;241m@\u001b[39m interp_term\n\u001b[0;32m    228\u001b[0m     )\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    230\u001b[0m     predictive_covar \u001b[38;5;241m=\u001b[39m SumLinearOperator(\n\u001b[0;32m    231\u001b[0m         data_data_covar\u001b[38;5;241m.\u001b[39madd_jitter(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjitter_val),\n\u001b[1;32m--> 232\u001b[0m         MatmulLinearOperator(interp_term\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m), \u001b[43mmiddle_term\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minterp_term\u001b[49m),\n\u001b[0;32m    233\u001b[0m     )\n\u001b[0;32m    235\u001b[0m \u001b[38;5;66;03m# Return the distribution\u001b[39;00m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m MultivariateNormal(predictive_mean, predictive_covar)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\linear_operator\\operators\\_linear_operator.py:2897\u001b[0m, in \u001b[0;36mLinearOperator.__matmul__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   2891\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__matmul__\u001b[39m(\n\u001b[0;32m   2892\u001b[0m     \u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   2893\u001b[0m     other: Union[\n\u001b[0;32m   2894\u001b[0m         Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch2 N D\u001b[39m\u001b[38;5;124m\"\u001b[39m], Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mN\u001b[39m\u001b[38;5;124m\"\u001b[39m], Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch2 N D\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2895\u001b[0m     ],\n\u001b[0;32m   2896\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... M D\u001b[39m\u001b[38;5;124m\"\u001b[39m], Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... M\u001b[39m\u001b[38;5;124m\"\u001b[39m], Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... M D\u001b[39m\u001b[38;5;124m\"\u001b[39m]]:\n\u001b[1;32m-> 2897\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\linear_operator\\operators\\_linear_operator.py:1831\u001b[0m, in \u001b[0;36mLinearOperator.matmul\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1827\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatmul_linear_operator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatmulLinearOperator\n\u001b[0;32m   1829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m MatmulLinearOperator(\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m-> 1831\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatmul\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    547\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\linear_operator\\functions\\_matmul.py:21\u001b[0m, in \u001b[0;36mMatmul.forward\u001b[1;34m(ctx, representation_tree, rhs, *matrix_args)\u001b[0m\n\u001b[0;32m     18\u001b[0m     is_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     20\u001b[0m linear_op \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39mrepresentation_tree(\u001b[38;5;241m*\u001b[39mmatrix_args)\n\u001b[1;32m---> 21\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_matmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m to_save \u001b[38;5;241m=\u001b[39m [orig_rhs] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(matrix_args)\n\u001b[0;32m     24\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mto_save)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\linear_operator\\operators\\sum_linear_operator.py:49\u001b[0m, in \u001b[0;36mSumLinearOperator._matmul\u001b[1;34m(self, rhs)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_matmul\u001b[39m(\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     47\u001b[0m     rhs: Union[Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch2 N C\u001b[39m\u001b[38;5;124m\"\u001b[39m], Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch2 N\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n\u001b[0;32m     48\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... M C\u001b[39m\u001b[38;5;124m\"\u001b[39m], Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... M\u001b[39m\u001b[38;5;124m\"\u001b[39m]]:\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlinear_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_matmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mlinear_op\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear_ops\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\linear_operator\\operators\\sum_linear_operator.py:49\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_matmul\u001b[39m(\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     47\u001b[0m     rhs: Union[Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch2 N C\u001b[39m\u001b[38;5;124m\"\u001b[39m], Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch2 N\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n\u001b[0;32m     48\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... M C\u001b[39m\u001b[38;5;124m\"\u001b[39m], Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... M\u001b[39m\u001b[38;5;124m\"\u001b[39m]]:\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msum\u001b[39m(\u001b[43mlinear_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_matmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m linear_op \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_ops)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\linear_operator\\operators\\root_linear_operator.py:70\u001b[0m, in \u001b[0;36mRootLinearOperator._matmul\u001b[1;34m(self, rhs)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_matmul\u001b[39m(\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     68\u001b[0m     rhs: Union[Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch2 N C\u001b[39m\u001b[38;5;124m\"\u001b[39m], Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch2 N\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n\u001b[0;32m     69\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... M C\u001b[39m\u001b[38;5;124m\"\u001b[39m], Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... M\u001b[39m\u001b[38;5;124m\"\u001b[39m]]:\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_matmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mroot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_t_matmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\linear_operator\\operators\\triangular_linear_operator.py:107\u001b[0m, in \u001b[0;36mTriangularLinearOperator._matmul\u001b[1;34m(self, rhs)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_matmul\u001b[39m(\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m    105\u001b[0m     rhs: Union[Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch2 N C\u001b[39m\u001b[38;5;124m\"\u001b[39m], Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch2 N\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n\u001b[0;32m    106\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... M C\u001b[39m\u001b[38;5;124m\"\u001b[39m], Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... M\u001b[39m\u001b[38;5;124m\"\u001b[39m]]:\n\u001b[1;32m--> 107\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\linear_operator\\operators\\_linear_operator.py:1831\u001b[0m, in \u001b[0;36mLinearOperator.matmul\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m   1827\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmatmul_linear_operator\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MatmulLinearOperator\n\u001b[0;32m   1829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m MatmulLinearOperator(\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m-> 1831\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mMatmul\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation_tree\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\function.py:539\u001b[0m, in \u001b[0;36mFunction.apply\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m    536\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[0;32m    537\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[0;32m    538\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[1;32m--> 539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[0;32m    542\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    543\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    544\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    545\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    546\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    547\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\linear_operator\\functions\\_matmul.py:21\u001b[0m, in \u001b[0;36mMatmul.forward\u001b[1;34m(ctx, representation_tree, rhs, *matrix_args)\u001b[0m\n\u001b[0;32m     18\u001b[0m     is_vector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     20\u001b[0m linear_op \u001b[38;5;241m=\u001b[39m ctx\u001b[38;5;241m.\u001b[39mrepresentation_tree(\u001b[38;5;241m*\u001b[39mmatrix_args)\n\u001b[1;32m---> 21\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_matmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     23\u001b[0m to_save \u001b[38;5;241m=\u001b[39m [orig_rhs] \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(matrix_args)\n\u001b[0;32m     24\u001b[0m ctx\u001b[38;5;241m.\u001b[39msave_for_backward(\u001b[38;5;241m*\u001b[39mto_save)\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\linear_operator\\operators\\dense_linear_operator.py:65\u001b[0m, in \u001b[0;36mDenseLinearOperator._matmul\u001b[1;34m(self, rhs)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_matmul\u001b[39m(\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m: Float[LinearOperator, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch M N\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     63\u001b[0m     rhs: Union[Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch2 N C\u001b[39m\u001b[38;5;124m\"\u001b[39m], Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*batch2 N\u001b[39m\u001b[38;5;124m\"\u001b[39m]],\n\u001b[0;32m     64\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... M C\u001b[39m\u001b[38;5;124m\"\u001b[39m], Float[torch\u001b[38;5;241m.\u001b[39mTensor, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m... M\u001b[39m\u001b[38;5;124m\"\u001b[39m]]:\n\u001b[1;32m---> 65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrhs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gpytorch\n",
    "from gpytorch.variational import CholeskyVariationalDistribution\n",
    "from gpytorch.variational import VariationalStrategy\n",
    "\n",
    "N_TRIALS=2\n",
    "\n",
    "def SVGP_opt(trial):\n",
    "\n",
    "    seed=10\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    lengthscale=trial.suggest_float('lengthscale', 1e-8, 10, log=True)\n",
    "\n",
    "    class SVGPMODEL(gpytorch.models.ApproximateGP):\n",
    "        def __init__(self, inducing_points):\n",
    "            variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "            variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True)\n",
    "            super(SVGPMODEL, self).__init__(variational_strategy)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "                gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=inducing_points.shape[1], lengthscale=lengthscale),\n",
    "                ard_num_dims=inducing_points.shape[1]\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            mean_x = self.mean_module(x)\n",
    "            covar_x = self.covar_module(x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train_.values, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train_.values, dtype=torch.float32)\n",
    "\n",
    "    # Initialize the Gaussian Process model and likelihood\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = SVGPMODEL(X_train_tensor)\n",
    "\n",
    "    # Set the model in training mode\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Define the learning params\n",
    "    n_epochs=5 #trial.suggest_int('n_epochs', 100, 5000)\n",
    "    learning_rate=trial.suggest_float('learning_rate', 0.0001, 0.05, log=True)\n",
    "\n",
    "    # Use the negative log likelihood as the loss\n",
    "    mll = gpytorch.mlls.VariationalELBO(likelihood, model, num_data=y_train_tensor.numel())\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_tensor)\n",
    "        print(output)\n",
    "        loss = -mll(output, y_train_tensor)\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Set the model in evaluation mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        y_pred = model(torch.tensor(X_val.values, dtype=torch.float32))\n",
    "\n",
    "    # Calculate RMSE\n",
    "    RMSE_SVGP = torch.sqrt(torch.mean(torch.square(torch.tensor(y_val.values) - y_pred.mean)))\n",
    "\n",
    "    return RMSE_SVGP\n",
    "\n",
    "sampler_SVGP = optuna.samplers.TPESampler(seed=10)\n",
    "study_SVGP = optuna.create_study(sampler=sampler_SVGP, direction='minimize')\n",
    "study_SVGP.optimize(SVGP_opt, n_trials=N_TRIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'lengthscale'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Access the best parameters\u001b[39;00m\n\u001b[0;32m      2\u001b[0m best_params_SVGP \u001b[38;5;241m=\u001b[39m study_SVGP\u001b[38;5;241m.\u001b[39mbest_params\n\u001b[1;32m----> 3\u001b[0m lengthscale \u001b[38;5;241m=\u001b[39m \u001b[43mbest_params_SVGP\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlengthscale\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      4\u001b[0m n_epochs \u001b[38;5;241m=\u001b[39m best_params_SVGP[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_epochs\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      5\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m best_params_SVGP[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'lengthscale'"
     ]
    }
   ],
   "source": [
    "# Access the best parameters\n",
    "best_params_SVGP = study_SVGP.best_params\n",
    "lengthscale = best_params_SVGP['lengthscale']\n",
    "n_epochs = best_params_SVGP['n_epochs']\n",
    "learning_rate = best_params_SVGP['learning_rate']\n",
    "\n",
    "class SVGPMODEL(gpytorch.models.ApproximateGP):\n",
    "    def __init__(self, inducing_points):\n",
    "        variational_distribution = CholeskyVariationalDistribution(inducing_points.size(0))\n",
    "        variational_strategy = VariationalStrategy(self, inducing_points, variational_distribution, learn_inducing_locations=True)\n",
    "        super(SVGPMODEL, self).__init__(variational_strategy)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=inducing_points.shape[1], lengthscale=lengthscale),\n",
    "            ard_num_dims=inducing_points.shape[1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "# Initialize the final Gaussian Process model with the best parameters\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "final_model = SVGPMODEL(X_tensor)\n",
    "\n",
    "# Set the model in training mode\n",
    "final_model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the negative log likelihood as the loss\n",
    "mll = gpytorch.mlls.VariationalELBO(likelihood, final_model, num_data=y_tensor.numel())\n",
    "optimizer = torch.optim.Adam(final_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = final_model(X_tensor)\n",
    "    loss = -mll(output, y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Set the final model in evaluation mode\n",
    "final_model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Make predictions on the validation set\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    y_pred = final_model(torch.tensor(X_test.values, dtype=torch.float32))\n",
    "\n",
    "# Calculate RMSE\n",
    "RMSE_SVGP = torch.sqrt(torch.mean(torch.square(torch.tensor(y_test.values) - y_pred.mean)))\n",
    "print(\"RMSE SVGP: \", RMSE_SVGP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gaussian process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-09 15:39:52,596] A new study created in memory with name: no-name-5b64c314-cee9-4253-bb1a-6badf9a912e4\n",
      "[W 2024-01-09 16:24:17,381] Trial 0 failed with parameters: {'loc': 0.7724640400504122, 'scale': 0.011355222730341348, 'n_epochs': 3205, 'learning_rate': 0.03746531373867673} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py\", line 200, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"C:\\Users\\dalma\\AppData\\Local\\Temp\\ipykernel_29424\\1361086071.py\", line 54, in GP_opt\n",
      "    loss.backward()\n",
      "  File \"c:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py\", line 492, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"c:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py\", line 251, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "[W 2024-01-09 16:24:17,559] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 72\u001b[0m\n\u001b[0;32m     70\u001b[0m sampler_GP \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39msamplers\u001b[38;5;241m.\u001b[39mTPESampler(seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m     71\u001b[0m study_GP \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(sampler\u001b[38;5;241m=\u001b[39msampler_GP, direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 72\u001b[0m \u001b[43mstudy_GP\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mGP_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mN_TRIALS\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\study.py:451\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[1;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    350\u001b[0m     func: ObjectiveFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    357\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    358\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[0;32m    360\u001b[0m \n\u001b[0;32m    361\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[0;32m    450\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 451\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    453\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    456\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    460\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    461\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py:66\u001b[0m, in \u001b[0;36m_optimize\u001b[1;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m---> 66\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     75\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     76\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     78\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py:163\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[1;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[0;32m    160\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 163\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    165\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[0;32m    166\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[0;32m    167\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[0;32m    168\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py:251\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    247\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[0;32m    248\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    249\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[0;32m    250\u001b[0m ):\n\u001b[1;32m--> 251\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[0;32m    252\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\optuna\\study\\_optimize.py:200\u001b[0m, in \u001b[0;36m_run_trial\u001b[1;34m(study, func, catch)\u001b[0m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[0;32m    199\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    202\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[0;32m    203\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[1;32mIn[31], line 54\u001b[0m, in \u001b[0;36mGP_opt\u001b[1;34m(trial)\u001b[0m\n\u001b[0;32m     52\u001b[0m     output \u001b[38;5;241m=\u001b[39m model(X_train_tensor)\n\u001b[0;32m     53\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39mmll(output, y_train_tensor)\n\u001b[1;32m---> 54\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# Set the model in evaluation mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    491\u001b[0m     )\n\u001b[1;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\autograd\\__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "N_TRIALS=5\n",
    "\n",
    "def GP_opt(trial):\n",
    "\n",
    "    seed=10\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    lengthscale=trial.suggest_float('lengthscale', 1e-8, 10, log=True)\n",
    "\n",
    "    class GPModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(GPModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            # Define a prior for the lengthscale\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "                gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=train_x.shape[1], lengthscale=lengthscale),\n",
    "                ard_num_dims=train_x.shape[1]\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            mean_x = self.mean_module(x)\n",
    "            covar_x = self.covar_module(x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "    # Convert data to PyTorch tensors\n",
    "    X_train_tensor = torch.tensor(X_train_.values, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train_.values, dtype=torch.float32)\n",
    "\n",
    "    # Initialize the Gaussian Process model and likelihood\n",
    "    likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    model = GPModel(X_train_tensor, y_train_tensor, likelihood)\n",
    "\n",
    "    # Set the model in training mode\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Define the learning params\n",
    "    n_epochs=1#trial.suggest_int('n_epochs', 100, 5000)\n",
    "    learning_rate=trial.suggest_float('learning_rate', 0.0001, 0.05, log=True)\n",
    "\n",
    "    # Use the negative log likelihood as the loss\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(n_epochs):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_tensor)\n",
    "        loss = -mll(output, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Set the model in evaluation mode\n",
    "    model.eval()\n",
    "    likelihood.eval()\n",
    "\n",
    "    # Make predictions on the validation set\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        y_pred = model(torch.tensor(X_val.values, dtype=torch.float32))\n",
    "\n",
    "    # Calculate RMSE\n",
    "    RMSE_GP = torch.sqrt(torch.mean(torch.square(torch.tensor(y_val.values) - y_pred.mean)))\n",
    "\n",
    "    return RMSE_GP\n",
    "\n",
    "sampler_GP = optuna.samplers.TPESampler(seed=10)\n",
    "study_GP = optuna.create_study(sampler=sampler_GP, direction='minimize')\n",
    "study_GP.optimize(GP_opt, n_trials=N_TRIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the best parameters\n",
    "best_params_GP = study_GP.best_params\n",
    "lengthscale = best_params_SVGP['lengthscale']\n",
    "n_epochs=best_params_GP['n_epochs']\n",
    "learning_rate=best_params_GP['learning_rate']\n",
    "\n",
    "class GPModel(gpytorch.models.ExactGP):\n",
    "        def __init__(self, train_x, train_y, likelihood):\n",
    "            super(GPModel, self).__init__(train_x, train_y, likelihood)\n",
    "            self.mean_module = gpytorch.means.ConstantMean()\n",
    "            # Define a prior for the lengthscale\n",
    "            self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "                gpytorch.kernels.MaternKernel(nu=1.5, ard_num_dims=train_x.shape[1], lengthscale=lengthscale),\n",
    "                ard_num_dims=train_x.shape[1]\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            mean_x = self.mean_module(x)\n",
    "            covar_x = self.covar_module(x)\n",
    "            return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "# Initialize the final Gaussian Process model with the best parameters\n",
    "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "final_model = GPModel(X_tensor, y_tensor, likelihood)\n",
    "\n",
    "# Set the model in training mode\n",
    "final_model.train()\n",
    "likelihood.train()\n",
    "\n",
    "# Use the negative log likelihood as the loss\n",
    "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, final_model)\n",
    "optimizer = torch.optim.Adam(final_model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(n_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    output = final_model(X_tensor)\n",
    "    loss = -mll(output, y_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Set the final model in evaluation mode\n",
    "final_model.eval()\n",
    "likelihood.eval()\n",
    "\n",
    "# Make predictions on the validation set\n",
    "with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "    y_pred = final_model(torch.tensor(X_test.values, dtype=torch.float32))\n",
    "\n",
    "# Calculate RMSE\n",
    "RMSE_GP = torch.sqrt(torch.mean(torch.square(torch.tensor(y_test.values) - y_pred.mean)))\n",
    "print(\"RMSE GP: \", RMSE_GP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-23 17:24:18,248] A new study created in memory with name: no-name-da428c83-3742-4845-8cb0-a33c232b7db0\n",
      "[I 2023-12-23 17:24:38,519] Trial 0 finished with value: 60.821720123291016 and parameters: {'n_blocks': 4, 'd_block': 20, 'dropout': 0.6336482349262754, 'n_epochs': 3769, 'learning_rate': 0.02497549991389926}. Best is trial 0 with value: 60.821720123291016.\n",
      "[I 2023-12-23 17:24:48,376] Trial 1 finished with value: 502.2777099609375 and parameters: {'n_blocks': 2, 'd_block': 107, 'dropout': 0.7605307121989587, 'n_epochs': 928, 'learning_rate': 0.004508156727283112}. Best is trial 0 with value: 60.821720123291016.\n",
      "[I 2023-12-23 17:35:15,252] Trial 2 finished with value: 25.43311882019043 and parameters: {'n_blocks': 4, 'd_block': 478, 'dropout': 0.003948266327914451, 'n_epochs': 2610, 'learning_rate': 0.040649785986440465}. Best is trial 2 with value: 25.43311882019043.\n",
      "[I 2023-12-23 17:46:23,421] Trial 3 finished with value: 79.76200103759766 and parameters: {'n_blocks': 4, 'd_block': 364, 'dropout': 0.29187606817063316, 'n_epochs': 4598, 'learning_rate': 0.03575733159154476}. Best is trial 2 with value: 25.43311882019043.\n",
      "[I 2023-12-23 17:47:11,332] Trial 4 finished with value: 77.59317779541016 and parameters: {'n_blocks': 3, 'd_block': 79, 'dropout': 0.3733407600514692, 'n_epochs': 3403, 'learning_rate': 0.022147475403707505}. Best is trial 2 with value: 25.43311882019043.\n"
     ]
    }
   ],
   "source": [
    "N_TRIALS=5\n",
    "\n",
    "d_out = 1  \n",
    "d_in=X_train_.shape[1]\n",
    "\n",
    "def MLP_opt(trial):\n",
    "\n",
    "    seed=10\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    n_blocks = trial.suggest_int(\"n_blocks\", 1, 5)\n",
    "    d_block = trial.suggest_int(\"d_block\", 10, 500)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0, 1)\n",
    "\n",
    "    MLP_model = MLP(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    n_blocks=n_blocks,\n",
    "    d_block=d_block,\n",
    "    dropout=dropout,\n",
    "    )\n",
    "    n_epochs=trial.suggest_int('n_epochs', 100, 5000)\n",
    "    learning_rate=trial.suggest_float('learning_rate', 0.0001, 0.05, log=True)\n",
    "    optimizer=torch.optim.Adam(MLP_model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    loss_Adam=[]\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        # making a pridiction in forward pass\n",
    "        y_train_hat = MLP_model(torch.Tensor(X_train_.values)).reshape(-1,)\n",
    "        # calculating the loss between original and predicted data points\n",
    "        loss = criterion(y_train_hat, torch.Tensor(y_train_.values))\n",
    "        # store loss into list\n",
    "        loss_Adam.append(loss.item())\n",
    "        # zeroing gradients after each iteration\n",
    "        optimizer.zero_grad()\n",
    "        # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n",
    "        loss.backward()\n",
    "        # updating the parameters after each iteration\n",
    "        optimizer.step()\n",
    "\n",
    "    y_val_hat_MLP=(MLP_model(torch.Tensor(X_val.values)).reshape(-1,)).detach().numpy()\n",
    "    RMSE_MLP=np.sqrt(np.mean((y_val-y_val_hat_MLP)**2))\n",
    "\n",
    "    return RMSE_MLP\n",
    "\n",
    "sampler_MLP = optuna.samplers.TPESampler(seed=10)\n",
    "study_MLP = optuna.create_study(sampler=sampler_MLP, direction='minimize')\n",
    "study_MLP.optimize(MLP_opt, n_trials=N_TRIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE MLP:  2127.5535\n"
     ]
    }
   ],
   "source": [
    "seed=10\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "MLP_model = MLP(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    n_blocks=study_MLP.best_params['n_blocks'],\n",
    "    d_block=study_MLP.best_params['d_block'],\n",
    "    dropout=study_MLP.best_params['dropout'],\n",
    "    )\n",
    "n_epochs=study_MLP.best_params['n_epochs']\n",
    "learning_rate=study_MLP.best_params['learning_rate']\n",
    "optimizer=torch.optim.Adam(MLP_model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "loss_Adam=[]\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    # making a pridiction in forward pass\n",
    "    y_train_hat = MLP_model(torch.Tensor(X_train.values)).reshape(-1,)\n",
    "    # calculating the loss between original and predicted data points\n",
    "    loss = criterion(y_train_hat, torch.Tensor(y_train.values))\n",
    "    # store loss into list\n",
    "    loss_Adam.append(loss.item())\n",
    "    # zeroing gradients after each iteration\n",
    "    optimizer.zero_grad()\n",
    "    # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n",
    "    loss.backward()\n",
    "    # updating the parameters after each iteration\n",
    "    optimizer.step()\n",
    "\n",
    "y_test_hat_MLP=(MLP_model(torch.Tensor(X_test.values)).reshape(-1,)).detach().numpy()\n",
    "RMSE_MLP=np.sqrt(np.mean((y_test-y_test_hat_MLP)**2))\n",
    "print(\"RMSE MLP: \", RMSE_MLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-23 18:10:31,640] A new study created in memory with name: no-name-944004cd-5e6d-4aa6-9fe6-432d77df0455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-23 18:10:52,768] Trial 0 finished with value: 30.78984260559082 and parameters: {'n_blocks': 4, 'd_block': 20, 'dropout1': 0.6336482349262754, 'dropout2': 0.7488038825386119, 'd_hidden_multiplier': 1.7462675307564761, 'n_epochs': 1201, 'learning_rate': 0.009983336951505236}. Best is trial 0 with value: 30.78984260559082.\n",
      "[I 2023-12-23 18:11:04,384] Trial 1 finished with value: 34.90650177001953 and parameters: {'n_blocks': 4, 'd_block': 93, 'dropout1': 0.08833981417401027, 'dropout2': 0.6853598183677972, 'd_hidden_multiplier': 2.8834833654873413, 'n_epochs': 119, 'learning_rate': 0.02565839394295025}. Best is trial 0 with value: 30.78984260559082.\n",
      "[I 2023-12-27 08:43:55,096] Trial 2 finished with value: 23.0227108001709 and parameters: {'n_blocks': 5, 'd_block': 310, 'dropout1': 0.7217553174317995, 'dropout2': 0.29187606817063316, 'd_hidden_multiplier': 2.7944353062823586, 'n_epochs': 3602, 'learning_rate': 0.027172963963761936}. Best is trial 2 with value: 23.0227108001709.\n",
      "[I 2023-12-27 08:49:54,747] Trial 3 finished with value: 26.41994285583496 and parameters: {'n_blocks': 1, 'd_block': 193, 'dropout1': 0.6741336150663453, 'dropout2': 0.4418331744229961, 'd_hidden_multiplier': 1.5850349833332342, 'n_epochs': 3127, 'learning_rate': 0.025705598303464104}. Best is trial 2 with value: 23.0227108001709.\n",
      "[I 2023-12-27 09:23:20,686] Trial 4 finished with value: 25.368022918701172 and parameters: {'n_blocks': 4, 'd_block': 305, 'dropout1': 0.8052231968327465, 'dropout2': 0.5216471523936341, 'd_hidden_multiplier': 2.7716222020216708, 'n_epochs': 1664, 'learning_rate': 0.0046139215286182785}. Best is trial 2 with value: 23.0227108001709.\n"
     ]
    }
   ],
   "source": [
    "N_TRIALS=5\n",
    "\n",
    "d_out = 1  \n",
    "d_in=X_train_.shape[1]\n",
    "\n",
    "def ResNet_opt(trial):\n",
    "\n",
    "    seed=10\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    n_blocks = trial.suggest_int(\"n_blocks\", 1, 5)\n",
    "    d_block = trial.suggest_int(\"d_block\", 10, 500)\n",
    "    dropout1 = trial.suggest_float(\"dropout1\", 0, 1)\n",
    "    dropout2 = trial.suggest_float(\"dropout2\", 0, 1)\n",
    "    d_hidden_multiplier=trial.suggest_float(\"d_hidden_multiplier\", 0.5, 3)\n",
    "\n",
    "    ResNet_model = ResNet(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    n_blocks=n_blocks,\n",
    "    d_block=d_block,\n",
    "    d_hidden=None,\n",
    "    d_hidden_multiplier=d_hidden_multiplier,\n",
    "    dropout1=dropout1,\n",
    "    dropout2=dropout2,\n",
    "    )\n",
    "    n_epochs=trial.suggest_int('n_epochs', 100, 5000)\n",
    "    learning_rate=trial.suggest_float('learning_rate', 0.0001, 0.05, log=True)\n",
    "    optimizer=torch.optim.Adam(ResNet_model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    loss_Adam=[]\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        # making a pridiction in forward pass\n",
    "        y_train_hat = ResNet_model(torch.Tensor(X_train_.values)).reshape(-1,)\n",
    "        # calculating the loss between original and predicted data points\n",
    "        loss = criterion(y_train_hat, torch.Tensor(y_train_.values))\n",
    "        # store loss into list\n",
    "        loss_Adam.append(loss.item())\n",
    "        # zeroing gradients after each iteration\n",
    "        optimizer.zero_grad()\n",
    "        # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n",
    "        loss.backward()\n",
    "        # updating the parameters after each iteration\n",
    "        optimizer.step()\n",
    "\n",
    "    y_val_hat_ResNet=(ResNet_model(torch.Tensor(X_val.values)).reshape(-1,)).detach().numpy()\n",
    "    RMSE_ResNet=np.sqrt(np.mean((y_val-y_val_hat_ResNet)**2))\n",
    "\n",
    "    return RMSE_ResNet\n",
    "\n",
    "sampler_ResNet = optuna.samplers.TPESampler(seed=10)\n",
    "study_ResNet = optuna.create_study(sampler=sampler_ResNet, direction='minimize')\n",
    "study_ResNet.optimize(ResNet_opt, n_trials=N_TRIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE ResNet:  28.044754\n"
     ]
    }
   ],
   "source": [
    "seed=10\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "ResNet_model = ResNet(\n",
    "    d_in=d_in,\n",
    "    d_out=d_out,\n",
    "    n_blocks=study_ResNet.best_params['n_blocks'],\n",
    "    d_block=study_ResNet.best_params['d_block'],\n",
    "    d_hidden=None,\n",
    "    d_hidden_multiplier=study_ResNet.best_params['d_hidden_multiplier'],\n",
    "    dropout1=study_ResNet.best_params['dropout1'],\n",
    "    dropout2=study_ResNet.best_params['dropout2'],\n",
    "    )\n",
    "n_epochs=study_ResNet.best_params['n_epochs']\n",
    "learning_rate=study_ResNet.best_params['learning_rate']\n",
    "optimizer=torch.optim.Adam(ResNet_model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "loss_Adam=[]\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    # making a pridiction in forward pass\n",
    "    y_train_hat = ResNet_model(torch.Tensor(X_train.values)).reshape(-1,)\n",
    "    # calculating the loss between original and predicted data points\n",
    "    loss = criterion(y_train_hat, torch.Tensor(y_train.values))\n",
    "    # store loss into list\n",
    "    loss_Adam.append(loss.item())\n",
    "    # zeroing gradients after each iteration\n",
    "    optimizer.zero_grad()\n",
    "    # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n",
    "    loss.backward()\n",
    "    # updating the parameters after each iteration\n",
    "    optimizer.step()\n",
    "\n",
    "y_test_hat_ResNet=(ResNet_model(torch.Tensor(X_test.values)).reshape(-1,)).detach().numpy()\n",
    "RMSE_ResNet=np.sqrt(np.mean((y_test-y_test_hat_ResNet)**2))\n",
    "print(\"RMSE ResNet: \", RMSE_ResNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FFTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-08 14:46:52,555] A new study created in memory with name: no-name-7acaca2b-54b2-449c-8afa-679aaa0aa628\n"
     ]
    }
   ],
   "source": [
    "N_TRIALS=5\n",
    "\n",
    "d_out = 1  \n",
    "d_in=X_train_.shape[1]\n",
    "\n",
    "def FTTrans_opt(trial):\n",
    "\n",
    "    seed=10\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    n_blocks = trial.suggest_int(\"n_blocks\", 1, 5)\n",
    "    d_block_multiplier = trial.suggest_int(\"d_block_multiplier\", 1, 25)\n",
    "    attention_n_heads = trial.suggest_int(\"attention_n_heads\", 1, 20)\n",
    "    attention_dropout = trial.suggest_float(\"attention_dropout\", 0, 1)\n",
    "    ffn_d_hidden_multiplier=trial.suggest_float(\"ffn_d_hidden_multiplier\", 0.5, 3)\n",
    "    ffn_dropout = trial.suggest_float(\"ffn_dropout\", 0, 1)\n",
    "    residual_dropout = trial.suggest_float(\"residual_dropout\", 0, 1)\n",
    "\n",
    "    FTTrans_model = FTTransformer(\n",
    "    n_cont_features=d_in,\n",
    "    cat_cardinalities=[],\n",
    "    d_out=d_out,\n",
    "    n_blocks=n_blocks,\n",
    "    d_block=d_block_multiplier*attention_n_heads,\n",
    "    attention_n_heads=attention_n_heads,\n",
    "    attention_dropout=attention_dropout,\n",
    "    ffn_d_hidden=None,\n",
    "    ffn_d_hidden_multiplier=ffn_d_hidden_multiplier,\n",
    "    ffn_dropout=ffn_dropout,\n",
    "    residual_dropout=residual_dropout,\n",
    "    )\n",
    "\n",
    "    n_epochs=trial.suggest_int('n_epochs', 100, 5000)\n",
    "    learning_rate=trial.suggest_float('learning_rate', 0.0001, 0.05, log=True)\n",
    "    optimizer=torch.optim.Adam(FTTrans_model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    loss_Adam=[]\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        # making a pridiction in forward pass\n",
    "        y_train_hat = FTTrans_model(torch.Tensor(X_train_.values),None).reshape(-1,)\n",
    "        # calculating the loss between original and predicted data points\n",
    "        loss = criterion(y_train_hat, torch.Tensor(y_train_.values))\n",
    "        # store loss into list\n",
    "        loss_Adam.append(loss.item())\n",
    "        # zeroing gradients after each iteration\n",
    "        optimizer.zero_grad()\n",
    "        # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n",
    "        loss.backward()\n",
    "        # updating the parameters after each iteration\n",
    "        optimizer.step()\n",
    "\n",
    "    y_val_hat_FTTrans=(FTTrans_model(torch.Tensor(X_val.values), None).reshape(-1,)).detach().numpy()\n",
    "    RMSE_FTTrans=np.sqrt(np.mean((y_val-y_val_hat_FTTrans)**2))\n",
    "\n",
    "    return RMSE_FTTrans\n",
    "\n",
    "sampler_FTTrans = optuna.samplers.TPESampler(seed=10)\n",
    "study_FTTrans = optuna.create_study(sampler=sampler_FTTrans, direction='minimize')\n",
    "study_FTTrans.optimize(FTTrans_opt, n_trials=N_TRIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=10\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "FTTrans_model = FTTransformer(\n",
    "    n_cont_features=d_in,\n",
    "    cat_cardinalities=[],\n",
    "    d_out=d_out,\n",
    "    n_blocks=study_FTTrans.best_params['n_blocks'],\n",
    "    d_block=study_FTTrans.best_params['d_block'],\n",
    "    attention_n_heads=study_FTTrans.best_params['attention_n_heads'],\n",
    "    attention_dropout=study_FTTrans.best_params['attention_dropout'],\n",
    "    ffn_d_hidden=None,\n",
    "    ffn_d_hidden_multiplier=study_FTTrans.best_params['ffn_d_hidden_multiplier'],\n",
    "    ffn_dropout=study_FTTrans.best_params['ffn_dropout'],\n",
    "    residual_dropout=study_FTTrans.best_params['residual_dropout'],\n",
    "    )\n",
    "n_epochs=study_FTTrans.best_params['n_epochs']\n",
    "learning_rate=study_FTTrans.best_params['learning_rate']\n",
    "optimizer=torch.optim.Adam(FTTrans_model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "loss_Adam=[]\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    # making a pridiction in forward pass\n",
    "    y_train_hat = FTTrans_model(torch.Tensor(X_train.values), None).reshape(-1,)\n",
    "    # calculating the loss between original and predicted data points\n",
    "    loss = criterion(y_train_hat, torch.Tensor(y_train.values))\n",
    "    # store loss into list\n",
    "    loss_Adam.append(loss.item())\n",
    "    # zeroing gradients after each iteration\n",
    "    optimizer.zero_grad()\n",
    "    # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n",
    "    loss.backward()\n",
    "    # updating the parameters after each iteration\n",
    "    optimizer.step()\n",
    "\n",
    "y_test_hat_FTTrans=(FTTrans_model(torch.Tensor(X_test.values)).reshape(-1,)).detach().numpy()\n",
    "RMSE_FTTrans=np.sqrt(np.mean((y_test-y_test_hat_FTTrans)**2))\n",
    "print(\"RMSE FTTrans: \", RMSE_FTTrans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosted trees, random forest, engression, linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-09 17:19:35,587] A new study created in memory with name: no-name-e8dcd721-fd53-4290-b8a6-fcad6c9bc482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002880 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3766\n",
      "[LightGBM] [Info] Number of data points in the train set: 5242, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 91.111980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-09 17:19:36,267] Trial 0 finished with value: 22.871447920679902 and parameters: {'learning_rate': 0.12071779104534666, 'n_estimators': 108, 'reg_lambda': 0.005044685709888605, 'max_depth': 23, 'min_child_samples': 55}. Best is trial 0 with value: 22.871447920679902.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000568 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3766\n",
      "[LightGBM] [Info] Number of data points in the train set: 5242, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 91.111980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-09 17:19:36,526] Trial 1 finished with value: 23.787172223598073 and parameters: {'learning_rate': 0.004043145805966843, 'n_estimators': 179, 'reg_lambda': 0.0699481785242808, 'max_depth': 6, 'min_child_samples': 18}. Best is trial 0 with value: 22.871447920679902.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000754 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3766\n",
      "[LightGBM] [Info] Number of data points in the train set: 5242, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 91.111980\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-09 17:19:37,013] Trial 2 finished with value: 22.8953419509751 and parameters: {'learning_rate': 0.07075637776590661, 'n_estimators': 482, 'reg_lambda': 1.08526150100961e-08, 'max_depth': 16, 'min_child_samples': 83}. Best is trial 0 with value: 22.871447920679902.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000532 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3766\n",
      "[LightGBM] [Info] Number of data points in the train set: 5242, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 91.111980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-09 17:19:37,588] Trial 3 finished with value: 22.83896387711106 and parameters: {'learning_rate': 0.044997613517186334, 'n_estimators': 389, 'reg_lambda': 4.235304245072407e-06, 'max_depth': 28, 'min_child_samples': 75}. Best is trial 3 with value: 22.83896387711106.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000562 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3766\n",
      "[LightGBM] [Info] Number of data points in the train set: 5242, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 91.111980\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-09 17:19:37,775] Trial 4 finished with value: 22.974478457403965 and parameters: {'learning_rate': 0.029128020748551788, 'n_estimators': 157, 'reg_lambda': 2.2912202578440842e-05, 'max_depth': 21, 'min_child_samples': 50}. Best is trial 3 with value: 22.83896387711106.\n",
      "[I 2024-01-09 17:19:37,779] A new study created in memory with name: no-name-23e3892a-3b27-4333-8de6-d909dc5ffada\n",
      "[I 2024-01-09 17:19:42,165] Trial 0 finished with value: 24.071689948252683 and parameters: {'n_estimators': 409, 'max_depth': 1, 'max_features': 20, 'min_samples_leaf': 78}. Best is trial 0 with value: 24.071689948252683.\n",
      "[I 2024-01-09 17:19:46,451] Trial 1 finished with value: 23.238447270707244 and parameters: {'n_estimators': 299, 'max_depth': 7, 'max_features': 6, 'min_samples_leaf': 79}. Best is trial 1 with value: 23.238447270707244.\n",
      "[I 2024-01-09 17:19:54,325] Trial 2 finished with value: 23.22888692152243 and parameters: {'n_estimators': 167, 'max_depth': 3, 'max_features': 21, 'min_samples_leaf': 96}. Best is trial 2 with value: 23.22888692152243.\n",
      "[I 2024-01-09 17:20:03,243] Trial 3 finished with value: 23.141474923060013 and parameters: {'n_estimators': 101, 'max_depth': 16, 'max_features': 25, 'min_samples_leaf': 65}. Best is trial 3 with value: 23.141474923060013.\n",
      "[I 2024-01-09 17:20:34,727] Trial 4 finished with value: 23.154899861273986 and parameters: {'n_estimators': 389, 'max_depth': 9, 'max_features': 28, 'min_samples_leaf': 75}. Best is trial 3 with value: 23.141474923060013.\n",
      "[I 2024-01-09 17:20:34,738] A new study created in memory with name: no-name-4b66b3e3-5656-4718-9011-8f719af39f2a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 6] energy-loss: 0.9121,  E(|Y-Yhat|): 1.3543,  E(|Yhat-Yhat'|): 0.8843\n",
      "[Epoch 100 (84%), batch 6] energy-loss: 0.1901,  E(|Y-Yhat|): 0.3871,  E(|Yhat-Yhat'|): 0.3939\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.3739,  E(|Y-Yhat|): 2.2580,  E(|Yhat-Yhat'|): 1.7682\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-09 17:21:04,272] Trial 0 finished with value: 36.79140090942383 and parameters: {'learning_rate': 0.12071779104534666, 'num_epoches': 118, 'num_layer': 4, 'hidden_dim': 88, 'noise_dim': 75}. Best is trial 0 with value: 36.79140090942383.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 6] energy-loss: 0.3614,  E(|Y-Yhat|): 0.7589,  E(|Yhat-Yhat'|): 0.7950\n",
      "[Epoch 100 (36%), batch 6] energy-loss: 0.2124,  E(|Y-Yhat|): 0.4042,  E(|Yhat-Yhat'|): 0.3835\n",
      "[Epoch 200 (72%), batch 6] energy-loss: 0.1777,  E(|Y-Yhat|): 0.3707,  E(|Yhat-Yhat'|): 0.3861\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.3022,  E(|Y-Yhat|): 2.3922,  E(|Yhat-Yhat'|): 2.1800\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-09 17:21:59,787] Trial 1 finished with value: 11.905889511108398 and parameters: {'learning_rate': 0.004043145805966843, 'num_epoches': 278, 'num_layer': 5, 'hidden_dim': 58, 'noise_dim': 54}. Best is trial 1 with value: 11.905889511108398.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 6] energy-loss: 0.4692,  E(|Y-Yhat|): 0.9000,  E(|Yhat-Yhat'|): 0.8614\n",
      "[Epoch 100 (10%), batch 6] energy-loss: 0.2179,  E(|Y-Yhat|): 0.4216,  E(|Yhat-Yhat'|): 0.4073\n",
      "[Epoch 200 (21%), batch 6] energy-loss: 0.1987,  E(|Y-Yhat|): 0.4070,  E(|Yhat-Yhat'|): 0.4165\n",
      "[Epoch 300 (31%), batch 6] energy-loss: 0.2008,  E(|Y-Yhat|): 0.4185,  E(|Yhat-Yhat'|): 0.4355\n",
      "[Epoch 400 (42%), batch 6] energy-loss: 0.2021,  E(|Y-Yhat|): 0.4047,  E(|Yhat-Yhat'|): 0.4052\n",
      "[Epoch 500 (52%), batch 6] energy-loss: 0.2187,  E(|Y-Yhat|): 0.4066,  E(|Yhat-Yhat'|): 0.3759\n",
      "[Epoch 600 (62%), batch 6] energy-loss: 0.1813,  E(|Y-Yhat|): 0.4092,  E(|Yhat-Yhat'|): 0.4558\n",
      "[Epoch 700 (73%), batch 6] energy-loss: 0.2360,  E(|Y-Yhat|): 0.4224,  E(|Yhat-Yhat'|): 0.3729\n",
      "[Epoch 800 (83%), batch 6] energy-loss: 0.2073,  E(|Y-Yhat|): 0.4213,  E(|Yhat-Yhat'|): 0.4280\n",
      "[Epoch 900 (94%), batch 6] energy-loss: 0.1963,  E(|Y-Yhat|): 0.3887,  E(|Yhat-Yhat'|): 0.3848\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0569,  E(|Y-Yhat|): 2.1244,  E(|Yhat-Yhat'|): 2.1350\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-09 17:23:47,825] Trial 2 finished with value: 58.36392593383789 and parameters: {'learning_rate': 0.07075637776590661, 'num_epoches': 959, 'num_layer': 2, 'hidden_dim': 76, 'noise_dim': 91}. Best is trial 1 with value: 11.905889511108398.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 6] energy-loss: 0.7008,  E(|Y-Yhat|): 1.2413,  E(|Yhat-Yhat'|): 1.0810\n",
      "[Epoch 100 (13%), batch 6] energy-loss: 0.2129,  E(|Y-Yhat|): 0.3976,  E(|Yhat-Yhat'|): 0.3693\n",
      "[Epoch 200 (27%), batch 6] energy-loss: 0.2157,  E(|Y-Yhat|): 0.4177,  E(|Yhat-Yhat'|): 0.4039\n",
      "[Epoch 300 (40%), batch 6] energy-loss: 0.2099,  E(|Y-Yhat|): 0.4005,  E(|Yhat-Yhat'|): 0.3813\n",
      "[Epoch 400 (53%), batch 6] energy-loss: 0.2069,  E(|Y-Yhat|): 0.3826,  E(|Yhat-Yhat'|): 0.3513\n",
      "[Epoch 500 (67%), batch 6] energy-loss: 0.1762,  E(|Y-Yhat|): 0.3536,  E(|Yhat-Yhat'|): 0.3549\n",
      "[Epoch 600 (80%), batch 6] energy-loss: 0.2083,  E(|Y-Yhat|): 0.3914,  E(|Yhat-Yhat'|): 0.3662\n",
      "[Epoch 700 (93%), batch 6] energy-loss: 0.1773,  E(|Y-Yhat|): 0.3976,  E(|Yhat-Yhat'|): 0.4407\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.8961,  E(|Y-Yhat|): 1.9229,  E(|Yhat-Yhat'|): 2.0536\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-09 17:25:45,068] Trial 3 finished with value: 72.61552429199219 and parameters: {'learning_rate': 0.044997613517186334, 'num_epoches': 750, 'num_layer': 3, 'hidden_dim': 96, 'noise_dim': 86}. Best is trial 1 with value: 11.905889511108398.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 6] energy-loss: 0.3242,  E(|Y-Yhat|): 0.7643,  E(|Yhat-Yhat'|): 0.8801\n",
      "[Epoch 100 (43%), batch 6] energy-loss: 0.1910,  E(|Y-Yhat|): 0.4149,  E(|Yhat-Yhat'|): 0.4479\n",
      "[Epoch 200 (87%), batch 6] energy-loss: 0.1900,  E(|Y-Yhat|): 0.3945,  E(|Yhat-Yhat'|): 0.4091\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0098,  E(|Y-Yhat|): 2.1299,  E(|Yhat-Yhat'|): 2.2402\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-01-09 17:26:20,537] Trial 4 finished with value: 30.41522979736328 and parameters: {'learning_rate': 0.029128020748551788, 'num_epoches': 228, 'num_layer': 3, 'hidden_dim': 84, 'noise_dim': 72}. Best is trial 1 with value: 11.905889511108398.\n"
     ]
    }
   ],
   "source": [
    "N_TRIALS=5\n",
    "\n",
    "def boosted(trial):\n",
    "\n",
    "    params = {'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.5, log=True),\n",
    "              'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "              'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "              'max_depth': trial.suggest_int('max_depth', 1, 30),\n",
    "              'min_child_samples': trial.suggest_int('min_child_samples', 10, 100)}\n",
    "    \n",
    "    boosted_tree_model=lgbm.LGBMRegressor(**params)\n",
    "    boosted_tree_model.fit(X_train_, y_train_)\n",
    "    y_val_hat_boost=boosted_tree_model.predict(X_val)\n",
    "    RMSE_boost=np.sqrt(np.mean((y_val-y_val_hat_boost)**2))\n",
    "\n",
    "    return RMSE_boost\n",
    "\n",
    "sampler_boost = optuna.samplers.TPESampler(seed=10)\n",
    "study_boost = optuna.create_study(sampler=sampler_boost, direction='minimize')\n",
    "study_boost.optimize(boosted, n_trials=N_TRIALS)\n",
    "\n",
    "boosted_model=lgbm.LGBMRegressor(**study_boost.best_params)\n",
    "\n",
    "def rf(trial):\n",
    "\n",
    "    params = {'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "              'max_depth': trial.suggest_int('max_depth', 1, 30),\n",
    "              'max_features': trial.suggest_int('max_features', 1, 30),\n",
    "              'min_samples_leaf': trial.suggest_int('min_samples_leaf', 10, 100)}\n",
    "    \n",
    "    rf_model=RandomForestRegressor(**params)\n",
    "    rf_model.fit(X_train_, y_train_)\n",
    "    y_val_hat_rf=rf_model.predict(X_val)\n",
    "    RMSE_rf=np.sqrt(np.mean((y_val-y_val_hat_rf)**2))\n",
    "\n",
    "    return RMSE_rf\n",
    "\n",
    "sampler_rf = optuna.samplers.TPESampler(seed=10)\n",
    "study_rf = optuna.create_study(sampler=sampler_rf, direction='minimize')\n",
    "study_rf.optimize(rf, n_trials=N_TRIALS)\n",
    "\n",
    "rf_model=RandomForestRegressor(**study_rf.best_params)\n",
    "\n",
    "\n",
    "def engressor_NN(trial):\n",
    "\n",
    "    params = {'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.5, log=True),\n",
    "              'num_epoches': trial.suggest_int('num_epoches', 100, 1000),\n",
    "              'num_layer': trial.suggest_int('num_layer', 2, 5),\n",
    "              'hidden_dim': trial.suggest_int('hidden_dim', 50, 100),\n",
    "              'noise_dim': trial.suggest_int('noise_dim', 50, 100),}\n",
    "    \n",
    "    engressor_model=engression(torch.Tensor(np.array(X_train_)), torch.Tensor(np.array(y_train_).reshape(-1,1)), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=1000)\n",
    "    y_val_hat_engression=engressor_model.predict(torch.Tensor(np.array(X_val)), target=\"mean\")\n",
    "    RMSE_engression=np.sqrt((((torch.Tensor(np.array(y_val).reshape(-1,1)))-y_val_hat_engression)**2).mean(axis=0))\n",
    "\n",
    "    return RMSE_engression\n",
    "\n",
    "sampler_engression = optuna.samplers.TPESampler(seed=10)\n",
    "study_engression = optuna.create_study(sampler=sampler_engression, direction='minimize')\n",
    "study_engression.optimize(engressor_NN, n_trials=N_TRIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002846 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4351\n",
      "[LightGBM] [Info] Number of data points in the train set: 6553, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 88.320464\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 7] energy-loss: 0.6135,  E(|Y-Yhat|): 0.6353,  E(|Yhat-Yhat'|): 0.0437\n",
      "[Epoch 100 (84%), batch 7] energy-loss: 0.1079,  E(|Y-Yhat|): 0.2349,  E(|Yhat-Yhat'|): 0.2540\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 14.6823,  E(|Y-Yhat|): 16.6768,  E(|Yhat-Yhat'|): 3.9891\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "RMSE linear regression:  62.070783717577555\n",
      "RMSE boosted trees 5.276375538454642\n",
      "RMSE random forest 9.298867936154243\n",
      "RMSE engression tensor([58.7347])\n"
     ]
    }
   ],
   "source": [
    "boosted_model.fit(X_train, y_train)\n",
    "y_test_hat_boosted=boosted_model.predict(X_test)\n",
    "RMSE_boosted=np.sqrt(np.mean((y_test-y_test_hat_boosted)**2))\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_test_hat_rf=rf_model.predict(X_test)\n",
    "RMSE_rf=np.sqrt(np.mean((y_test-y_test_hat_rf)**2))\n",
    "\n",
    "lin_reg=LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "y_test_hat_linreg=lin_reg.predict(X_test)\n",
    "RMSE_linreg=np.sqrt(np.mean((y_test-y_test_hat_linreg)**2))\n",
    "\n",
    "params=study_engression.best_params\n",
    "engressor_model=engression(torch.Tensor(np.array(X_train)), torch.Tensor(np.array(y_train).reshape(-1,1)), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=1000)\n",
    "y_test_hat_engression=engressor_model.predict(torch.Tensor(np.array(X_test)), target=\"mean\")\n",
    "RMSE_engression=np.sqrt((((torch.Tensor(np.array(y_test).reshape(-1,1)))-y_test_hat_engression)**2).mean(axis=0))\n",
    "\n",
    "print(\"RMSE linear regression: \",RMSE_linreg)\n",
    "print(\"RMSE boosted trees\", RMSE_boosted)\n",
    "print(\"RMSE random forest\", RMSE_rf)\n",
    "print(\"RMSE engression\", RMSE_engression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Old way to define MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N_TRIALS=5\n",
    "\n",
    "def MLP_opt(trial):\n",
    "\n",
    "    seed=10\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
    "    layers = []\n",
    "\n",
    "    in_features = X_train_.shape[1]\n",
    "    for i in range(n_layers):\n",
    "        out_features = trial.suggest_int(\"n_units_l{}\".format(i), 4, 128)\n",
    "        layers.append(nn.Linear(in_features, out_features))\n",
    "        layers.append(nn.ReLU())\n",
    "        p = trial.suggest_uniform(\"dropout_l{}\".format(i), 0.2, 0.5)\n",
    "        layers.append(nn.Dropout(p))\n",
    "\n",
    "        in_features = out_features\n",
    "    layers.append(nn.Linear(in_features, 1))\n",
    "    MLP_model=nn.Sequential(*layers)\n",
    "    n_epochs=trial.suggest_int('n_epochs', 100, 5000)\n",
    "    learning_rate=trial.suggest_float('learning_rate', 0.0001, 0.05)\n",
    "    optimizer=torch.optim.Adam(MLP_model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    loss_Adam=[]\n",
    "\n",
    "    for i in range(n_epochs):\n",
    "        # making a pridiction in forward pass\n",
    "        y_train_hat = MLP_model(torch.Tensor(X_train_.values)).reshape(-1,)\n",
    "        # calculating the loss between original and predicted data points\n",
    "        loss = criterion(y_train_hat, torch.Tensor(y_train_.values))\n",
    "        # store loss into list\n",
    "        loss_Adam.append(loss.item())\n",
    "        # zeroing gradients after each iteration\n",
    "        optimizer.zero_grad()\n",
    "        # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n",
    "        loss.backward()\n",
    "        # updating the parameters after each iteration\n",
    "        optimizer.step()\n",
    "\n",
    "    y_val_hat_MLP=(MLP_model(torch.Tensor(X_val.values)).reshape(-1,)).detach().numpy()\n",
    "    RMSE_MLP=np.sqrt(np.mean((y_val-y_val_hat_MLP)**2))\n",
    "\n",
    "    return RMSE_MLP\n",
    "\n",
    "sampler_MLP = optuna.samplers.TPESampler(seed=10)\n",
    "study_MLP = optuna.create_study(sampler=sampler_MLP, direction='minimize')\n",
    "study_MLP.optimize(MLP_opt, n_trials=N_TRIALS)\n",
    "\n",
    "#gp_model=GaussianProcessRegressor(kernel=Matern(length_scale=study_gp.best_params['lenghtscale'], nu=1.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed=10\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "np.random.seed(seed)\n",
    "\n",
    "layers = []\n",
    "\n",
    "in_features = X_train.shape[1]\n",
    "for i in range(study_MLP.best_params['n_layers']):\n",
    "    out_features = study_MLP.best_params[\"n_units_l{}\".format(i)]\n",
    "    layers.append(nn.Linear(in_features, out_features))\n",
    "    layers.append(nn.ReLU())\n",
    "    p = study_MLP.best_params[\"dropout_l{}\".format(i)]\n",
    "    layers.append(nn.Dropout(p))\n",
    "\n",
    "    in_features = out_features\n",
    "\n",
    "layers.append(nn.Linear(in_features, 1))\n",
    "MLP_model=nn.Sequential(*layers)\n",
    "n_epochs=study_MLP.best_params['n_epochs']\n",
    "learning_rate=study_MLP.best_params['learning_rate']\n",
    "optimizer=torch.optim.Adam(MLP_model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.MSELoss()\n",
    "loss_Adam=[]\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    # making a pridiction in forward pass\n",
    "    y_train_hat = MLP_model(torch.Tensor(X_train.values)).reshape(-1,)\n",
    "    # calculating the loss between original and predicted data points\n",
    "    loss = criterion(y_train_hat, torch.Tensor(y_train.values))\n",
    "    # store loss into list\n",
    "    loss_Adam.append(loss.item())\n",
    "    # zeroing gradients after each iteration\n",
    "    optimizer.zero_grad()\n",
    "    # backward pass for computing the gradients of the loss w.r.t to learnable parameters\n",
    "    loss.backward()\n",
    "    # updating the parameters after each iteration\n",
    "    optimizer.step()\n",
    "\n",
    "y_test_hat_MLP=(MLP_model(torch.Tensor(X_test.values)).reshape(-1,)).detach().numpy()\n",
    "RMSE_MLP=np.sqrt(np.mean((y_test-y_test_hat_MLP)**2))\n",
    "print(\"RMSE MLP: \", RMSE_MLP)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dfcdafd4c2f198e9231774dbaa691ef2a9d56c92dabac0fe3d9f172dfc08608e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
