{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import openml\n",
    "from sklearn.linear_model import LinearRegression \n",
    "import lightgbm as lgbm\n",
    "import optuna\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern\n",
    "from engression import engression, engression_bagged\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openml\\tasks\\functions.py:372: FutureWarning: Starting from Version 0.15.0 `download_splits` will default to ``False`` instead of ``True`` and be independent from `download_data`. To disable this message until version 0.15 explicitly set `download_splits` to a bool.\n",
      "  warnings.warn(\n",
      "c:\\Users\\dalma\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\openml\\datasets\\functions.py:438: FutureWarning: Starting from Version 0.15 `download_data`, `download_qualities`, and `download_features_meta_data` will all be ``False`` instead of ``True`` by default to enable lazy loading. To disable this message until version 0.15 explicitly set `download_data`, `download_qualities`, and `download_features_meta_data` to a bool while calling `get_dataset`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#openml.config.apikey = 'FILL_IN_OPENML_API_KEY'  # set the OpenML Api Key\n",
    "SUITE_ID = 336 # Regression on numerical features\n",
    "#SUITE_ID = 337 # Classification on numerical features\n",
    "#SUITE_ID = 335 # Regression on numerical and categorical features\n",
    "#SUITE_ID = 334 # Classification on numerical and categorical features\n",
    "benchmark_suite = openml.study.get_suite(SUITE_ID)  # obtain the benchmark suite\n",
    "\n",
    "task = openml.tasks.get_task(361072)  # download the OpenML task\n",
    "dataset = task.get_dataset()\n",
    "\n",
    "X, y, categorical_indicator, attribute_names = dataset.get_data(\n",
    "        dataset_format=\"dataframe\", target=dataset.default_target_attribute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the mean and covariance matrix of the dataset\n",
    "mean = np.mean(X, axis=0)\n",
    "cov = np.cov(X.T)\n",
    "\n",
    "# calculate the Mahalanobis distance for each data point\n",
    "mahalanobis_dist = [mahalanobis(x, mean, np.linalg.inv(cov)) for x in X.values]\n",
    "\n",
    "mahalanobis_dist=pd.Series(mahalanobis_dist,index=X.index)\n",
    "far_index=mahalanobis_dist.index[np.where(mahalanobis_dist>=np.quantile(mahalanobis_dist,0.8))[0]]\n",
    "close_index=mahalanobis_dist.index[np.where(mahalanobis_dist<np.quantile(mahalanobis_dist,0.8))[0]]\n",
    "\n",
    "X_train = X.loc[close_index,:]\n",
    "X_test = X.loc[far_index,:]\n",
    "y_train = y.loc[close_index]\n",
    "y_test = y.loc[far_index]\n",
    "\n",
    "mean = np.mean(X_train, axis=0)\n",
    "cov = np.cov(X_train.T)\n",
    "\n",
    "# calculate the Mahalanobis distance for each data point\n",
    "mahalanobis_dist_ = [mahalanobis(x, mean, np.linalg.inv(cov)) for x in X_train.values]\n",
    "\n",
    "mahalanobis_dist_=pd.Series(mahalanobis_dist_,index=X_train.index)\n",
    "far_index_=mahalanobis_dist_.index[np.where(mahalanobis_dist_>=np.quantile(mahalanobis_dist_,0.8))[0]]\n",
    "close_index_=mahalanobis_dist_.index[np.where(mahalanobis_dist_<np.quantile(mahalanobis_dist_,0.8))[0]]\n",
    "\n",
    "X_train_ = X_train.loc[close_index_,:]\n",
    "X_val = X_train.loc[far_index_,:]\n",
    "y_train_ = y_train.loc[close_index_]\n",
    "y_val = y_train.loc[far_index_]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 16:52:36,341] A new study created in memory with name: no-name-76f857dd-846f-45c7-a01d-187f2ac6312b\n",
      "[I 2023-12-04 16:54:07,410] Trial 0 finished with value: 36.3988536368184 and parameters: {'lenghtscale': 7.71320643266746}. Best is trial 0 with value: 36.3988536368184.\n",
      "[I 2023-12-04 16:54:17,239] Trial 1 finished with value: 79.97454305109773 and parameters: {'lenghtscale': 0.207519493594015}. Best is trial 0 with value: 36.3988536368184.\n",
      "[I 2023-12-04 16:55:48,661] Trial 2 finished with value: 36.398853635730084 and parameters: {'lenghtscale': 6.336482349262754}. Best is trial 2 with value: 36.398853635730084.\n",
      "[I 2023-12-04 16:58:50,774] Trial 3 finished with value: 36.398854693719755 and parameters: {'lenghtscale': 7.488038825386118}. Best is trial 2 with value: 36.398853635730084.\n",
      "[I 2023-12-04 17:02:23,089] Trial 4 finished with value: 36.39890062205961 and parameters: {'lenghtscale': 4.9850701230259045}. Best is trial 2 with value: 36.398853635730084.\n",
      "[I 2023-12-04 17:02:23,106] A new study created in memory with name: no-name-2e9c725f-4a6f-4c69-bf03-4b64798ca76e\n",
      "[I 2023-12-04 17:02:23,373] Trial 0 finished with value: 22.916525483936596 and parameters: {'learning_rate': 0.38588900099010626, 'n_estimators': 108, 'reg_lambda': 0.005044685709888605, 'max_depth': 23, 'min_child_samples': 55}. Best is trial 0 with value: 22.916525483936596.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001871 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 3766\n",
      "[LightGBM] [Info] Number of data points in the train set: 5242, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 91.111980\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:02:23,549] Trial 1 finished with value: 22.810531505875442 and parameters: {'learning_rate': 0.11317352611989298, 'n_estimators': 179, 'reg_lambda': 0.0699481785242808, 'max_depth': 6, 'min_child_samples': 18}. Best is trial 1 with value: 22.810531505875442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000399 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3766\n",
      "[LightGBM] [Info] Number of data points in the train set: 5242, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 91.111980\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000463 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3766\n",
      "[LightGBM] [Info] Number of data points in the train set: 5242, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 91.111980\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:02:24,143] Trial 2 finished with value: 22.85055837450557 and parameters: {'learning_rate': 0.3429945493655308, 'n_estimators': 482, 'reg_lambda': 1.08526150100961e-08, 'max_depth': 16, 'min_child_samples': 83}. Best is trial 1 with value: 22.810531505875442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000630 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3766\n",
      "[LightGBM] [Info] Number of data points in the train set: 5242, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 91.111980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:02:24,584] Trial 3 finished with value: 22.88449372285418 and parameters: {'learning_rate': 0.3066505073478647, 'n_estimators': 389, 'reg_lambda': 4.235304245072407e-06, 'max_depth': 28, 'min_child_samples': 75}. Best is trial 1 with value: 22.810531505875442.\n",
      "[I 2023-12-04 17:02:24,768] Trial 4 finished with value: 22.964538631424723 and parameters: {'learning_rate': 0.27172963963761937, 'n_estimators': 157, 'reg_lambda': 2.2912202578440842e-05, 'max_depth': 21, 'min_child_samples': 50}. Best is trial 1 with value: 22.810531505875442.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000496 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3766\n",
      "[LightGBM] [Info] Number of data points in the train set: 5242, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 91.111980\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:02:24,776] A new study created in memory with name: no-name-c0168aba-99ef-4fef-b419-1802efd524b0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000604 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3766\n",
      "[LightGBM] [Info] Number of data points in the train set: 5242, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 91.111980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:02:25,369] Trial 0 finished with value: 23.17178639471135 and parameters: {'n_estimators': 409, 'reg_lambda': 1.537331564587801e-08, 'max_depth': 20, 'subsample_freq': 8, 'subsample': 0.7492535061512953, 'feature_fraction_bynode': 0.22479664553084766, 'min_child_samples': 28}. Best is trial 0 with value: 23.17178639471135.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000402 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3766\n",
      "[LightGBM] [Info] Number of data points in the train set: 5242, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 91.111980\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:02:25,571] Trial 1 finished with value: 23.717464823127113 and parameters: {'n_estimators': 404, 'reg_lambda': 3.32657660618516e-07, 'max_depth': 3, 'subsample_freq': 7, 'subsample': 0.9766966730974682, 'feature_fraction_bynode': 0.003948266327914451, 'min_child_samples': 56}. Best is trial 0 with value: 23.17178639471135.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000613 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3766\n",
      "[LightGBM] [Info] Number of data points in the train set: 5242, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 91.111980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:02:26,191] Trial 2 finished with value: 23.160358983735637 and parameters: {'n_estimators': 425, 'reg_lambda': 0.003256376421394008, 'max_depth': 22, 'subsample_freq': 3, 'subsample': 0.9588870612564717, 'feature_fraction_bynode': 0.7145757833976906, 'min_child_samples': 59}. Best is trial 2 with value: 23.160358983735637.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000499 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3766\n",
      "[LightGBM] [Info] Number of data points in the train set: 5242, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 91.111980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:02:26,405] Trial 3 finished with value: 23.17696691866533 and parameters: {'n_estimators': 157, 'reg_lambda': 2.2912202578440842e-05, 'max_depth': 21, 'subsample_freq': 5, 'subsample': 0.7170069966666468, 'feature_fraction_bynode': 0.6177669784693172, 'min_child_samples': 56}. Best is trial 2 with value: 23.160358983735637.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000597 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 3766\n",
      "[LightGBM] [Info] Number of data points in the train set: 5242, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 91.111980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:02:26,985] Trial 4 finished with value: 23.139511147941665 and parameters: {'n_estimators': 360, 'reg_lambda': 0.0025665550309028774, 'max_depth': 25, 'subsample_freq': 6, 'subsample': 0.9543244404043341, 'feature_fraction_bynode': 0.3192360889885453, 'min_child_samples': 18}. Best is trial 4 with value: 23.139511147941665.\n",
      "[I 2023-12-04 17:02:26,985] A new study created in memory with name: no-name-5cfbfe91-4ddc-4892-811c-d43e05addf88\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 6] energy-loss: 1.1156,  E(|Y-Yhat|): 2.6892,  E(|Yhat-Yhat'|): 3.1472\n",
      "[Epoch 100 (84%), batch 6] energy-loss: 0.2423,  E(|Y-Yhat|): 0.4988,  E(|Yhat-Yhat'|): 0.5131\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.6624,  E(|Y-Yhat|): 2.3485,  E(|Yhat-Yhat'|): 1.3722\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:02:43,009] Trial 0 finished with value: 23.97712516784668 and parameters: {'learning_rate': 0.38588900099010626, 'num_epoches': 118, 'num_layer': 4, 'hidden_dim': 88, 'noise_dim': 75}. Best is trial 0 with value: 23.97712516784668.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 6] energy-loss: 0.6543,  E(|Y-Yhat|): 1.2957,  E(|Yhat-Yhat'|): 1.2829\n",
      "[Epoch 100 (36%), batch 6] energy-loss: 0.2030,  E(|Y-Yhat|): 0.4312,  E(|Yhat-Yhat'|): 0.4564\n",
      "[Epoch 200 (72%), batch 6] energy-loss: 0.1917,  E(|Y-Yhat|): 0.4500,  E(|Yhat-Yhat'|): 0.5166\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.2158,  E(|Y-Yhat|): 2.0559,  E(|Yhat-Yhat'|): 1.6801\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:03:17,051] Trial 1 finished with value: 13.67954158782959 and parameters: {'learning_rate': 0.11317352611989298, 'num_epoches': 278, 'num_layer': 5, 'hidden_dim': 58, 'noise_dim': 54}. Best is trial 1 with value: 13.67954158782959.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 6] energy-loss: 0.7065,  E(|Y-Yhat|): 1.9986,  E(|Yhat-Yhat'|): 2.5843\n",
      "[Epoch 100 (10%), batch 6] energy-loss: 0.2301,  E(|Y-Yhat|): 0.4942,  E(|Yhat-Yhat'|): 0.5281\n",
      "[Epoch 200 (21%), batch 6] energy-loss: 0.2387,  E(|Y-Yhat|): 0.4149,  E(|Yhat-Yhat'|): 0.3525\n",
      "[Epoch 300 (31%), batch 6] energy-loss: 0.2588,  E(|Y-Yhat|): 0.4397,  E(|Yhat-Yhat'|): 0.3617\n",
      "[Epoch 400 (42%), batch 6] energy-loss: 0.2555,  E(|Y-Yhat|): 0.4489,  E(|Yhat-Yhat'|): 0.3867\n",
      "[Epoch 500 (52%), batch 6] energy-loss: 0.2133,  E(|Y-Yhat|): 0.4156,  E(|Yhat-Yhat'|): 0.4047\n",
      "[Epoch 600 (62%), batch 6] energy-loss: 0.2685,  E(|Y-Yhat|): 0.4865,  E(|Yhat-Yhat'|): 0.4360\n",
      "[Epoch 700 (73%), batch 6] energy-loss: 0.2068,  E(|Y-Yhat|): 0.4261,  E(|Yhat-Yhat'|): 0.4385\n",
      "[Epoch 800 (83%), batch 6] energy-loss: 0.2202,  E(|Y-Yhat|): 0.4231,  E(|Yhat-Yhat'|): 0.4057\n",
      "[Epoch 900 (94%), batch 6] energy-loss: 0.2052,  E(|Y-Yhat|): 0.4222,  E(|Yhat-Yhat'|): 0.4339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:04:35,226] Trial 2 finished with value: 127.4811782836914 and parameters: {'learning_rate': 0.3429945493655308, 'num_epoches': 959, 'num_layer': 2, 'hidden_dim': 76, 'noise_dim': 91}. Best is trial 1 with value: 13.67954158782959.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.2577,  E(|Y-Yhat|): 2.6051,  E(|Yhat-Yhat'|): 2.6949\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 6] energy-loss: 0.7243,  E(|Y-Yhat|): 1.5722,  E(|Yhat-Yhat'|): 1.6958\n",
      "[Epoch 100 (13%), batch 6] energy-loss: 0.2150,  E(|Y-Yhat|): 0.4371,  E(|Yhat-Yhat'|): 0.4441\n",
      "[Epoch 200 (27%), batch 6] energy-loss: 0.2290,  E(|Y-Yhat|): 0.4668,  E(|Yhat-Yhat'|): 0.4756\n",
      "[Epoch 300 (40%), batch 6] energy-loss: 0.2033,  E(|Y-Yhat|): 0.4286,  E(|Yhat-Yhat'|): 0.4506\n",
      "[Epoch 400 (53%), batch 6] energy-loss: 0.2049,  E(|Y-Yhat|): 0.4491,  E(|Yhat-Yhat'|): 0.4884\n",
      "[Epoch 500 (67%), batch 6] energy-loss: 0.2045,  E(|Y-Yhat|): 0.4100,  E(|Yhat-Yhat'|): 0.4109\n",
      "[Epoch 600 (80%), batch 6] energy-loss: 0.1917,  E(|Y-Yhat|): 0.4252,  E(|Yhat-Yhat'|): 0.4671\n",
      "[Epoch 700 (93%), batch 6] energy-loss: 0.2201,  E(|Y-Yhat|): 0.4813,  E(|Yhat-Yhat'|): 0.5224\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 0.9905,  E(|Y-Yhat|): 2.1008,  E(|Yhat-Yhat'|): 2.2206\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:05:57,508] Trial 3 finished with value: 170.4100799560547 and parameters: {'learning_rate': 0.3066505073478647, 'num_epoches': 750, 'num_layer': 3, 'hidden_dim': 96, 'noise_dim': 86}. Best is trial 1 with value: 13.67954158782959.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 6] energy-loss: 0.8017,  E(|Y-Yhat|): 1.2023,  E(|Yhat-Yhat'|): 0.8012\n",
      "[Epoch 100 (43%), batch 6] energy-loss: 0.2099,  E(|Y-Yhat|): 0.4407,  E(|Yhat-Yhat'|): 0.4616\n",
      "[Epoch 200 (87%), batch 6] energy-loss: 0.2332,  E(|Y-Yhat|): 0.4366,  E(|Yhat-Yhat'|): 0.4068\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 1.0694,  E(|Y-Yhat|): 2.2098,  E(|Yhat-Yhat'|): 2.2809\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2023-12-04 17:06:21,196] Trial 4 finished with value: 30.25997543334961 and parameters: {'learning_rate': 0.27172963963761937, 'num_epoches': 228, 'num_layer': 3, 'hidden_dim': 84, 'noise_dim': 72}. Best is trial 1 with value: 13.67954158782959.\n"
     ]
    }
   ],
   "source": [
    "N_TRIALS=5\n",
    "\n",
    "def gp(trial):\n",
    "\n",
    "    params = {'lenghtscale': trial.suggest_float('lenghtscale', 0, 10)}\n",
    "    \n",
    "    gp=GaussianProcessRegressor(kernel=Matern(length_scale=params['lenghtscale'], nu=1.5))\n",
    "    gp.fit(X_train_, y_train_)\n",
    "    y_val_hat_gp=gp.predict(X_val)\n",
    "    RMSE_gp=np.sqrt(np.mean((y_val-y_val_hat_gp)**2))\n",
    "    return RMSE_gp\n",
    "\n",
    "sampler_gp = optuna.samplers.TPESampler(seed=10)\n",
    "study_gp = optuna.create_study(sampler=sampler_gp, direction='minimize')\n",
    "study_gp.optimize(gp, n_trials=N_TRIALS)\n",
    "\n",
    "gp_model=GaussianProcessRegressor(kernel=Matern(length_scale=study_gp.best_params['lenghtscale'], nu=1.5))\n",
    "\n",
    "\n",
    "def boosted(trial):\n",
    "\n",
    "    params = {'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.5),\n",
    "              'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "              'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "              'max_depth': trial.suggest_int('max_depth', 1, 30),\n",
    "              #'num_leaves': trial.suggest_int('num_leaves', 2, 100),\n",
    "              'min_child_samples': trial.suggest_int('min_child_samples', 10, 100)}\n",
    "    \n",
    "    boosted_tree_model=lgbm.LGBMRegressor(**params)\n",
    "    boosted_tree_model.fit(X_train_, y_train_)\n",
    "    y_val_hat_boost=boosted_tree_model.predict(X_val)\n",
    "    RMSE_boost=np.sqrt(np.mean((y_val-y_val_hat_boost)**2))\n",
    "\n",
    "    return RMSE_boost\n",
    "\n",
    "sampler_boost = optuna.samplers.TPESampler(seed=10)\n",
    "study_boost = optuna.create_study(sampler=sampler_boost, direction='minimize')\n",
    "study_boost.optimize(boosted, n_trials=N_TRIALS)\n",
    "\n",
    "boosted_model=lgbm.LGBMRegressor(**study_boost.best_params)\n",
    "\n",
    "\n",
    "def rf(trial):\n",
    "\n",
    "    params = {'boosting_type':\"rf\",\n",
    "              'n_estimators': trial.suggest_int('n_estimators', 100, 500),\n",
    "              'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 10.0, log=True),\n",
    "              'max_depth': trial.suggest_int('max_depth', 1, 30),\n",
    "              'subsample_freq': trial.suggest_int('subsample_freq', 1, 10),\n",
    "              'subsample': trial.suggest_float('subsample', 0.5, 1),\n",
    "              'feature_fraction_bynode': trial.suggest_float('feature_fraction_bynode', 0, 1),\n",
    "              #'num_leaves': trial.suggest_int('num_leaves', 2, 100),\n",
    "              'min_child_samples': trial.suggest_int('min_child_samples', 10, 100)}\n",
    "    \n",
    "    rf_model=lgbm.LGBMRegressor(**params)\n",
    "    rf_model.fit(X_train_, y_train_)\n",
    "    y_val_hat_rf=rf_model.predict(X_val)\n",
    "    RMSE_rf=np.sqrt(np.mean((y_val-y_val_hat_rf)**2))\n",
    "\n",
    "    return RMSE_rf\n",
    "\n",
    "sampler_rf = optuna.samplers.TPESampler(seed=10)\n",
    "study_rf = optuna.create_study(sampler=sampler_rf, direction='minimize')\n",
    "study_rf.optimize(rf, n_trials=N_TRIALS)\n",
    "\n",
    "rf_model=lgbm.LGBMRegressor(**study_rf.best_params)\n",
    "\n",
    "def engressor_NN(trial):\n",
    "\n",
    "    params = {'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.5),\n",
    "              'num_epoches': trial.suggest_int('num_epoches', 100, 1000),\n",
    "              'num_layer': trial.suggest_int('num_layer', 2, 5),\n",
    "              'hidden_dim': trial.suggest_int('hidden_dim', 50, 100),\n",
    "              'noise_dim': trial.suggest_int('noise_dim', 50, 100),}\n",
    "    \n",
    "    engressor_model=engression(torch.Tensor(np.array(X_train_)), torch.Tensor(np.array(y_train_).reshape(-1,1)), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=1000)\n",
    "    y_val_hat_engression=engressor_model.predict(torch.Tensor(np.array(X_val)), target=\"mean\")\n",
    "    RMSE_engression=np.sqrt((((torch.Tensor(np.array(y_val).reshape(-1,1)))-y_val_hat_engression)**2).mean(axis=0))\n",
    "\n",
    "    return RMSE_engression\n",
    "\n",
    "sampler_engression = optuna.samplers.TPESampler(seed=10)\n",
    "study_engression = optuna.create_study(sampler=sampler_engression, direction='minimize')\n",
    "study_engression.optimize(engressor_NN, n_trials=N_TRIALS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002056 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4351\n",
      "[LightGBM] [Info] Number of data points in the train set: 6553, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 88.320464\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000958 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4351\n",
      "[LightGBM] [Info] Number of data points in the train set: 6553, number of used features: 21\n",
      "[LightGBM] [Info] Start training from score 88.320464\n",
      "[LightGBM] [Warning] Accuracy may be bad since you didn't explicitly set num_leaves OR 2^max_depth > num_leaves. (num_leaves=31).\n",
      "Running on CPU.\n",
      "\n",
      "Data is standardized for training only; the printed training losses are on the standardized scale. \n",
      "However during evaluation, the predictions, evaluation metrics, and plots will be on the original scale.\n",
      "\n",
      "Training based on mini-batch gradient descent with a batch size of 1000.\n",
      "[Epoch 1 (0%), batch 7] energy-loss: 0.3851,  E(|Y-Yhat|): 0.6563,  E(|Yhat-Yhat'|): 0.5424\n",
      "[Epoch 100 (36%), batch 7] energy-loss: 0.1259,  E(|Y-Yhat|): 0.2477,  E(|Yhat-Yhat'|): 0.2436\n",
      "[Epoch 200 (72%), batch 7] energy-loss: 0.1082,  E(|Y-Yhat|): 0.2585,  E(|Yhat-Yhat'|): 0.3007\n",
      "\n",
      "Training loss on the original (non-standardized) scale:\n",
      "\tEnergy-loss: 5.9870,  E(|Y-Yhat|): 6.8690,  E(|Yhat-Yhat'|): 1.7640\n",
      "\n",
      "Prediction-loss E(|Y-Yhat|) and variance-loss E(|Yhat-Yhat'|) should ideally be equally large\n",
      "-- consider training for more epochs or adjusting hyperparameters if there is a mismatch \n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1311) must match the size of tensor b (1639) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 21\u001b[0m\n\u001b[0;32m     19\u001b[0m engressor_model\u001b[39m=\u001b[39mengression(torch\u001b[39m.\u001b[39mTensor(np\u001b[39m.\u001b[39marray(X_train)), torch\u001b[39m.\u001b[39mTensor(np\u001b[39m.\u001b[39marray(y_train)\u001b[39m.\u001b[39mreshape(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m1\u001b[39m)), lr\u001b[39m=\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m], num_epoches\u001b[39m=\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mnum_epoches\u001b[39m\u001b[39m'\u001b[39m],num_layer\u001b[39m=\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mnum_layer\u001b[39m\u001b[39m'\u001b[39m], hidden_dim\u001b[39m=\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mhidden_dim\u001b[39m\u001b[39m'\u001b[39m], noise_dim\u001b[39m=\u001b[39mparams[\u001b[39m'\u001b[39m\u001b[39mnoise_dim\u001b[39m\u001b[39m'\u001b[39m], batch_size\u001b[39m=\u001b[39m\u001b[39m1000\u001b[39m)\n\u001b[0;32m     20\u001b[0m y_test_hat_engression\u001b[39m=\u001b[39mengressor_model\u001b[39m.\u001b[39mpredict(torch\u001b[39m.\u001b[39mTensor(np\u001b[39m.\u001b[39marray(X_test)), target\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmean\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 21\u001b[0m RMSE_engression\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39msqrt((((torch\u001b[39m.\u001b[39;49mTensor(np\u001b[39m.\u001b[39;49marray(y_val)\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\u001b[39m1\u001b[39;49m)))\u001b[39m-\u001b[39;49my_test_hat_engression)\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean(axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n\u001b[0;32m     23\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRMSE linear regression: \u001b[39m\u001b[39m\"\u001b[39m,RMSE_linreg)\n\u001b[0;32m     24\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mRMSE boosted trees\u001b[39m\u001b[39m\"\u001b[39m, RMSE_boosted)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (1311) must match the size of tensor b (1639) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "gp_model.fit(X_train, y_train)\n",
    "y_test_hat_gp=gp_model.predict(X_test)\n",
    "RMSE_gp=np.sqrt(np.mean((y_test-y_test_hat_gp)**2))\n",
    "\n",
    "boosted_model.fit(X_train, y_train)\n",
    "y_test_hat_boosted=boosted_model.predict(X_test)\n",
    "RMSE_boosted=np.sqrt(np.mean((y_test-y_test_hat_boosted)**2))\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_test_hat_rf=rf_model.predict(X_test)\n",
    "RMSE_rf=np.sqrt(np.mean((y_test-y_test_hat_rf)**2))\n",
    "\n",
    "lin_reg=LinearRegression()\n",
    "lin_reg.fit(X_train, y_train)\n",
    "y_test_hat_linreg=lin_reg.predict(X_test)\n",
    "RMSE_linreg=np.sqrt(np.mean((y_test-y_test_hat_linreg)**2))\n",
    "\n",
    "params=study_engression.best_params\n",
    "engressor_model=engression(torch.Tensor(np.array(X_train)), torch.Tensor(np.array(y_train).reshape(-1,1)), lr=params['learning_rate'], num_epoches=params['num_epoches'],num_layer=params['num_layer'], hidden_dim=params['hidden_dim'], noise_dim=params['noise_dim'], batch_size=1000)\n",
    "y_test_hat_engression=engressor_model.predict(torch.Tensor(np.array(X_test)), target=\"mean\")\n",
    "RMSE_engression=np.sqrt((((torch.Tensor(np.array(y_test).reshape(-1,1)))-y_test_hat_engression)**2).mean(axis=0))\n",
    "\n",
    "print(\"RMSE linear regression: \",RMSE_linreg)\n",
    "print(\"RMSE boosted trees\", RMSE_boosted)\n",
    "print(\"RMSE random forest\", RMSE_rf)\n",
    "print(\"RMSE gaussian process\", RMSE_gp)\n",
    "print(\"RMSE engression\", RMSE_engression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE linear regression:  62.070783717577555\n",
      "RMSE boosted trees 5.276375538454642\n",
      "RMSE random forest 5.220881219934237\n",
      "RMSE gaussian process 38.12484784299995\n",
      "RMSE engression tensor([34.8711])\n"
     ]
    }
   ],
   "source": [
    "RMSE_engression=np.sqrt((((torch.Tensor(np.array(y_test).reshape(-1,1)))-y_test_hat_engression)**2).mean(axis=0))\n",
    "\n",
    "print(\"RMSE linear regression: \",RMSE_linreg)\n",
    "print(\"RMSE boosted trees\", RMSE_boosted)\n",
    "print(\"RMSE random forest\", RMSE_rf)\n",
    "print(\"RMSE gaussian process\", RMSE_gp)\n",
    "print(\"RMSE engression\", RMSE_engression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dfcdafd4c2f198e9231774dbaa691ef2a9d56c92dabac0fe3d9f172dfc08608e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
